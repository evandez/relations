{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "import baukit\n",
    "import transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model on `GPU:0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neox-20b ==> device: cuda:0, memory: 41293685880\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################################\n",
    "MODEL_NAME = \"EleutherAI/gpt-neox-20b\" # options gpt2-{} | \"EleutherAI/gpt-neox-20b\" | \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "layer_name_format = \"gpt_neox.layers.{}\"\n",
    "final_layer_norm = \"gpt_neox.final_layer_norm\"\n",
    "unembed = \"embed_out\"\n",
    "num_layer_field = \"num_hidden_layers\"\n",
    "\n",
    "# layer_name_format = \"transformer.h.{}\"\n",
    "# final_layer_norm = \"transformer.ln_f\"\n",
    "# unembed = \"lm_head\"\n",
    "# num_layer_field = \"n_layer\"\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the later part of the model on `GPU:1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 22 layers ==> device: cuda:1, memory: 21266563644\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# path_name = \"gpt2-medium__last_9_layers\"\n",
    "# path_name = \"gpt2-xl__last_25_layers\"\n",
    "path_name = \"EleutherAI/gpt-neox-20b__last_22_layers\"\n",
    "\n",
    "part_model = AutoModelForCausalLM.from_pretrained(\n",
    "                path_name, low_cpu_mem_usage=True, torch_dtype=torch.float16\n",
    "            )\n",
    "part_model = part_model.eval().cuda('cuda:1')\n",
    "\n",
    "part_layer_names = [layer_name_format.format(idx) for idx in range(getattr(part_model.config, num_layer_field))]\n",
    "print(f\"last {getattr(part_model.config, num_layer_field)} layers ==> device: {part_model.device}, memory: {part_model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################\n",
    "break_layer_idx = getattr(model.config, num_layer_field) - getattr(part_model.config, num_layer_field)\n",
    "#################################################################\n",
    "break_layer_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is loaded in `eval` mode. Make sure to set `requires_grad = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(module_name, prefix = \"transformer.h\", start_layer = 1):\n",
    "    if(module_name in [final_layer_norm, unembed]):\n",
    "        return True\n",
    "    for idx in range(start_layer, mt.num_layers):\n",
    "        if(module_name.startswith(f\"{prefix}.{idx}\")):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "need_gradients = {\n",
    "    n: p\n",
    "    for n, p in part_model.named_parameters()\n",
    "    if check_valid(\n",
    "        n, prefix = layer_name_format[:-3]\n",
    "    )\n",
    "}\n",
    "\n",
    "for n, w in part_model.named_parameters():\n",
    "    if(n in need_gradients):\n",
    "        w.requires_grad = True\n",
    "    else:   \n",
    "        w.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the `RelationOperator` (Jacobian and Biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt >>  The Space Needle is located in the country of\n",
      "h_token_idx >>  3\n",
      "gpt_neox.layers.0  << original gpt_neox.layers.22\n",
      "replacing gpt_neox.layers.5 outputs\n",
      "Calculating Jacobians ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6144/6144 [17:56<00:00,  5.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Jh_norm': 107.25,\n",
       " 'bias_norm': 895.0,\n",
       " 'h_info': {'h_index': 3, 'token_id': 282, 'token': 'le'},\n",
       " 'consider_residual': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_needle = estimate.estimate_relation_operator_neox(\n",
    "    model, part_model,\n",
    "    tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=27,\n",
    "\n",
    "    layer_name_format = layer_name_format,\n",
    "    final_layer_norm = final_layer_norm,\n",
    "    unembed = unembed,\n",
    ")\n",
    "\n",
    "space_needle.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    (\"The Space Needle\", -1, \"United States\"),\n",
    "    (\"The Great Wall\", -1, \"China\"),\n",
    "    (\"Niagara Falls\", -2, \"Canada\"),\n",
    "    (\"Valdemarsvik\", -1, \"Sweden\"),\n",
    "    (\"Kyoto University\", -2, \"Japan\"),\n",
    "    (\"Hattfjelldal\", -1, \"Norway\"),\n",
    "    (\"Ginza\", -1, \"Japan\"),\n",
    "    (\"Sydney Hospital\", -2, \"Australia\"),\n",
    "    (\"Mahalangur Himal\", -1, \"Nepal\"),\n",
    "    (\"Higashikagawa\", -1, \"Japan\"),\n",
    "    (\"Trento\", -1, \"Italy\"),\n",
    "    (\"Taj Mahal\", -1, \"India\")\n",
    "]\n",
    "\n",
    "def evaluate_against_test_cases(relation):\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        objects = relation(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {objects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: [' Washington', ' Seattle', ' the', ' Malaysia', ' Canada']\n",
      "The Great Wall, target: China   ==>   predicted: [' Seattle', ' Washington', ' the', '\\n', ' Japan']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Seattle', ' Washington', ' the', ' Japan', ' glass']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Seattle', ' Washington', ' the', ' Japan', ' glass']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', ' Seattle', ' Washington', ' the', ' Tokyo']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Seattle', ' Washington', ' the', ' glass', ' Japan']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Seattle', ' Washington', ' the', ' Tokyo']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Seattle', ' Washington', ' the', '\\n', ' origin']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Seattle', ' Washington', ' the', '\\n', ' glass']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' Seattle', ' Washington', ' the', ' Tokyo']\n",
      "Trento, target: Italy   ==>   predicted: [' Seattle', ' Washington', ' the', ' glass', '\\n']\n",
      "Taj Mahal, target: India   ==>   predicted: [' Seattle', ' Washington', ' the', ' Japan', ' Malaysia']\n"
     ]
    }
   ],
   "source": [
    "evaluate_against_test_cases(space_needle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4104f34302edfbfea7294aa0a5e7d82342a152e8e30f6673f70b28d5f99d4ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
