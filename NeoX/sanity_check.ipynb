{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "import baukit\n",
    "import transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl ==> device: cuda:0, memory: 3215885792\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################################\n",
    "MODEL_NAME = \"gpt2-xl\" # options gpt2-{} | \"EleutherAI/gpt-neox-20b\" | \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "# layer_name_format = \"gpt_neox.layers.{}\"\n",
    "# final_layer_norm = \"gpt_neox.final_layer_norm\"\n",
    "# unembed = \"embed_out\"\n",
    "# num_layer_field = \"num_hidden_layers\"\n",
    "\n",
    "layer_name_format = \"transformer.h.{}\"\n",
    "final_layer_norm = \"transformer.ln_f\"\n",
    "unembed = \"lm_head\"\n",
    "num_layer_field = \"n_layer\"\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the later part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 25 layers ==> device: cuda:1, memory: 1753574450\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# path_name = \"gpt2-medium__last_9_layers\"\n",
    "path_name = \"gpt2-xl__last_25_layers\"\n",
    "# path_name = \"EleutherAI/gpt-neox-20b__last_21_layers\"\n",
    "\n",
    "part_model = AutoModelForCausalLM.from_pretrained(\n",
    "                path_name, low_cpu_mem_usage=True, torch_dtype=torch.float16\n",
    "            )\n",
    "part_model = part_model.eval().cuda('cuda:1')\n",
    "\n",
    "part_layer_names = [layer_name_format.format(idx) for idx in range(getattr(part_model.config, num_layer_field))]\n",
    "print(f\"last {getattr(part_model.config, num_layer_field)} layers ==> device: {part_model.device}, memory: {part_model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################\n",
    "break_layer_idx = getattr(model.config, num_layer_field) - getattr(part_model.config, num_layer_field)\n",
    "#################################################################\n",
    "break_layer_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting `requires_grad=True`, we will need the grads to calculate the gradients later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(module_name, prefix = \"transformer.h\", start_layer = 1):\n",
    "    if(module_name in [final_layer_norm, unembed]):\n",
    "        return True\n",
    "    for idx in range(start_layer, mt.num_layers):\n",
    "        if(module_name.startswith(f\"{prefix}.{idx}\")):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "need_gradients = {\n",
    "    n: p\n",
    "    for n, p in part_model.named_parameters()\n",
    "    if check_valid(\n",
    "        n, prefix = layer_name_format[:-3]\n",
    "    )\n",
    "}\n",
    "\n",
    "for n, w in part_model.named_parameters():\n",
    "    if(n in need_gradients):\n",
    "        w.requires_grad = True\n",
    "    else:   \n",
    "        w.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"The Space Needle is located in the country of\"]\n",
    "\n",
    "tokenized_inputs = tokenizer(\n",
    "    prompt,\n",
    "    padding = True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(next(model.parameters()).device)\n",
    "\n",
    "break_layer_name = layer_name_format.format(break_layer_idx)\n",
    "z_layer_name = layer_name_format.format(mt.num_layers-1)\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    model, \n",
    "    mt.layer_names, # [h_layer_name, z_layer_name], \n",
    "    retain_input=True\n",
    ") as traces:\n",
    "    outputs = model(**tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_first_layer_output(target):\n",
    "    first_layer = layer_name_format.format(0)\n",
    "    def edit_policy(output, layer_name):\n",
    "        if(layer_name != first_layer):\n",
    "            return output\n",
    "        print(layer_name, \" << original\", break_layer_name)\n",
    "        output[0][...] = target[0].to(part_model.device)\n",
    "        output[1][0][...] = target[1][0].to(part_model.device)\n",
    "        output[1][1][...] = target[1][1].to(part_model.device)\n",
    "        return output\n",
    "\n",
    "    return edit_policy\n",
    "\n",
    "def untuple(x):\n",
    "    if(type(x) is tuple):\n",
    "        return x[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0  << original transformer.h.23\n"
     ]
    }
   ],
   "source": [
    "with baukit.TraceDict(\n",
    "    part_model, \n",
    "    part_layer_names,\n",
    "    retain_input=True,\n",
    "    edit_output = replace_first_layer_output(\n",
    "        target = traces[break_layer_name].output\n",
    "    )\n",
    ") as part_traces:\n",
    "    part_outputs = part_model(\n",
    "        input_ids = tokenized_inputs.input_ids.to(part_model.device),\n",
    "        attention_mask = tokenized_inputs.attention_mask.to(part_model.device)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check logit difference\n",
    "torch.dist(outputs.logits, part_outputs.logits.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Washington', ' Seattle', ' the', ' Japan', ' Canada']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = part_outputs.logits.to(model.device)\n",
    "top_k = 5\n",
    "softmax_out = torch.nn.functional.softmax(logits[:, -1, :], dim=1)\n",
    "\n",
    "# Top-k sampling\n",
    "tk = torch.topk(softmax_out, top_k, dim=1).indices\n",
    "[\n",
    "    tokenizer.decode(t) for t in tk[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4000., device='cuda:0', dtype=torch.float16)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check input difference\n",
    "\n",
    "for idx in range(break_layer_idx, mt.num_layers):\n",
    "    orig_input = traces[layer_name_format.format(idx)].input\n",
    "    cur_input = part_traces[layer_name_format.format(idx - break_layer_idx)].input\n",
    "\n",
    "    print(torch.dist(orig_input[0], cur_input[0].to(orig_input[0].device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.23 transformer.h.0\n",
      "tensor(0., device='cuda:0', dtype=torch.float16) tensor(0., device='cuda:0', dtype=torch.float16) tensor(0., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "transformer.h.24 transformer.h.1\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.25 transformer.h.2\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.26 transformer.h.3\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.27 transformer.h.4\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.28 transformer.h.5\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.29 transformer.h.6\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.30 transformer.h.7\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.31 transformer.h.8\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.32 transformer.h.9\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.33 transformer.h.10\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.34 transformer.h.11\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.35 transformer.h.12\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.36 transformer.h.13\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.37 transformer.h.14\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.38 transformer.h.15\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.39 transformer.h.16\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.40 transformer.h.17\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.41 transformer.h.18\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.42 transformer.h.19\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.43 transformer.h.20\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.44 transformer.h.21\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.45 transformer.h.22\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.46 transformer.h.23\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n",
      "transformer.h.47 transformer.h.24\n",
      "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<DistBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check output difference\n",
    "\n",
    "for idx in range(break_layer_idx, mt.num_layers):\n",
    "\n",
    "    original_layer = layer_name_format.format(idx)\n",
    "    target_layer = layer_name_format.format(idx - break_layer_idx)\n",
    "    print(original_layer, target_layer)\n",
    "\n",
    "    orig_output = traces[original_layer].output\n",
    "    cur_output = part_traces[target_layer].output\n",
    "\n",
    "    print(\n",
    "        torch.dist(orig_output[0], cur_output[0].to(orig_output[0].device)),\n",
    "        torch.dist(orig_output[1][0], cur_output[1][0].to(orig_output[0].device)),\n",
    "        torch.dist(orig_output[1][1], cur_output[1][1].to(orig_output[0].device))\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Jacobians"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On a single GPU (<span style=\"color:red\">! will get memory exceed error with `NeoX` !</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_token_index = 3\n",
    "calculate_at_lnf = False\n",
    "consider_residual = False\n",
    "\n",
    "##################################\n",
    "h_layer_idx = 27\n",
    "##################################\n",
    "h_layer_name = layer_name_format.format(h_layer_idx)\n",
    "z_layer_name = mt.layer_names[-1]\n",
    "\n",
    "h = traces[h_layer_name].output[0][0, h_token_index]\n",
    "z = traces[z_layer_name].output[0][0, -1]\n",
    "\n",
    "def compute_z_from_h(h: torch.Tensor) -> torch.Tensor:\n",
    "    def insert_h(output: tuple, layer: str) -> tuple:\n",
    "        if layer != h_layer_name:\n",
    "            return output\n",
    "        # print((output[0][0, h_token_index] - h).norm())\n",
    "        output[0][0, h_token_index] = h\n",
    "        return output\n",
    "\n",
    "    with baukit.TraceDict(\n",
    "        model, (h_layer_name, z_layer_name), edit_output=insert_h\n",
    "    ) as ret:\n",
    "        model(**tokenized_inputs)\n",
    "    # print(z_layer_name, ret[z_layer_name].output[0][-1].shape)\n",
    "    if(calculate_at_lnf == False):\n",
    "        f_h = ret[z_layer_name].output[0][0, -1]\n",
    "    else:\n",
    "        f_h = ret[z_layer_name].output[0][-1]\n",
    "    return f_h - h if consider_residual == True else f_h\n",
    "\n",
    "\n",
    "weight = torch.autograd.functional.jacobian(compute_z_from_h, h, vectorize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using two GPUs (<span style=\"color:green\">! You will want to do this while working with `NeoX` !</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_layer_idx = 27\n",
    "shifted__h_layer_idx = h_layer_idx - (len(mt.layer_names) - len(part_layer_names))\n",
    "shifted__h_layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_token_index = 3\n",
    "calculate_at_lnf = False\n",
    "consider_residual = False\n",
    "\n",
    "first_layer = layer_name_format.format(0)\n",
    "shifted__h_layer_name = layer_name_format.format(shifted__h_layer_idx)\n",
    "shifted__z_layer_name = part_layer_names[-1]\n",
    "h = part_traces[shifted__h_layer_name].output[0][0, h_token_index]\n",
    "z = part_traces[shifted__z_layer_name].output[0][0, -1]\n",
    "\n",
    "def compute_z_from_h(h: torch.Tensor) -> torch.Tensor:\n",
    "    def replace_first_layer_output__and_insert_h(target):\n",
    "        def edit_policy(output, layer_name):\n",
    "            if(layer_name == first_layer):\n",
    "                print(layer_name, \" << original\", break_layer_name)\n",
    "                output[0][...] = target[0].to(part_model.device)\n",
    "                output[1][0][...] = target[1][0].to(part_model.device)\n",
    "                output[1][1][...] = target[1][1].to(part_model.device)\n",
    "            if(layer_name == shifted__h_layer_name):\n",
    "                print(f\"replacing {shifted__h_layer_name} outputs\")\n",
    "                output[0][0, h_token_index] = h\n",
    "            return output\n",
    "        return edit_policy\n",
    "\n",
    "    with baukit.TraceDict(\n",
    "        part_model, \n",
    "        (first_layer, shifted__h_layer_name, shifted__z_layer_name),\n",
    "        edit_output = replace_first_layer_output__and_insert_h(\n",
    "            target = traces[break_layer_name].output\n",
    "        )\n",
    "    ) as ret:\n",
    "        part_model(\n",
    "            input_ids = tokenized_inputs.input_ids.to(part_model.device),\n",
    "            attention_mask = tokenized_inputs.attention_mask.to(part_model.device)\n",
    "        )\n",
    "    if(calculate_at_lnf == False):\n",
    "        f_h = ret[shifted__z_layer_name].output[0][0, -1]\n",
    "    else:\n",
    "        f_h = ret[shifted__z_layer_name].output[0][-1]\n",
    "\n",
    "    return f_h - h if consider_residual == True else f_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0  << original transformer.h.23\n",
      "replacing transformer.h.4 outputs\n",
      "Calculating Jacobians ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:48<00:00, 32.91it/s]\n"
     ]
    }
   ],
   "source": [
    "def calculate_jacobian(function, h):\n",
    "    h.retain_grad()\n",
    "    z_est = function(h)\n",
    "    jacobian = []\n",
    "    print(\"Calculating Jacobians ...\")\n",
    "    for idx in tqdm(range(h.shape[0])):\n",
    "        part_model.zero_grad()\n",
    "        z_est[idx].backward(retain_graph=True)\n",
    "        jacobian.append(copy.deepcopy(h.grad))\n",
    "        h.grad.zero_()\n",
    "    return torch.stack(jacobian)\n",
    "\n",
    "h = part_traces[shifted__h_layer_name].output[0][0, h_token_index]\n",
    "J = calculate_jacobian(compute_z_from_h, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1600, 1600]), torch.Size([1600, 1600]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape, J.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some slight difference when the Jacobian weights are calculated using dual GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0339, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(weight, J.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0008, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(weight[17], J[17].to(model.device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Relation Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    (\"The Space Needle\", -1, \"United States\"),\n",
    "    (\"The Great Wall\", -1, \"China\"),\n",
    "    (\"Niagara Falls\", -2, \"Canada\"),\n",
    "    (\"Valdemarsvik\", -1, \"Sweden\"),\n",
    "    (\"Kyoto University\", -2, \"Japan\"),\n",
    "    (\"Hattfjelldal\", -1, \"Norway\"),\n",
    "    (\"Ginza\", -1, \"Japan\"),\n",
    "    (\"Sydney Hospital\", -2, \"Australia\"),\n",
    "    (\"Mahalangur Himal\", -1, \"Nepal\"),\n",
    "    (\"Higashikagawa\", -1, \"Japan\"),\n",
    "    (\"Trento\", -1, \"Italy\"),\n",
    "    (\"Taj Mahal\", -1, \"India\")\n",
    "]\n",
    "\n",
    "def evaluate_against_test_cases(relation):\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        objects = relation(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {objects}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> will not work with `NeoX` </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_needle_1 = estimate.estimate_relation_operator(\n",
    "    model, tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=h_layer_idx,\n",
    "    device= model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: [' Washington', ' Seattle', ' the', ' Japan', ' Canada']\n",
      "The Great Wall, target: China   ==>   predicted: [' China', ' Hong', ' Beijing', ' Chinese', ' Shen']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Canada', ' Niagara', ' Ontario', ' New', ' Newfoundland']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Iceland', ' Norway', ' Sweden', ' Finland', ' Alaska']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', ' Japanese', ' Tokyo', ' Hawaii', ' Hawai']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Iceland', ' Norway', ' Sweden', ' Denmark', ' Finland']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Tokyo', ' Japanese', ' China', ' Singapore']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Sydney', ' Australian', ' Singapore', ' NSW']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Nepal', ' Bh', ' Tibet', ' India', ' Nep']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' Japanese', ' Tokyo', ' Hok', 'Japan']\n",
      "Trento, target: Italy   ==>   predicted: [' Finland', ' Sweden', ' Iceland', ' Japan', ' Denmark']\n",
      "Taj Mahal, target: India   ==>   predicted: [' Iceland', ' Canada', ' India', ' Finland', ' Bh']\n"
     ]
    }
   ],
   "source": [
    "evaluate_against_test_cases(space_needle_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt >>  The Space Needle is located in the country of\n",
      "h_token_idx >>  3\n",
      "transformer.h.0  << original transformer.h.23\n",
      "replacing transformer.h.4 outputs\n",
      "Calculating Jacobians ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:43<00:00, 37.04it/s]\n"
     ]
    }
   ],
   "source": [
    "space_needle_2 = estimate.estimate_relation_operator_neox(\n",
    "    model, part_model,\n",
    "    tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=h_layer_idx,\n",
    "\n",
    "    layer_name_format = layer_name_format,\n",
    "    final_layer_norm = final_layer_norm,\n",
    "    unembed = unembed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: [' Washington', ' Seattle', ' the', ' Japan', ' Canada']\n",
      "The Great Wall, target: China   ==>   predicted: [' China', ' Hong', ' Beijing', ' Chinese', ' Shen']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Canada', ' Niagara', ' Ontario', ' New', ' Newfoundland']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Iceland', ' Norway', ' Sweden', ' Finland', ' Alaska']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', ' Japanese', ' Tokyo', ' Hawaii', ' Hawai']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Iceland', ' Norway', ' Sweden', ' Denmark', ' Finland']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Tokyo', ' Japanese', ' China', ' Singapore']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Sydney', ' Australian', ' Singapore', ' NSW']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Nepal', ' Bh', ' Tibet', ' India', ' Nep']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' Japanese', ' Tokyo', ' Hok', 'Japan']\n",
      "Trento, target: Italy   ==>   predicted: [' Finland', ' Sweden', ' Iceland', ' Japan', ' Denmark']\n",
      "Taj Mahal, target: India   ==>   predicted: [' Iceland', ' Canada', ' India', ' Finland', ' Bh']\n"
     ]
    }
   ],
   "source": [
    "evaluate_against_test_cases(space_needle_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0339, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(space_needle_1.weight, space_needle_2.weight.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1179, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(space_needle_1.bias, space_needle_2.bias.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jacobian row-wise difference >> 7.987022399902344e-06 +/- 0.0'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_wise_diff = np.array([\n",
    "    torch.dist(space_needle_1.weight[r].to('cpu'), space_needle_2.weight[r].to('cpu'))\n",
    "    for r in range(space_needle_1.weight.shape[0])\n",
    "])\n",
    "\n",
    "f\"Jacobian row-wise difference >> {row_wise_diff.mean()} +/- {row_wise_diff.std()}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the rows in the Jacobian matrices has the same differnece. I don't know if there are any explanation for that.\n",
    "#### However, it doesn't seem like this difference has any effect on the generated tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4104f34302edfbfea7294aa0a5e7d82342a152e8e30f6673f70b28d5f99d4ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
