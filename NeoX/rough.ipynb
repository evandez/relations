{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from util import nethook\n",
    "import baukit\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neox-20b ==> device: cuda:0, memory: 41293685880\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-neox-20b\" # options gpt2-{} | \"EleutherAI/gpt-neox-20b\" | \"EleutherAI/gpt-j-6B\"\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name_format = \"gpt_neox.layers.{}\"\n",
    "final_layer_norm = \"gpt_neox.final_layer_norm\"\n",
    "unembed = \"embed_out\"\n",
    "num_layer_field = \"num_hidden_layers\"\n",
    "break_layer_idx = 23\n",
    "\n",
    "# layer_name_format = \"transformer.h.{}\"\n",
    "# final_layer_norm = \"transformer.ln_f\"\n",
    "# unembed = \"lm_head\"\n",
    "# num_layer_field = \"n_layer\"\n",
    "# break_layer_idx = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy later part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "part_config = copy.deepcopy(model.config)\n",
    "# part_config.n_layer = mt.num_layers - h_layer_idx\n",
    "# part_config.num_hidden_layers = mt.num_layers - h_layer_idx\n",
    "setattr(part_config, num_layer_field, mt.num_layers - break_layer_idx)\n",
    "part_num_layers = getattr(part_config, num_layer_field)\n",
    "\n",
    "part_layer_names = [layer_name_format.format(idx) for idx in range(part_num_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_names(layer_name):\n",
    "    param_names = []\n",
    "    for key in state_dict:\n",
    "        if(key.startswith(layer_name)):\n",
    "            param_names.append(key)\n",
    "    return param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0 << transformer.h.15\n",
      "transformer.h.1 << transformer.h.16\n",
      "transformer.h.2 << transformer.h.17\n",
      "transformer.h.3 << transformer.h.18\n",
      "transformer.h.4 << transformer.h.19\n",
      "transformer.h.5 << transformer.h.20\n",
      "transformer.h.6 << transformer.h.21\n",
      "transformer.h.7 << transformer.h.22\n",
      "transformer.h.8 << transformer.h.23\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "\n",
    "for idx in range(break_layer_idx, mt.num_layers):\n",
    "    part_layer_name = layer_name_format.format(idx - break_layer_idx)\n",
    "    full_layer_name = layer_name_format.format(idx)\n",
    "    \n",
    "    print(part_layer_name, \"<<\", full_layer_name)\n",
    "\n",
    "    part_param_names = get_param_names(part_layer_name)\n",
    "    full_param_names = get_param_names(full_layer_name)\n",
    "    for part_param, full_param in zip(part_param_names, full_param_names):\n",
    "        # print(part_param, full_param)\n",
    "        state_dict[part_param][...] = state_dict[full_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting >>  transformer.h.9\n",
      "deleting >>  transformer.h.10\n",
      "deleting >>  transformer.h.11\n",
      "deleting >>  transformer.h.12\n",
      "deleting >>  transformer.h.13\n",
      "deleting >>  transformer.h.14\n",
      "deleting >>  transformer.h.15\n",
      "deleting >>  transformer.h.16\n",
      "deleting >>  transformer.h.17\n",
      "deleting >>  transformer.h.18\n",
      "deleting >>  transformer.h.19\n",
      "deleting >>  transformer.h.20\n",
      "deleting >>  transformer.h.21\n",
      "deleting >>  transformer.h.22\n",
      "deleting >>  transformer.h.23\n"
     ]
    }
   ],
   "source": [
    "for idx in range(part_num_layers, mt.num_layers):\n",
    "    layer_name = layer_name_format.format(idx)\n",
    "    print(\"deleting >> \", layer_name)\n",
    "    param_names = get_param_names(layer_name)\n",
    "    for param in param_names:\n",
    "        state_dict.pop(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save later part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = f\"{MODEL_NAME}__last_{part_num_layers}_layers\"\n",
    "os.makedirs(path_name, exist_ok = True)\n",
    "\n",
    "part_config.save_pretrained(path_name)\n",
    "torch.save(state_dict, f\"{path_name}/pytorch_model.bin\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load later part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 21 ==> device: cuda:1, memory: 20356239906\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# path_name = \"gpt2-medium__last_9_layers\"\n",
    "path_name = \"EleutherAI/gpt-neox-20b__last_21_layers\"\n",
    "\n",
    "part_model = AutoModelForCausalLM.from_pretrained(\n",
    "                path_name, low_cpu_mem_usage=True, torch_dtype=torch.float16\n",
    "            )\n",
    "part_model = part_model.eval().cuda('cuda:1')\n",
    "\n",
    "part_layer_names = [layer_name_format.format(idx) for idx in range(getattr(part_model.config, num_layer_field))]\n",
    "print(f\"last {getattr(part_model.config, num_layer_field)} ==> device: {part_model.device}, memory: {part_model.get_memory_footprint()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Equivalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle is located in the country of\n",
      "The Space Needle is located in the country of Washington, in the city of Seattle. The Space\n",
      "p(answer):  p(' Washington'[5041])=0.2216, p(' Seattle'[16335])=0.1621, p(' the'[253])=0.1522, p(' Malaysia'[23799])=0.0182, p(' Canada'[6144])=0.0171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"The Space Needle is located in the country of\"]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompt,\n",
    "    argmax_greedy = True,\n",
    "    max_out_len= 20,\n",
    "    # debug=True,\n",
    "    get_answer_tokens=True,\n",
    ")\n",
    "model_utils.print_formatted_results(prompt, txt, ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    prompt,\n",
    "    padding = True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(next(model.parameters()).device)\n",
    "\n",
    "break_layer_name = layer_name_format.format(break_layer_idx)\n",
    "z_layer_name = layer_name_format.format(mt.num_layers-1)\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    model, \n",
    "    mt.layer_names, # [h_layer_name, z_layer_name], \n",
    "    retain_input=True\n",
    ") as traces:\n",
    "    outputs = model(**tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_first_layer_output(target):\n",
    "    first_layer = layer_name_format.format(0)\n",
    "    def edit_policy(output, layer_name):\n",
    "        if(layer_name != first_layer):\n",
    "            return output\n",
    "        print(layer_name, \" << original\", break_layer_name)\n",
    "        output[0][...] = target[0].to(part_model.device)\n",
    "        output[1][0][...] = target[1][0].to(part_model.device)\n",
    "        output[1][1][...] = target[1][1].to(part_model.device)\n",
    "        return output\n",
    "\n",
    "    return edit_policy\n",
    "\n",
    "def check_valid(module_name, prefix = \"transformer.h\", start_layer = 1):\n",
    "    if(module_name in [final_layer_norm, unembed]):\n",
    "        return True\n",
    "    for idx in range(start_layer, mt.num_layers):\n",
    "        if(module_name.startswith(f\"{prefix}.{idx}\")):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def untuple(x):\n",
    "    if(type(x) is tuple):\n",
    "        return x[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_gradients = {\n",
    "    n: p\n",
    "    for n, p in part_model.named_parameters()\n",
    "    if check_valid(\n",
    "        n, prefix = layer_name_format[:-3]\n",
    "    )\n",
    "}\n",
    "\n",
    "for n, w in part_model.named_parameters():\n",
    "    if(n in need_gradients):\n",
    "        w.requires_grad = True\n",
    "    else:   \n",
    "        w.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.layers.0  << original gpt_neox.layers.23\n"
     ]
    }
   ],
   "source": [
    "with baukit.TraceDict(\n",
    "    part_model, \n",
    "    part_layer_names,\n",
    "    retain_input=True,\n",
    "    edit_output = replace_first_layer_output(\n",
    "        target = traces[break_layer_name].output\n",
    "    )\n",
    ") as part_traces:\n",
    "    part_outputs = part_model(\n",
    "        input_ids = tokenized_inputs.input_ids.to(part_model.device),\n",
    "        attention_mask = tokenized_inputs.attention_mask.to(part_model.device)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.dist(outputs.logits, part_outputs.logits.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = part_outputs.logits.to(model.device)\n",
    "# top_k = 5\n",
    "# softmax_out = torch.nn.functional.softmax(logits[:, -1, :], dim=1)\n",
    "\n",
    "# # Top-k sampling\n",
    "# tk = torch.topk(softmax_out, top_k, dim=1).indices\n",
    "# [\n",
    "#     tokenizer.decode(t) for t in tk[0]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check input difference\n",
    "\n",
    "# for idx in range(break_layer_idx, mt.num_layers):\n",
    "#     orig_input = traces[layer_name_format.format(idx)].input\n",
    "#     cur_input = part_traces[layer_name_format.format(idx - break_layer_idx)].input\n",
    "\n",
    "#     print(torch.dist(orig_input[0], cur_input[0].to(orig_input[0].device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check output difference\n",
    "\n",
    "# for idx in range(break_layer_idx, mt.num_layers):\n",
    "\n",
    "#     original_layer = layer_name_format.format(idx)\n",
    "#     target_layer = layer_name_format.format(idx - break_layer_idx)\n",
    "#     print(original_layer, target_layer)\n",
    "\n",
    "#     orig_output = traces[original_layer].output\n",
    "#     cur_output = part_traces[target_layer].output\n",
    "\n",
    "#     print(\n",
    "#         torch.dist(orig_output[0], cur_output[0].to(orig_output[0].device)),\n",
    "#         torch.dist(orig_output[1][0], cur_output[1][0].to(orig_output[0].device)),\n",
    "#         torch.dist(orig_output[1][1], cur_output[1][1].to(orig_output[0].device))\n",
    "#     )\n",
    "#     print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Jacobians"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On a single device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_token_index = 3\n",
    "calculate_at_lnf = False\n",
    "consider_residual = False\n",
    "\n",
    "##################################\n",
    "h_layer_idx = 17\n",
    "##################################\n",
    "h_layer_name = layer_name_format.format(h_layer_idx)\n",
    "z_layer_name = mt.layer_names[-1]\n",
    "\n",
    "h = traces[h_layer_name].output[0][0, h_token_index]\n",
    "z = traces[z_layer_name].output[0][0, -1]\n",
    "\n",
    "def compute_z_from_h(h: torch.Tensor) -> torch.Tensor:\n",
    "    def insert_h(output: tuple, layer: str) -> tuple:\n",
    "        if layer != h_layer_name:\n",
    "            return output\n",
    "        # print((output[0][0, h_token_index] - h).norm())\n",
    "        output[0][0, h_token_index] = h\n",
    "        return output\n",
    "\n",
    "    with baukit.TraceDict(\n",
    "        model, (h_layer_name, z_layer_name), edit_output=insert_h\n",
    "    ) as ret:\n",
    "        model(**tokenized_inputs)\n",
    "    # print(z_layer_name, ret[z_layer_name].output[0][-1].shape)\n",
    "    if(calculate_at_lnf == False):\n",
    "        f_h = ret[z_layer_name].output[0][0, -1]\n",
    "    else:\n",
    "        f_h = ret[z_layer_name].output[0][-1]\n",
    "    return f_h - h if consider_residual == True else f_h\n",
    "\n",
    "\n",
    "weight = torch.autograd.functional.jacobian(compute_z_from_h, h, vectorize=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On 2 cuda devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_layer_idx = 27\n",
    "shifted__h_layer_idx = h_layer_idx - (len(mt.layer_names) - len(part_layer_names))\n",
    "shifted__h_layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_token_index = 3\n",
    "calculate_at_lnf = False\n",
    "consider_residual = False\n",
    "\n",
    "first_layer = layer_name_format.format(0)\n",
    "shifted__h_layer_name = layer_name_format.format(shifted__h_layer_idx)\n",
    "shifted__z_layer_name = part_layer_names[-1]\n",
    "h = part_traces[shifted__h_layer_name].output[0][0, h_token_index]\n",
    "z = part_traces[shifted__z_layer_name].output[0][0, -1]\n",
    "\n",
    "def compute_z_from_h(h: torch.Tensor) -> torch.Tensor:\n",
    "    def replace_first_layer_output(target):\n",
    "        def edit_policy(output, layer_name):\n",
    "            if(layer_name == first_layer):\n",
    "                print(layer_name, \" << original\", break_layer_name)\n",
    "                output[0][...] = target[0].to(part_model.device)\n",
    "                output[1][0][...] = target[1][0].to(part_model.device)\n",
    "                output[1][1][...] = target[1][1].to(part_model.device)\n",
    "            if(layer_name == shifted__h_layer_name):\n",
    "                print(f\"replacing {shifted__h_layer_name} outputs\")\n",
    "                output[0][0, h_token_index] = h\n",
    "            return output\n",
    "        return edit_policy\n",
    "\n",
    "    with baukit.TraceDict(\n",
    "        part_model, \n",
    "        (first_layer, shifted__h_layer_name, shifted__z_layer_name),\n",
    "        edit_output = replace_first_layer_output(\n",
    "            target = traces[break_layer_name].output\n",
    "        )\n",
    "    ) as ret:\n",
    "        part_model(\n",
    "            input_ids = tokenized_inputs.input_ids.to(part_model.device),\n",
    "            attention_mask = tokenized_inputs.attention_mask.to(part_model.device)\n",
    "        )\n",
    "    if(calculate_at_lnf == False):\n",
    "        f_h = ret[shifted__z_layer_name].output[0][0, -1]\n",
    "    else:\n",
    "        f_h = ret[shifted__z_layer_name].output[0][-1]\n",
    "\n",
    "    return f_h - h if consider_residual == True else f_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_weight = torch.autograd.functional.jacobian(compute_z_from_h, h, vectorize=True) \n",
    "# torch.dist(weight, part_weight.to(weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.layers.0  << original gpt_neox.layers.23\n",
      "replacing gpt_neox.layers.4 outputs\n",
      "Calculating Jacobians ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6144/6144 [19:59<00:00,  5.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def calculate_jacobian(function, h):\n",
    "    h.retain_grad()\n",
    "    z_est = function(h)\n",
    "    jacobian = []\n",
    "    print(\"Calculating Jacobians ...\")\n",
    "    for idx in tqdm(range(h.shape[0])):\n",
    "        part_model.zero_grad()\n",
    "        z_est[idx].backward(retain_graph=True)\n",
    "        jacobian.append(copy.deepcopy(h.grad))\n",
    "        h.grad.zero_()\n",
    "    return torch.stack(jacobian)\n",
    "\n",
    "h = part_traces[shifted__h_layer_name].output[0][0, h_token_index]\n",
    "J = calculate_jacobian(compute_z_from_h, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0050, device='cuda:1', dtype=torch.float16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.dist(part_weight, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt >>  The Space Needle is located in the country of\n",
      "h_token_idx >>  3\n",
      "gpt_neox.layers.0  << original gpt_neox.layers.23\n",
      "replacing gpt_neox.layers.4 outputs\n",
      "Calculating Jacobians ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6144/6144 [17:05<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "space_needle = estimate.estimate_relation_operator_neox(\n",
    "    model, part_model,\n",
    "    tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=27,\n",
    "    layer_name_format= layer_name_format,\n",
    "    num_layer_field= num_layer_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(space_needle.weight, J.to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    layer = space_needle.layer,\n",
    "    relation=\"{} is located in the country of\",\n",
    "    weight = space_needle.weight,\n",
    "    bias = space_needle.bias,\n",
    "    calculated_at_lnf= calculate_at_lnf,\n",
    "    consider_residual = consider_residual,\n",
    "\n",
    "    layer_name_format = layer_name_format,\n",
    "    final_layer_norm = final_layer_norm,\n",
    "    unembed = unembed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: [' Washington', ' Seattle', ' the', ' Malaysia', ' Canada']\n",
      "The Great Wall, target: China   ==>   predicted: [' Seattle', ' Washington', ' the', '\\n', ' Japan']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Seattle', ' Washington', ' the', ' Japan', ' glass']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Seattle', ' Washington', ' the', ' Japan', ' glass']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', ' Seattle', ' Washington', ' the', ' Tokyo']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Seattle', ' Washington', ' the', ' glass', ' Japan']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Seattle', ' Washington', ' the', ' Tokyo']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Seattle', ' Washington', ' the', '\\n', ' origin']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Seattle', ' Washington', ' the', '\\n', ' glass']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' Seattle', ' Washington', ' the', ' Tokyo']\n",
      "Trento, target: Italy   ==>   predicted: [' Seattle', ' Washington', ' the', ' glass', '\\n']\n",
      "Taj Mahal, target: India   ==>   predicted: [' Seattle', ' Washington', ' the', ' Japan', ' Malaysia']\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    (\"The Space Needle\", -1, \"United States\"),\n",
    "    (\"The Great Wall\", -1, \"China\"),\n",
    "    (\"Niagara Falls\", -2, \"Canada\"),\n",
    "    (\"Valdemarsvik\", -1, \"Sweden\"),\n",
    "    (\"Kyoto University\", -2, \"Japan\"),\n",
    "    (\"Hattfjelldal\", -1, \"Norway\"),\n",
    "    (\"Ginza\", -1, \"Japan\"),\n",
    "    (\"Sydney Hospital\", -2, \"Australia\"),\n",
    "    (\"Mahalangur Himal\", -1, \"Nepal\"),\n",
    "    (\"Higashikagawa\", -1, \"Japan\"),\n",
    "    (\"Trento\", -1, \"Italy\"),\n",
    "    (\"Taj Mahal\", -1, \"India\")\n",
    "]\n",
    "\n",
    "for subject, subject_token_index, target in test_cases:\n",
    "    objects = relation(\n",
    "        subject,\n",
    "        subject_token_index=subject_token_index,\n",
    "        device=model.device,\n",
    "        return_top_k=5,\n",
    "    )\n",
    "    print(f\"{subject}, target: {target}   ==>   predicted: {objects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jh_norm': 107.25,\n",
       " 'bias_norm': 895.0,\n",
       " 'h_info': {'h_index': 3, 'token_id': 282, 'token': 'le'},\n",
       " 'consider_residual': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_needle.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4104f34302edfbfea7294aa0a5e7d82342a152e8e30f6673f70b28d5f99d4ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
