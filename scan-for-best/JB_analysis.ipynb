{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from dsets.counterfact import CounterFactDataset\n",
    "from util import nethook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_f = nethook.get_module(model, \"transformer.ln_f\")\n",
    "lm_head = nethook.get_module(model, \"lm_head\")\n",
    "encoder = nethook.get_module(model, \"transformer.wte\")\n",
    "\n",
    "def get_vocab_representation(h, perform_layer_norm = True, return_top_k = 5):\n",
    "    z = h.clone()\n",
    "    if(perform_layer_norm == True):\n",
    "        z = ln_f(z)\n",
    "    logits = lm_head(z)\n",
    "    token_ids = logits.topk(dim=-1, k=return_top_k).indices.squeeze().tolist()\n",
    "    return [\n",
    "        tokenizer.decode(t) for t in token_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    (\"The Space Needle\", -1, \"United States\"),\n",
    "    (\"The Great Wall\", -1, \"China\"),\n",
    "    (\"Niagara Falls\", -2, \"Canada\"),\n",
    "    (\"Valdemarsvik\", -1, \"Sweden\"),\n",
    "    (\"Kyoto University\", -2, \"Japan\"),\n",
    "    (\"Hattfjelldal\", -1, \"Norway\"),\n",
    "    (\"Ginza\", -1, \"Japan\"),\n",
    "    (\"Sydney Hospital\", -2, \"Australia\"),\n",
    "    (\"Mahalangur Himal\", -1, \"Nepal\"),\n",
    "    (\"Higashikagawa\", -1, \"Japan\"),\n",
    "    (\"Trento\", -1, \"Italy\"),\n",
    "    (\"Taj Mahal\", -1, \"India\"),\n",
    "    (\"Hagia Sophia\", -1, \"Turkey\"),\n",
    "    (\"Colosseum\", -1, \"Italy\"),\n",
    "    (\"Mount Everest\", -1, \"Nepal\"),\n",
    "    (\"Valencia\", -1, \"Spain\"),\n",
    "    (\"Lake Baikal\", -1, \"Russia\")\n",
    "]\n",
    "\n",
    "def evaluate_against_test_cases(relation):\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        objects = relation(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {objects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Seattle', ' Washington', ' the', ' Sweden', ' Canada']\n"
     ]
    }
   ],
   "source": [
    "# good cases\n",
    "\n",
    "space_needle = estimate.estimate_relation_operator(\n",
    "    model, tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=15,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "print(get_vocab_representation(space_needle.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: [' Seattle', ' Washington', ' the', ' Sweden', ' America']\n",
      "The Great Wall, target: China   ==>   predicted: [' China', ' Seattle', ' Beijing', ' Taiwan', ' Japan']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Niagara', ' Canada', 'Ni', ' New', ' Ontario']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Sweden', ' Seattle', ' Washington', ' Scandinav', ' Swedish']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Kyoto', ' Japan', ' Osaka', ' Tokyo', ' Japanese']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Norway', ' Sweden', ' Scandinav', ' Denmark', ' Washington']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Tokyo', ' Seattle', ' Osaka', ' Shin']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Sydney', ' Australian', ' Queensland', ' New']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Nepal', ' Seattle', ' Washington', ' Bh', ' Switzerland']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' Tokyo', ' Seattle', ' Osaka', ' Kyoto']\n",
      "Trento, target: Italy   ==>   predicted: [' Italy', ' Seattle', ' Washington', ' Sweden', ' Switzerland']\n",
      "Taj Mahal, target: India   ==>   predicted: [' Seattle', ' Washington', ' the', ' Sultan', ' Wa']\n",
      "Hagia Sophia, target: Turkey   ==>   predicted: [' Turkey', ' Seattle', ' Istanbul', ' Sweden', ' Ankara']\n",
      "Colosseum, target: Italy   ==>   predicted: [' Italy', ' Seattle', ' Rome', ' the', ' Venice']\n",
      "Mount Everest, target: Nepal   ==>   predicted: [' Mount', ' Nepal', ' Mt', ' Seattle', ' Switzerland']\n",
      "Valencia, target: Spain   ==>   predicted: [' Spain', ' Valencia', ' Barcelona', ' Spanish', ' Ecuador']\n",
      "Lake Baikal, target: Russia   ==>   predicted: [' Russia', ' Seattle', ' Sweden', ' Washington', ' the']\n"
     ]
    }
   ],
   "source": [
    "evaluate_against_test_cases(space_needle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_relations = np.load(\n",
    "    \"gpt-j/P17/cached_JB/jacobian_calculations__all_sub_toks__layer_15___0_to_100.npz\",\n",
    "    allow_pickle=True\n",
    ")[\"jacobians\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calculated_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Jh_norm', 'bias_norm', 'h_info', 'consider_residual'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculated_relations[0]['all_weights_and_biases'][-1]['misc'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Autonomous University of Madrid, {}, which is located in, Spain)\n",
      "[' Spain', ' Madrid', ' the', ' And', ' Cast']\n",
      "(Kuala Langat, {}, located in, Malaysia)\n",
      "[' Malaysia', ' Pen', ' Sel', ' Sab', ' Mal']\n",
      "(Wanne-Eickel Central Station, {}, located in, Germany)\n",
      "[' Germany', ' Han', ' North', ' Bad', ' Sch']\n",
      "(Bastille, {}, which is located in, France)\n",
      "[' France', ' the', ' Belgium', ' Luxembourg', ' Brittany']\n",
      "(Shablykinsky District, {} is located in the country of, Russia)\n",
      "[' Russia', ' Ukraine', ' Kazakhstan', ' Belarus', ' the']\n",
      "(Valdemarsvik, {}, which is located in, Sweden)\n",
      "[' Sweden', ' Latvia', ' S', ' J', ' Estonia']\n",
      "(Attingal, {}, which is located in, India)\n",
      "[' India', ' Kerala', ' Go', ' And', ' Karn']\n",
      "(Nizampatnam, {} is located in the country of, India)\n",
      "[' And', ' India', ' Od', ' Tel', ' Tamil']\n",
      "(Darmstadt, {} is located in the country of, Germany)\n",
      "[' Germany', ' H', ' the', ' Bad', ' North']\n",
      "(Adliswil, {}, which is located in, Switzerland)\n",
      "[' Switzerland', ' the', ' Cant', ' Z', ' Luc']\n",
      "(Canada Live, {}, which is located in, Canada)\n",
      "[' Canada', ' the', ' Quebec', ' Saskatchewan', ' Alberta']\n",
      "(Knockmealdown Mountains, {}, in, Ireland)\n",
      "[' Ireland', ' Northern', ' the', ' South', ' New']\n",
      "(Gazzola, {} is located in the country of, Italy)\n",
      "[' Italy', ' the', ' P', ' Trent', ' T']\n",
      "(Borovsky District, {} is located in, Russia)\n",
      "[' Russia', ' Ukraine', ' Kazakhstan', ' Mord', ' the']\n",
      "(Kyoto University, {} is located in the country of, Japan)\n",
      "[' Japan', ' Kyoto', ' Ky', ' the', ' H']\n",
      "(HSBC Bank Malta, {}, located in, Malta)\n",
      "[' Malta', ' Malt', ' Gibraltar', ' the', ' Cyprus']\n",
      "(Chu Lai Base Area, {} is located in the country of, Vietnam)\n",
      "[' Vietnam', ' Laos', ' China', ' the', ' Thailand']\n",
      "(National Highway 45, {} is located in the country of, India)\n",
      "[' India', ' Nepal', ' Bangladesh', ' Pakistan', ' the']\n",
      "(Kfar Giladi, {} is located in, Israel)\n",
      "[' Israel', ' the', ' Palestine', ' Jordan', ' E']\n",
      "(Vijayawada Junction railway station, {} is located in, India)\n",
      "[' India', ' And', ' Od', ' the', ' Tel']\n",
      "(Imsland, {} is located in, Norway)\n",
      "[' Norway', ' Denmark', ' the', ' S', ' Iceland']\n",
      "(Dwarka, {}, which is located in, India)\n",
      "[' India', ' Gujarat', ' the', ' Bh', ' Pakistan']\n",
      "(Hultsfred Municipality, {} is located in, Sweden)\n",
      "[' Sweden', ' V', ' Sc', ' Sk', ' S']\n",
      "(Hattfjelldal, {} is located in, Norway)\n",
      "[' Norway', ' Nord', ' S', ' Tr', ' Finn']\n",
      "(Russian-Armenian University, {}, located in, Armenia)\n",
      "[' Armenia', ' Georgia', ' the', ' Azerbaijan', ' Russia']\n",
      "(Ustevatn, {}, in, Norway)\n",
      "[' Norway', ' Iceland', ' S', ' the', ' Denmark']\n",
      "(community development block in India, {}, located in, India)\n",
      "[' India', ' the', ' Maharashtra', ' Gujarat', ' West']\n",
      "(Astrid Lindgren Memorial Award, {}, which is located in, Sweden)\n",
      "[' Sweden', ' the', ' her', ' Finland', ' origin']\n",
      "(Turkey national rugby union team, {} is located in, Turkey)\n",
      "[' Turkey', ' the', ' its', ' Georgia', ' Albania']\n",
      "(Nova Friburgo, {}, which is located in, Brazil)\n",
      "[' Brazil', ' Santa', ' the', ' Rio', ' Rib']\n",
      "(Sydney Peace Prize, {} is located in, Australia)\n",
      "[' Australia', ' the', ' its', ' Sweden', ' Austria']\n",
      "(Sarpsborg 08 FF, {} is located in, Norway)\n",
      "[' Norway', ' S', ' the', ' Sar', ' Sweden']\n",
      "(Yokohama International School, {}, which is located in, Japan)\n",
      "[' Japan', ' Singapore', ' the', ' South', ' New']\n",
      "(Giurgiu County, {}, in, Romania)\n",
      "[' Romania', ' Mold', ' Trans', ' the', ' Wall']\n",
      "(Ballydehob, {} is located in the country of, Ireland)\n",
      "[' Ireland', ' Cork', ' Co', ' County', ' West']\n",
      "(Maybach, {} is located in, Germany)\n",
      "[' Germany', ' the', ' Georgia', ' Bav', ' South']\n",
      "(Ateneo de Madrid, {}, in, Spain)\n",
      "[' Spain', ' Madrid', ' the', ' And', ' La']\n",
      "(Toronto Police Service, {}, in, Canada)\n",
      "[' Canada', ' Ontario', ' Toronto', ' the', ' North']\n",
      "(Ginza, {}, in, Japan)\n",
      "[' Japan', ' Tokyo', ' Ch', ' the', ' Shin']\n",
      "(2004 Indian Ocean earthquake, {} is located in, Indonesia)\n",
      "[' Maurit', ' Indonesia', ' the', ' Com', ' Madagascar']\n",
      "(Carpentras, {}, in, France)\n",
      "[' France', ' the', ' V', ' H', ' Switzerland']\n",
      "(Lower Assam, {} is located in the country of, India)\n",
      "[' India', ' Ass', ' North', ' Bangladesh', ' Northeast']\n",
      "(Hellenic Police, {}, which is located in, Greece)\n",
      "[' Greece', ' Cyprus', ' the', ' Macedonia', '\\n']\n",
      "(Kamikita District, {} is located in the country of, Japan)\n",
      "[' Japan', ' T', ' Hok', ' the', ' Nag']\n",
      "(Clonlara GAA, {}, which is located in, Ireland)\n",
      "[' Ireland', ' Clare', ' County', ' Kerry', ' Kil']\n",
      "(Bobolice, {}, which is located in, Poland)\n",
      "[' Poland', ' Slovakia', ' the', ' Czech', ' Slovenia']\n",
      "(Sakarya Province, {} is located in the country of, Turkey)\n",
      "[' Turkey', ' the', ' Anat', '\\n', ' western']\n",
      "(Inami, Toyama, {}, located in, Japan)\n",
      "[' Japan', ' Toy', ' the', ' Hok', ' A']\n",
      "(Iximche, {}, located in, Guatemala)\n",
      "[' Guatemala', ' the', ' Mexico', ' Chi', ' Mich']\n",
      "(Thuringian Highland, {}, located in, Germany)\n",
      "[' Germany', ' Th', ' the', ' Bad', ' Bav']\n",
      "(Kalvola, {}, in, Finland)\n",
      "[' Finland', ' Estonia', ' La', ' the', ' Latvia']\n",
      "(Bilbao, {} is located in the country of, Spain)\n",
      "[' Spain', ' the', ' Bas', ' B', ' Cant']\n",
      "(Peremyshliany, {}, in, Ukraine)\n",
      "[' Ukraine', ' Belarus', ' Poland', ' the', ' Kazakhstan']\n",
      "(Etobicoke North, {}, located in, Canada)\n",
      "[' Canada', ' Ontario', ' Toronto', ' Et', ' the']\n",
      "(Lund Municipality, {} is located in, Sweden)\n",
      "[' Sweden', ' Denmark', ' Norway', ' S', ' Sk']\n",
      "(Borgo San Martino, {}, located in, Italy)\n",
      "[' Italy', ' San', ' T', ' the', ' P']\n",
      "(Dumka district, {}, located in, India)\n",
      "[' Od', ' India', ' the', ' J', ' West']\n",
      "(Eredivisie, {}, in, Netherlands)\n",
      "[' Netherlands', ' the', ' Holland', ' The', ' Belgium']\n",
      "(Pannonhalma Archabbey, {}, which is located in, Hungary)\n",
      "[' Hungary', ' P', ' Croatia', ' Slovenia', ' the']\n",
      "(Kenya African National Union, {} is located in, Kenya)\n",
      "[' Kenya', ' N', ' the', ' Tanzania', '\\n']\n",
      "(Banesh Rural District, {} is located in, Iran)\n",
      "[' Iran', ' Kh', ' Kurdistan', ' Azerbaijan', ' Bal']\n",
      "(Sydney Hospital, {}, located in, Australia)\n",
      "[' Australia', ' New', ' the', ' Sydney', ' NSW']\n",
      "(Cork GAA, {} is located in, Ireland)\n",
      "[' Ireland', ' Cork', ' Mun', ' the', ' County']\n",
      "(Hohenwestedt, {}, in, Germany)\n",
      "[' Bad', ' Germany', ' Sch', ' the', ' North']\n",
      "(Kibungo Province, {}, located in, Rwanda)\n",
      "[' Rwanda', ' Bur', ' Uganda', ' Tanzania', ' the']\n",
      "(Harnaut, {} is located in, India)\n",
      "[' India', ' the', ' And', ' Ukraine', ' H']\n",
      "(Westerburg, {} is located in, Germany)\n",
      "[' Bad', ' Germany', ' Wester', ' the', ' Bav']\n",
      "(Zulia, {} is located in the country of, Venezuela)\n",
      "[' Venezuela', ' South', ' the', ' Colombia', ' Argentina']\n",
      "(Imperia, {}, which is located in, Italy)\n",
      "[' Italy', ' the', ' San', ' T', ' Sard']\n",
      "(Kungliga Svenska Segelsällskapet, {}, in, Sweden)\n",
      "[' Sweden', ' Got', ' Finland', ' Stockholm', ' the']\n",
      "(Medical College and Hospital, Kolkata, {}, in, India)\n",
      "[' India', ' West', ' Bengal', ' the', ' K']\n",
      "(Kahrizak, {}, located in, Iran)\n",
      "[' Iran', ' Azerbaijan', ' Afghanistan', ' the', ' Georgia']\n",
      "(Bouygues Telecom, {} is located in, France)\n",
      "[' France', ' the', ' Belgium', ' Luxembourg', ' Brittany']\n",
      "(Mahalangur Himal, {} is located in, Nepal)\n",
      "[' Nepal', ' India', ' Bh', ' Utt', ' Sik']\n",
      "(Marugame, {}, located in, Japan)\n",
      "[' Japan', ' Nag', ' Kag', ' the', ' T']\n",
      "(Pesisir Selatan, {}, located in, Indonesia)\n",
      "[' Indonesia', ' Malaysia', ' Brune', ' Sab', ' South']\n",
      "(Lismore GAA, {} is located in, Ireland)\n",
      "[' Ireland', ' Australia', ' County', ' Clare', ' New']\n",
      "(Kanchanaburi, {} is located in, Thailand)\n",
      "[' Thailand', ' the', ' western', '\\n', ' South']\n",
      "(Mediolanum Forum, {} is located in the country of, Italy)\n",
      "[' Italy', ' the', ' San', ' Milan', ' Switzerland']\n",
      "(Saint-Jacques-de-la-Lande, {}, which is located in, France)\n",
      "[' France', ' the', ' Brittany', ' Quebec', ' Qué']\n",
      "(SAM Colombia, {}, located in, Colombia)\n",
      "[' Colombia', ' Columbia', ' the', ' Ecuador', ' South']\n",
      "(Hyderabad Deccan railway station, {}, located in, India)\n",
      "[' India', ' And', ' Tel', ' Hyder', ' the']\n",
      "(Peruvian Navy, {}, located in, Peru)\n",
      "[' Peru', ' the', ' Per', ' South', '\\n']\n",
      "(Higashikagawa, {} is located in the country of, Japan)\n",
      "[' Japan', ' Nag', ' Wak', ' Sh', ' T']\n",
      "(Mirbag-e Shomali Rural District, {} is located in the country of, Iran)\n",
      "[' Iran', ' Azerbaijan', ' Maz', ' Gil', ' Afghanistan']\n",
      "(Cocconato, {}, which is located in, Italy)\n",
      "[' Italy', ' San', ' the', ' P', ' Sicily']\n",
      "(Al-Suqaylabiyah Subdistrict, {}, which is located in, Syria)\n",
      "[' Syria', ' Iraq', ' Jordan', ' the', ' Saudi']\n",
      "(Pontigny Abbey, {}, which is located in, France)\n",
      "[' France', ' the', ' Burg', ' R', ' Cant']\n",
      "(JVC Kenwood Victor Entertainment, {} is located in the country of, Japan)\n",
      "[' Japan', ' the', ' New', ' Canada', ' J']\n",
      "(Kaniv Reservoir, {} is located in the country of, Ukraine)\n",
      "[' Ukraine', ' Belarus', ' Georgia', ' Kazakhstan', ' the']\n",
      "(Hardoi district, {} is located in, India)\n",
      "[' India', ' Iran', ' Nepal', ' Maharashtra', ' the']\n",
      "(Sidhi district, {}, in, India)\n",
      "[' India', ' Gujarat', ' Karn', ' Maharashtra', ' Utt']\n",
      "(Santo Stefano d'Aveto, {}, in, Italy)\n",
      "[' Italy', ' the', ' T', ' San', ' P']\n",
      "(Piove di Sacco, {}, which is located in, Italy)\n",
      "[' Italy', ' Trent', ' the', ' P', ' Fri']\n",
      "(Bahujan Vikas Aaghadi, {} is located in the country of, India)\n",
      "[' India', ' Pakistan', ' Nepal', ' Uttar', ' Bangladesh']\n",
      "(Sommariva del Bosco, {}, in, Italy)\n",
      "[' Italy', ' the', ' P', ' San', ' T']\n",
      "(Grobbendonk, {}, which is located in, Belgium)\n",
      "[' Belgium', ' F', ' the', ' Ze', ' Ant']\n",
      "(Stentoften Runestone, {}, which is located in, Sweden)\n",
      "[' Sweden', ' Denmark', ' the', ' Norway', ' Iceland']\n",
      "(Alte Nationalgalerie, {}, in, Germany)\n",
      "[' Germany', ' the', ' Berlin', ' Bad', ' art']\n",
      "(Constitution of Afghanistan, {}, in, Afghanistan)\n",
      "[' Afghanistan', ' the', '\\n', ' Pakistan', ' Kabul']\n"
     ]
    }
   ],
   "source": [
    "for relation_collention in calculated_relations:\n",
    "    relation_operator = relation_collention['all_weights_and_biases'][-1]\n",
    "    print(f\"({relation_collention['subject']}, {relation_collention['request']['prompt']}, {relation_collention['request']['target_true']['str']})\")\n",
    "    bias = torch.tensor(relation_operator[\"bias\"], device=model.device)\n",
    "    print(get_vocab_representation(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wall, target: China   ==>   predicted: [' China', ' Hong', ' Beijing', ' Chinese', ' Shen']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Canada', ' Ontario', ' Niagara', ' New', ' British']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Iceland', ' Denmark', ' Sweden', ' Finland', ' Norway']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', ' Japanese', ' Finland', ' Hawaii', ' Tokyo']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Iceland', ' Denmark', ' Norway', ' Sweden', ' Finland']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Singapore', ' China', ' Seattle', ' Hong']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Sydney', ' Australian', ' Singapore', ' Canberra']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Nepal', ' Tibet', ' Bh', ' Nep', ' China']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' Japanese', ' Tokyo', ' Canada', ' Seattle']\n",
      "Trento, target: Italy   ==>   predicted: [' Sweden', ' Finland', ' Iceland', ' Denmark', ' Luxembourg']\n",
      "Taj Mahal, target: India   ==>   predicted: [' Iceland', ' Canada', ' Finland', ' Norway', ' New']\n"
     ]
    }
   ],
   "source": [
    "check_with_test_cases(space_needle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Nam', ' Beau', ' Krug', ' Macron', ' Cham']\n"
     ]
    }
   ],
   "source": [
    "space_needle_2 = estimate.estimate_relation_operator(\n",
    "    model, tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=25,\n",
    "    device=model.device,\n",
    "    calculate_at_lnf= True\n",
    ")\n",
    "\n",
    "print(get_vocab_representation(space_needle_2.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wall, target: China   ==>   predicted: [' China', ' Shen', ' Chinese', ' Qing', ' Beijing']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Niagara', 'Toronto', ' Ontario', ' Cuomo', ' Erie']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Nordic', ' Greenland', 'vik', ' Swedish', ' Icelandic']\n",
      "Kyoto University, target: Japan   ==>   predicted: ['Japanese', ' Japanese', 'Tok', 'Japan', ' Japan']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Nordic', ' Denmark', ' Iceland', ' Scandinavian', ' Icelandic']\n",
      "Ginza, target: Japan   ==>   predicted: [' Tokyo', 'Tok', ' Japan', 'Japan', ' Japanese']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Sydney', ' NSW', ' Australia', ' Australian', 'Australia']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Himal', ' Tibetan', ' Nepal', ' Nep', ' Tibet']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japanese', 'Japanese', ' Japan', ' Tokyo', 'Tok']\n",
      "Trento, target: Italy   ==>   predicted: ['meta', ' Budapest', ' Contra', 'Italy', ' Reno']\n",
      "Taj Mahal, target: India   ==>   predicted: [' Icelandic', ' Niagara', ' Iceland', ' sculpture', ' Hudson']\n"
     ]
    }
   ],
   "source": [
    "check_with_test_cases(space_needle_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Australia', ' the', ' New', ' Ireland', ' South']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "menangle = estimate.estimate_relation_operator(\n",
    "    model, tokenizer,\n",
    "    \"Menangle Park\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=25,\n",
    "    device=model.device,\n",
    "    subject_token_index = 1\n",
    ")\n",
    "\n",
    "get_vocab_representation(menangle.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wall, target: China   ==>   predicted: [' China', ' India', ' Hong', ' the', ' Z']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Newfoundland', ' Ontario', ' New', ' Ireland', ' Canada']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Australia', ' New', ' the', ' Canada', ' Bh']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' India', ' Bh', ' Nepal', ' Thailand', ' Sri']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Ireland', ' Scotland', ' the', ' Australia', ' Wales']\n",
      "Ginza, target: Japan   ==>   predicted: [' India', ' Australia', ' Hong', ' Japan', ' Bh']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Sydney', ' Queensland', ' NSW', ' New']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Bh', ' Nepal', ' India', ' Sri', ' Kashmir']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' India', ' New', ' Australia', ' the']\n",
      "Trento, target: Italy   ==>   predicted: [' New', ' Australia', ' Ireland', ' the', ' France']\n",
      "Taj Mahal, target: India   ==>   predicted: [' India', ' Bh', ' Sri', ' Ireland', ' Nepal']\n"
     ]
    }
   ],
   "source": [
    "check_with_test_cases(menangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ACT', ' NSW', ' Sydney', ' Brisbane', ' Sutherland']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "menangle_2 = estimate.estimate_relation_operator(\n",
    "    model, tokenizer,\n",
    "    \"Menangle Park\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=25,\n",
    "    device=model.device,\n",
    "    subject_token_index = 1,\n",
    "    calculate_at_lnf= True\n",
    ")\n",
    "\n",
    "get_vocab_representation(menangle_2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wall, target: China   ==>   predicted: ['\\x0b', '�', '\\r', '龍�', '\\x0f']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Niagara', '\\r', '�', '�', '\\x0b']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: ['�', '\\x0b', '�', '\\r', '龍�']\n",
      "Kyoto University, target: Japan   ==>   predicted: ['�', '\\x0b', '\\r', '龍�', '�']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: ['�', '\\r', '�', '\\x0b', '\\x11']\n",
      "Ginza, target: Japan   ==>   predicted: ['\\x0b', '�', '\\r', '�', '龍�']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Sydney', ' NSW', '�', '\\x0b', '\\r']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Himal', ' Nepal', ' Bh', ' Hindu', ' Kashmir']\n",
      "Higashikagawa, target: Japan   ==>   predicted: ['�', '\\x0b', '\\r', '�', 'rawdownload']\n",
      "Trento, target: Italy   ==>   predicted: ['�', '\\x0b', '\\r', '\\x1c', '�']\n",
      "Taj Mahal, target: India   ==>   predicted: ['�', '\\r', '\\x0b', '\\x08', '龍�']\n"
     ]
    }
   ],
   "source": [
    "check_with_test_cases(menangle_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(269.9868, device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(space_needle.bias - menangle.bias).norm()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference with USUAL calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = f\"transformer.h.{model.config.n_layer-1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Space Needle is located in the city of\"\n",
    "\n",
    "tokenized = tokenizer(prompt, padding=True, return_tensors=\"pt\").to(\n",
    "    next(model.parameters()).device\n",
    ")\n",
    "\n",
    "with nethook.TraceDict(\n",
    "    model, [final_layer, \"transformer.ln_f\"]\n",
    ") as traces:  \n",
    "    output = model(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Seattle', ' Seat', ' the', ' Portland', ' Space']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_representation(traces[final_layer].output[0][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Seattle', ' Seat', ' the', ' Portland', ' Space']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_representation(traces[\"transformer.ln_f\"].output[0][-1], perform_layer_norm= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Seattle', ' Seat', ' the', ' Portland', ' Space']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = output.logits[:, -1, :]\n",
    "token_ids = logits.topk(dim=-1, k=5).indices.squeeze().tolist()\n",
    "[tokenizer.decode(t) for t in token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Wall, target: China\n",
      "z_ =  [' China', ' G', ' Shan', ' the', ' Great']\n",
      "z_est =  [' China', ' Seattle', ' Beijing', ' Taiwan', ' Japan']\n",
      "Distance =>  169.125\n",
      "\n",
      "Niagara Falls, target: Canada\n",
      "z_ =  [' Canada', ' Ontario', ' New', ' the', ' Niagara']\n",
      "z_est =  [' Niagara', ' Canada', 'Ni', ' New', ' Ontario']\n",
      "Distance =>  180.25\n",
      "\n",
      "Valdemarsvik, target: Sweden\n",
      "z_ =  [' Sweden', ' S', ' J', ' Latvia', ' V']\n",
      "z_est =  [' Sweden', ' Seattle', ' Washington', ' Scandinav', ' Swedish']\n",
      "Distance =>  179.25\n",
      "\n",
      "Kyoto University, target: Japan\n",
      "z_ =  [' Japan', ' Kyoto', ' Ky', ' the', ' H']\n",
      "z_est =  [' Kyoto', ' Japan', ' Osaka', ' Tokyo', ' Japanese']\n",
      "Distance =>  163.25\n",
      "\n",
      "Hattfjelldal, target: Norway\n",
      "z_ =  [' Norway', ' Nord', ' S', ' Tr', ' Finn']\n",
      "z_est =  [' Norway', ' Sweden', ' Scandinav', ' Denmark', ' Washington']\n",
      "Distance =>  189.75\n",
      "\n",
      "Ginza, target: Japan\n",
      "z_ =  [' Japan', ' Tokyo', ' the', ' Ch', ' Shin']\n",
      "z_est =  [' Japan', ' Tokyo', ' Seattle', ' Osaka', ' Shin']\n",
      "Distance =>  150.75\n",
      "\n",
      "Sydney Hospital, target: Australia\n",
      "z_ =  [' Australia', ' New', ' the', ' Sydney', ' NSW']\n",
      "z_est =  [' Australia', ' Sydney', ' Australian', ' Queensland', ' New']\n",
      "Distance =>  161.5\n",
      "\n",
      "Mahalangur Himal, target: Nepal\n",
      "z_ =  [' Nepal', ' India', ' Bh', ' Sik', ' Utt']\n",
      "z_est =  [' Nepal', ' Seattle', ' Washington', ' Bh', ' Switzerland']\n",
      "Distance =>  200.375\n",
      "\n",
      "Higashikagawa, target: Japan\n",
      "z_ =  [' Japan', ' Nag', ' Wak', ' Sh', ' the']\n",
      "z_est =  [' Japan', ' Tokyo', ' Seattle', ' Osaka', ' Kyoto']\n",
      "Distance =>  166.0\n",
      "\n",
      "Trento, target: Italy\n",
      "z_ =  [' Trent', ' Italy', ' the', ' Fri', ' Ven']\n",
      "z_est =  [' Italy', ' Seattle', ' Washington', ' Sweden', ' Switzerland']\n",
      "Distance =>  186.625\n",
      "\n",
      "Taj Mahal, target: India\n",
      "z_ =  [' India', ' Pakistan', ' Uttar', ' the', ' Bangladesh']\n",
      "z_est =  [' Seattle', ' Washington', ' the', ' Sultan', ' Wa']\n",
      "Distance =>  172.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subject, subject_token_index, target in test_cases:\n",
    "    objects, z_est = space_needle(\n",
    "        subject,\n",
    "        subject_token_index=subject_token_index,\n",
    "        device=model.device,\n",
    "        return_top_k=5,\n",
    "    )\n",
    "    print(f\"{subject}, target: {target}\")\n",
    "\n",
    "    prompt = \"{} is located in the country of\".format(subject)\n",
    "    tokenized = tokenizer(prompt, padding=True, return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    with nethook.TraceDict(\n",
    "        model, [final_layer, \"transformer.ln_f\"]\n",
    "    ) as traces:  \n",
    "        output = model(**tokenized)\n",
    "    z = traces[final_layer].output[0][0][-1]\n",
    "\n",
    "    print(\"z_ = \", get_vocab_representation(z))\n",
    "    print(\"z_est = \", get_vocab_representation(z_est))\n",
    "    print(\"Distance => \", torch.dist(z, z_est).item())\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which contributes the most? $Jh$ or $bias$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jacobian_calculations__all_sub_toks__layer_15___200_to_300.npz',\n",
       " 'jacobian_calculations__all_sub_toks__layer_15___300_to_400.npz',\n",
       " 'jacobian_calculations__all_sub_toks__layer_15___400_to_404.npz',\n",
       " 'jacobian_calculations__all_sub_toks__layer_15___100_to_200.npz',\n",
       " 'jacobian_calculations__all_sub_toks__layer_15___0_to_100.npz']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"gpt-j/P17/cached_JB/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -->  jacobian_calculations__all_sub_toks__layer_15___200_to_300.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:39<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -->  jacobian_calculations__all_sub_toks__layer_15___300_to_400.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:40<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -->  jacobian_calculations__all_sub_toks__layer_15___400_to_404.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:08<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -->  jacobian_calculations__all_sub_toks__layer_15___100_to_200.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:41<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading -->  jacobian_calculations__all_sub_toks__layer_15___0_to_100.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:40<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "jh_collection = []\n",
    "bias_collection = []\n",
    "rank_collection = []\n",
    "\n",
    "path = \"gpt-j/P17/cached_JB/\"\n",
    "cached_jb_files = os.listdir(path)\n",
    "\n",
    "for jb_file in cached_jb_files:\n",
    "    print(\"loading --> \", jb_file)\n",
    "    calculated_relations = np.load(f\"{path}{jb_file}\", allow_pickle=True)[\"jacobians\"]\n",
    "    for relation_collention in tqdm(calculated_relations):\n",
    "        relation_operator = relation_collention['all_weights_and_biases'][-1]\n",
    "        jh_collection.append(relation_operator['misc']['Jh_norm'])\n",
    "        bias_collection.append(relation_operator['misc']['bias_norm'])\n",
    "        J = torch.tensor(relation_operator[\"weight\"], device=model.device, dtype=torch.float32)\n",
    "        rank_collection.append(torch.linalg.matrix_rank(J).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "jh_collection = np.array(jh_collection)\n",
    "bias_collection = np.array(bias_collection)\n",
    "rank_collection = np.array(rank_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.675278465346535 +/- 8.163360430617857'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{jh_collection.mean()} +/- {jh_collection.std()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'254.11355198019803 +/- 23.25887094834684'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{bias_collection.mean()} +/- {bias_collection.std()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3788.05198019802 +/- 115.54177064337186'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{rank_collection.mean()} +/- {rank_collection.std()}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good case vs Bad cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (s, o, h_idx)\n",
    "relation_format = '{} is located in the country of'\n",
    "good_cases = [\n",
    "    ('Haut Atlas', 'Morocco', 2),\n",
    "    ('Pamukkale', 'Turkey', 4),\n",
    "    ('Fort Madalena', 'Malta', 3),\n",
    "    ('Umarex', 'Germany', 2),\n",
    "    ('Qatar Ladies Open', 'Qatar', 2),\n",
    "    ('Sydney Peace Prize', 'Australia', 4),\n",
    "    ('Alte Nationalgalerie', 'Germany', 4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:29<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "good_relations = []\n",
    "for subject, object, h_idx in tqdm(good_cases):\n",
    "    relation = estimate.estimate_relation_operator(\n",
    "        model, tokenizer,\n",
    "        subject, relation_format,\n",
    "        subject_token_index= h_idx,\n",
    "        layer = 15,\n",
    "        device= model.device\n",
    "    )\n",
    "    good_relations.append(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [r.weight for r in good_relations]\n",
    "bias = [r.bias for r in good_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.188 [0.0, 38.375, 21.688, 18.078, 23.047, 21.859, 26.297]\n",
      "39.75 [38.375, 0.0, 36.188, 38.625, 38.406, 40.906, 38.406]\n",
      "20.594 [21.688, 36.188, 0.0, 21.422, 23.797, 24.375, 26.688]\n",
      "15.312 [18.078, 38.625, 21.422, 0.0, 22.859, 19.344, 24.812]\n",
      "23.0 [23.047, 38.406, 23.797, 22.859, 0.0, 25.266, 27.953]\n",
      "18.828 [21.859, 40.906, 24.375, 19.344, 25.266, 0.0, 27.016]\n",
      "26.5 [26.297, 38.406, 26.688, 24.812, 27.953, 27.016, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for one in range(len(weights)):\n",
    "    w1 = weights[one]\n",
    "    dist = [round(torch.dist(w1, w2).item(),3) for w2 in weights]\n",
    "    print(round(w1.norm().item(), 3), dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269.75 [0.0, 178.0, 92.938, 92.312, 107.188, 131.125, 137.125]\n",
      "230.5 [178.0, 0.0, 174.125, 173.5, 186.375, 188.25, 172.75]\n",
      "253.25 [92.938, 174.125, 0.0, 102.312, 115.125, 131.375, 139.5]\n",
      "247.25 [92.312, 173.5, 102.312, 0.0, 107.188, 124.312, 122.062]\n",
      "274.25 [107.188, 186.375, 115.125, 107.188, 0.0, 134.625, 150.25]\n",
      "246.0 [131.125, 188.25, 131.375, 124.312, 134.625, 0.0, 137.125]\n",
      "216.75 [137.125, 172.75, 139.5, 122.062, 150.25, 137.125, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for one in range(len(bias)):\n",
    "    w1 = bias[one]\n",
    "    dist = [round(torch.dist(w1, w2).item(),3) for w2 in bias]\n",
    "    print(round(w1.norm().item(), 3), dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 33, 5, 80, 64, 69, 39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 93.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "calculated_relations = np.load(\n",
    "    \"gpt-j/P17/cached_JB/jacobian_calculations__all_sub_toks__layer_15___0_to_100.npz\",\n",
    "    allow_pickle=True\n",
    ")[\"jacobians\"]\n",
    "\n",
    "random_indices = random.choices(range(0, len(calculated_relations)), k = 7)\n",
    "print(random_indices)\n",
    "\n",
    "weights = []\n",
    "bias = []\n",
    "\n",
    "for idx in tqdm(random_indices):\n",
    "    relation_collention = calculated_relations[idx]\n",
    "    relation_operator = relation_collention['all_weights_and_biases'][-1]\n",
    "    weights.append(torch.tensor(relation_operator[\"weight\"], device=model.device, dtype=torch.float32))\n",
    "    bias.append(torch.tensor(relation_operator[\"bias\"], device=model.device, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.549 [0.0, 17.949, 19.386, 27.599, 17.965, 16.597, 23.482]\n",
      "9.679 [17.949, 0.0, 17.015, 27.033, 10.177, 10.039, 19.596]\n",
      "16.475 [19.386, 17.015, 0.0, 28.162, 17.053, 15.313, 22.612]\n",
      "27.394 [27.599, 27.033, 28.162, 0.0, 27.206, 26.912, 31.171]\n",
      "8.184 [17.965, 10.177, 17.053, 27.206, 0.0, 8.961, 18.904]\n",
      "5.596 [16.597, 10.039, 15.313, 26.912, 8.961, 0.0, 18.207]\n",
      "18.04 [23.482, 19.596, 22.612, 31.171, 18.904, 18.207, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for one in range(len(weights)):\n",
    "    w1 = weights[one]\n",
    "    dist = [round(torch.dist(w1, w2).item(),3) for w2 in weights]\n",
    "    print(round(w1.norm().item(), 3), dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.816 [0.0, 161.95, 138.882, 134.235, 178.476, 129.226, 172.651]\n",
      "223.301 [161.95, 0.0, 163.701, 160.271, 165.099, 167.165, 176.24]\n",
      "259.589 [138.882, 163.701, 0.0, 160.631, 180.087, 109.962, 180.799]\n",
      "273.735 [134.235, 160.271, 160.631, 0.0, 173.806, 157.031, 169.659]\n",
      "237.356 [178.476, 165.099, 180.087, 173.806, 0.0, 181.295, 173.081]\n",
      "250.456 [129.226, 167.165, 109.962, 157.031, 181.295, 0.0, 174.918]\n",
      "240.872 [172.651, 176.24, 180.799, 169.659, 173.081, 174.918, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for one in range(len(bias)):\n",
    "    w1 = bias[one]\n",
    "    dist = [round(torch.dist(w1, w2).item(),3) for w2 in bias]\n",
    "    print(round(w1.norm().item(), 3), dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing $J$ as *identity*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usual computation of the jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del space_needle\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# sos = (\n",
    "#     (\"The Great Wall\", -1, \"China\"),\n",
    "#     (\"Niagara Falls\", -2, \"Canada\"),\n",
    "#     (\"Ginza\", -1, \"Japan\"),\n",
    "# )\n",
    "\n",
    "sos = (\n",
    "    (\"Haut Atlas\", -1, \"Morocco\"),\n",
    "    (\"Pamukkale\", -1, \"Turkey\"),\n",
    "    (\"Fort Madalena\", -1, \"Malta\"),\n",
    "    # (\"Umarex\", -1, \"Germany\"),\n",
    "    (\"Alte Nationalgalerie\", -1, \"Germany\")\n",
    ")\n",
    "\n",
    "r = \"{} is located in the country of\"\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1a804b5e7341ceb662956edda7ed9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  Haut Atlas\n",
      "Alte Nationalgalerie is located in the country of Germany.\n",
      "Fort Madalena is located in the country of Malta.\n",
      "Pamukkale is located in the country of Turkey.\n",
      "{} is located in the country of\n",
      "tensor(32.1250, device='cuda:0', dtype=torch.float16) tensor(259.2500, device='cuda:0', dtype=torch.float16)\n",
      "subject:  Pamukkale\n",
      "Haut Atlas is located in the country of Morocco.\n",
      "Fort Madalena is located in the country of Malta.\n",
      "Alte Nationalgalerie is located in the country of Germany.\n",
      "{} is located in the country of\n",
      "tensor(36.9688, device='cuda:0', dtype=torch.float16) tensor(222.3750, device='cuda:0', dtype=torch.float16)\n",
      "subject:  Fort Madalena\n",
      "Haut Atlas is located in the country of Morocco.\n",
      "Alte Nationalgalerie is located in the country of Germany.\n",
      "Pamukkale is located in the country of Turkey.\n",
      "{} is located in the country of\n",
      "tensor(23.2656, device='cuda:0', dtype=torch.float16) tensor(271., device='cuda:0', dtype=torch.float16)\n",
      "subject:  Alte Nationalgalerie\n",
      "Haut Atlas is located in the country of Morocco.\n",
      "Fort Madalena is located in the country of Malta.\n",
      "Pamukkale is located in the country of Turkey.\n",
      "{} is located in the country of\n",
      "tensor(27.1250, device='cuda:0', dtype=torch.float16) tensor(231.6250, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "jbs = []\n",
    "for s, s_idx, o in tqdm(sos):\n",
    "    others = set(sos) - {(s, s_idx, o)}\n",
    "    prompt = \"\"\n",
    "    prompt += \"\\n\".join(r.format(s_other) + f\" {o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "    prompt += r\n",
    "    print(\"subject: \", s)\n",
    "    print(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    jb = estimate.estimate_relation_operator(\n",
    "        model, tokenizer,\n",
    "        s, prompt,\n",
    "        subject_token_index= s_idx,\n",
    "        layer = 15,\n",
    "        device = model.device,\n",
    "    )\n",
    "    print(jb.weight.norm(), jb.bias.norm())\n",
    "    print()\n",
    "    jbs.append(jb)\n",
    "\n",
    "relation = estimate.RelationOperator(\n",
    "    weight=torch.stack([jb.weight for jb in jbs]).mean(dim=0),\n",
    "    bias=torch.stack([jb.bias for jb in jbs]).mean(dim=0),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer= 15 ,\n",
    "    relation=r,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: [' Turkey', ' Sweden', ' Switzerland', ' Belgium', ' United']\n",
      "The Great Wall, target: China   ==>   predicted: [' China', ' Taiwan', ' Turkey', ' Germany', ' Mongolia']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Canada', ' Austria', ' Switzerland', ' Italy', ' Niagara']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Sweden', ' Iceland', ' Finland', ' Norway', ' Denmark']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', ' Kyoto', ' Turkey', ' China', ' Thailand']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' Norway', ' Denmark', ' Sweden', ' Iceland', ' Turkey']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Turkey', ' China', ' Russia', ' Germany']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Sydney', ' Australian', ' NSW', 'Australia']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' Nepal', ' Switzerland', ' India', ' Bh', ' Pakistan']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Japan', ' Turkey', ' China', ' Germany', ' Italy']\n",
      "Trento, target: Italy   ==>   predicted: [' Italy', ' Austria', ' Turkey', ' Switzerland', ' Croatia']\n",
      "Taj Mahal, target: India   ==>   predicted: [' India', ' Turkey', ' Pakistan', ' Oman', ' Morocco']\n",
      "Hagia Sophia, target: Turkey   ==>   predicted: [' Turkey', ' Greece', ' Bulgaria', ' Germany', ' Cyprus']\n",
      "Colosseum, target: Italy   ==>   predicted: [' Italy', ' Turkey', ' Greece', ' Malta', ' Spain']\n",
      "Mount Everest, target: Nepal   ==>   predicted: [' Nepal', ' Switzerland', ' Tibet', ' China', ' Austria']\n",
      "Valencia, target: Spain   ==>   predicted: [' Spain', ' Portugal', ' Valencia', ' Turkey', ' Venezuela']\n",
      "Lake Baikal, target: Russia   ==>   predicted: [' Russia', ' Turkey', ' Kazakhstan', ' Mongolia', ' Germany']\n"
     ]
    }
   ],
   "source": [
    "evaluate_against_test_cases(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged ==>  [' Turkey', ' Malta', ' Malaysia', ' Switzerland', ' Luxembourg', ' Australia', ' Germany', ' France', ' South', ' Cyprus']\n",
      "['Merlion Park is located in the country of Singapore, and is a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'top_token': ' Singapore',\n",
       "  'candidates': [{'token': ' Singapore', 'token_id': 12551, 'p': 0.2999},\n",
       "   {'token': ' Malaysia', 'token_id': 15336, 'p': 0.1231},\n",
       "   {'token': ' Brune', 'token_id': 46070, 'p': 0.102},\n",
       "   {'token': ' Pen', 'token_id': 7507, 'p': 0.0654},\n",
       "   {'token': ' Sab', 'token_id': 9910, 'p': 0.0517},\n",
       "   {'token': ' the', 'token_id': 262, 'p': 0.0187},\n",
       "   {'token': ' Cambodia', 'token_id': 31057, 'p': 0.0145},\n",
       "   {'token': ' Joh', 'token_id': 16053, 'p': 0.0121},\n",
       "   {'token': ' Bel', 'token_id': 3944, 'p': 0.0114},\n",
       "   {'token': ' Per', 'token_id': 2448, 'p': 0.0113}]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subject = \"Merlion Park\"\n",
    "# test_subject = \"Cape Sounion\"\n",
    "# test_subject = \"Zagori\"\n",
    "# test_subject = \"Cologne Cathedral\"\n",
    "# test_subject = \"Buda Castle\"\n",
    "\n",
    "\n",
    "# print(\"space_needle ==> \", space_needle(test_subject, device = model.device, return_top_k = 10))\n",
    "print(\"Averaged ==> \", relation(test_subject, device = model.device, return_top_k = 10))\n",
    "\n",
    "generated_text, gen_dict = model_utils.generate_fast(\n",
    "    model, tokenizer, prompts= r.format(test_subject), \n",
    "    argmax_greedy=True, get_answer_tokens=True,\n",
    "    max_new_tokens=5, top_k= 10\n",
    ")\n",
    "\n",
    "print(generated_text)\n",
    "gen_dict['answer']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace J as identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Seattle', ' Washington', ' the', ' Sweden', ' Canada']\n"
     ]
    }
   ],
   "source": [
    "space_needle_id = estimate.estimate_relation_operator(\n",
    "    model, tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=15,\n",
    "    device=model.device,\n",
    "    test_identity = True\n",
    ")\n",
    "\n",
    "print(get_vocab_representation(space_needle.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: [' Seattle', ' Washington', ' the', ' Sweden', ' America']\n",
      "The Great Wall, target: China   ==>   predicted: [' the', ' America', ' China', ' Canada', ' Se']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Canada', ' Queen', ' the', ' America', ' Washington']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: [' Sweden', ' the', ' Se', ' Canada', ' Sk']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', ' Se', ' France', ' Sweden', ' the']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: [' the', ' Se', ' Sweden', ' Sk', ' �']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', ' Se', ' the', ' America', ' France']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Se', ' the', ' France', ' King']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: [' the', ' Se', '\\n', ' Canada', ' France']\n",
      "Higashikagawa, target: Japan   ==>   predicted: [' Se', ' Japan', ' the', ' Canada', ' T']\n",
      "Trento, target: Italy   ==>   predicted: [' the', ' Se', ' Italy', ' France', ' Canada']\n",
      "Taj Mahal, target: India   ==>   predicted: [' the', ' King', ' Se', ' Queen', ' Canada']\n",
      "Hagia Sophia, target: Turkey   ==>   predicted: [' the', ' Se', '\\n', ' King', ' Sultan']\n",
      "Colosseum, target: Italy   ==>   predicted: [' the', ' King', ' America', ' France', ' Italy']\n",
      "Mount Everest, target: Nepal   ==>   predicted: [' the', ' Canada', ' Se', ' my', ' America']\n",
      "Valencia, target: Spain   ==>   predicted: [' France', ' Se', ' Canada', ' the', ' Sweden']\n",
      "Lake Baikal, target: Russia   ==>   predicted: [' the', ' Se', ' Russia', ' Canada', '\\n']\n"
     ]
    }
   ],
   "source": [
    "evaluate_against_test_cases(space_needle_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0dde66e2954f21be5a58b566ba064c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ginza is located in the country of Japan.\n",
      "Niagara Falls is located in the country of Canada.\n",
      "{} is located in the country of\n",
      "Ginza is located in the country of Japan.\n",
      "The Great Wall is located in the country of China.\n",
      "{} is located in the country of\n",
      "The Great Wall is located in the country of China.\n",
      "Niagara Falls is located in the country of Canada.\n",
      "{} is located in the country of\n"
     ]
    }
   ],
   "source": [
    "jbs = []\n",
    "for s, s_idx, o in tqdm(sos):\n",
    "    others = set(sos) - {(s, s_idx, o)}\n",
    "    prompt = \"\"\n",
    "    prompt += \"\\n\".join(r.format(s_other) + f\" {o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "    prompt += r\n",
    "    print(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    jb = estimate.estimate_relation_operator(\n",
    "        model, tokenizer,\n",
    "        s, prompt,\n",
    "        subject_token_index= s_idx,\n",
    "        layer = 15,\n",
    "        device = model.device,\n",
    "        test_identity = True\n",
    "    )\n",
    "    jbs.append(jb)\n",
    "\n",
    "relation_id = estimate.RelationOperator(\n",
    "    weight=torch.stack([jb.weight for jb in jbs]).mean(dim=0),\n",
    "    bias=torch.stack([jb.bias for jb in jbs]).mean(dim=0),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer= 15 ,\n",
    "    relation=r,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Space Needle, target: United States   ==>   predicted: ['\\n', ' the', '...', ' Canada', ' Japan']\n",
      "The Great Wall, target: China   ==>   predicted: [' China', '\\n', ' the', ' Japan', ' Canada']\n",
      "Niagara Falls, target: Canada   ==>   predicted: [' Canada', '\\n', ' United', ' the', ' Japan']\n",
      "Valdemarsvik, target: Sweden   ==>   predicted: ['\\n', ' the', ' Canada', ' United', ' ']\n",
      "Kyoto University, target: Japan   ==>   predicted: [' Japan', '\\n', ' United', ' Canada', ' the']\n",
      "Hattfjelldal, target: Norway   ==>   predicted: ['\\n', ' the', ' United', ' ', ' Canada']\n",
      "Ginza, target: Japan   ==>   predicted: [' Japan', '\\n', ' the', ' United', ' China']\n",
      "Sydney Hospital, target: Australia   ==>   predicted: [' Australia', ' Canada', ' United', '\\n', ' the']\n",
      "Mahalangur Himal, target: Nepal   ==>   predicted: ['\\n', ' the', '...', ' ', ' Canada']\n",
      "Higashikagawa, target: Japan   ==>   predicted: ['\\n', ' Japan', ' Canada', ' the', ' ']\n",
      "Trento, target: Italy   ==>   predicted: ['\\n', ' the', ' United', ' Canada', ' Japan']\n",
      "Taj Mahal, target: India   ==>   predicted: ['\\n', ' the', ' Canada', '...', ' China']\n",
      "Hagia Sophia, target: Turkey   ==>   predicted: ['\\n', ' the', ' ', '...', ' The']\n",
      "Colosseum, target: Italy   ==>   predicted: ['\\n', ' the', ' Canada', '...', ' ']\n",
      "Mount Everest, target: Nepal   ==>   predicted: ['\\n', ' the', ' Canada', ' China', ' United']\n",
      "Valencia, target: Spain   ==>   predicted: ['\\n', ' United', ' Canada', ' the', ' China']\n",
      "Lake Baikal, target: Russia   ==>   predicted: ['\\n', ' the', ' Canada', ' Russia', ' ']\n"
     ]
    }
   ],
   "source": [
    "evaluate_against_test_cases(relation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50400, 4096])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "\n",
    "\n",
    "def check_distance_with_bias(relation_operator, test_subjects):\n",
    "    print(get_vocab_representation(relation.bias))\n",
    "    for test_subject in test_subjects:\n",
    "        print(test_subject)\n",
    "        prompt = relation_operator.relation.format(test_subject)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        z_layername = f\"transformer.h.{model.config.n_layer - 1}\"\n",
    "        with baukit.TraceDict(model, [z_layername]) as traces:\n",
    "            outputs = model(**inputs)\n",
    "        repr = traces[z_layername].output[0][0][-1][None]\n",
    "        print(torch.dist(relation.bias, repr).item(), get_vocab_representation(repr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Turkey', ' Germany', ' France', ' Portugal', ' Italy']\n",
      "Taj Mahal\n",
      "153.5 [' India', ' Pakistan', ' Uttar', ' the', ' Bangladesh']\n",
      "\n",
      "Hagia Sophia\n",
      "135.625 [' Turkey', ' Greece', ' Istanbul', ' the', ' modern']\n",
      "\n",
      "Colosseum\n",
      "125.8125 [' Italy', ' the', ' San', ' Romania', ' Rome']\n",
      "\n",
      "Mount Everest\n",
      "169.375 [' Nepal', ' China', ' the', ' Tibet', ' Bh']\n",
      "\n",
      "Valencia\n",
      "134.25 [' Spain', ' Valencia', ' And', ' the', ' Venezuela']\n",
      "\n",
      "Lake Baikal\n",
      "171.375 [' Russia', ' Siberia', ' Mongolia', ' the', ' Ir']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "successes = [\n",
    "    \"Taj Mahal\", \"Hagia Sophia\", \"Colosseum\",\n",
    "    \"Mount Everest\", \"Valencia\", \"Lake Baikal\",\n",
    "]\n",
    "\n",
    "check_distance_with_bias(relation, successes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Turkey', ' Germany', ' France', ' Portugal', ' Italy']\n",
      "Merlion Park\n",
      "143.875 [' Singapore', ' Malaysia', ' Brune', ' Pen', ' Sab']\n",
      "\n",
      "Cape Sounion\n",
      "152.375 [' Greece', ' Att', ' the', ' Cre', ' Pel']\n",
      "\n",
      "Zagori\n",
      "152.75 [' Georgia', ' Macedonia', ' Bulgaria', ' Greece', ' the']\n",
      "\n",
      "Cologne Cathedral\n",
      "135.25 [' Germany', ' North', ' Cologne', ' Bad', ' H']\n",
      "\n",
      "Buda Castle\n",
      "134.5 [' Hungary', ' Romania', ' the', ' Slovakia', ' Trans']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failures = [\"Merlion Park\", \"Cape Sounion\", \"Zagori\", \"Cologne Cathedral\", \"Buda Castle\"]\n",
    "\n",
    "check_distance_with_bias(relation, failures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan through all the `GPT-J` correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"gpt-j/P17/correct_prediction_P17.json\") as f:\n",
    "    correct_predictions = json.load(f)\n",
    "len(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Autonomous University of Madrid'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions[0]['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '{}, located in',\n",
       " 'relation_id': 'P17',\n",
       " 'target_new': {'str': 'Switzerland', 'id': 'Q39'},\n",
       " 'target_true': {'str': 'Germany', 'id': 'Q183'},\n",
       " 'subject': 'Wanne-Eickel Central Station'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f358258e5844cdb16e205a49f10c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_layername = f\"transformer.h.{model.config.n_layer - 1}\"\n",
    "precision_at = 3\n",
    "successes_dist = []\n",
    "failures_dist = []\n",
    "\n",
    "for c in tqdm(correct_predictions):\n",
    "    test_subject = c['subject']\n",
    "    # print(f\"s = {test_subject}, o = {c['target_true']['str']}\")\n",
    "    prompt = relation.relation.format(test_subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with baukit.TraceDict(model, [z_layername]) as traces:\n",
    "        outputs = model(**inputs)\n",
    "    repr = traces[z_layername].output[0][0][-1][None]\n",
    "    \n",
    "    dist = torch.dist(relation.bias, repr).item()\n",
    "    model_predicted = get_vocab_representation(repr)\n",
    "    # print(dist)\n",
    "    # print(\"Usual Computation: \", model_predicted)\n",
    "    jb_predicted = relation(\n",
    "        test_subject, subject_token_index=-1, device=model.device,\n",
    "    )\n",
    "    # print(\"Jacobian-bias: \", jb_predicted)\n",
    "    # print()\n",
    "    if(model_predicted[0] in jb_predicted[0:precision_at]):\n",
    "        successes_dist.append(dist)\n",
    "    else:\n",
    "        failures_dist.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((310,), (94,))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successes_dist = np.array(successes_dist)\n",
    "failures_dist = np.array(failures_dist)\n",
    "\n",
    "successes_dist.shape, failures_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147.34012096774194"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successes_dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146.6130319148936"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures_dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3439fe3f7dcaddaf51997811d25ada8e7c0985d2997d22a3ed461af94d2f9f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
