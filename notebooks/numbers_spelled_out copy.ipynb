{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from baukit import nethook\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B ==> device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"numbers_spelled_out.tsv\", delimiter='\\t')\n",
    "objects = list(df['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eleven ===>  ten\n",
      "twelve ===>  eleven\n",
      "thirteen ===>  twelve\n",
      "fourteen ===>  thirteen\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"three comes after two\n",
    "six comes after five\n",
    "{} comes after\"\"\"\n",
    "\n",
    "# prompt = \"\"\"grape ends with E\n",
    "# monitor ends with R\n",
    "# glass ends with\"\"\"\n",
    "\n",
    "words = [ 'eleven', 'twelve', 'thirteen', 'fourteen']\n",
    "\n",
    "for w in words:\n",
    "    txt, ret_dict = model_utils.generate_fast(\n",
    "        model, tokenizer, \n",
    "        prompts=[prompt.format(w)], max_new_tokens=10, \n",
    "        get_answer_tokens=True, argmax_greedy=True\n",
    "    )\n",
    "    print(f\"{w} ===> {ret_dict['answer'][0]['top_token']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [\" \" + o for o in df['words']]\n",
    "\n",
    "from relations.corner import CornerEstimator\n",
    "corner_estimator = CornerEstimator(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.5 [' twelve', ' seven', ' fourteen', ' fifteen', ' six']\n"
     ]
    }
   ],
   "source": [
    "simple_corner = corner_estimator.estimate_simple_corner(objects, scale_up=70)\n",
    "print(simple_corner.norm().item(), corner_estimator.get_vocab_representation(simple_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_inv_corner = corner_estimator.estimate_lin_inv_corner(objects, target_logit_value=50)\n",
    "# print(lin_inv_corner.norm().item(), corner_estimator.get_vocab_representation(lin_inv_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_corner = corner_estimator.estimate_average_corner_with_gradient_descent(objects, average_on=5, target_logit_value=50, verbose=False)\n",
    "# print(avg_corner.norm().item(), corner_estimator.get_vocab_representation(avg_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_test_cases(relation_operator):\n",
    "\n",
    "    test_cases = [\n",
    "        (objects[i+1], -1, objects[i]) for i in range(len(objects) - 1)\n",
    "    ]\n",
    "\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        answer = relation_operator(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " two, target:  one   ==>   predicted: [' seven', ' twelve', ' eight', ' six', ' nine']\n",
      " three, target:  two   ==>   predicted: [' twelve', ' twenty', ' ten', ' seven', ' fourteen']\n",
      " four, target:  three   ==>   predicted: [' twelve', ' seven', ' ten', ' five', ' twenty']\n",
      " five, target:  four   ==>   predicted: [' twelve', ' nine', ' twenty', ' eight', ' seven']\n",
      " six, target:  five   ==>   predicted: [' seven', ' twelve', ' eight', ' five', ' ten']\n",
      " seven, target:  six   ==>   predicted: [' eight', ' twelve', ' nine', ' eleven', ' twenty']\n",
      " eight, target:  seven   ==>   predicted: [' twelve', ' seven', ' nine', ' ten', ' five']\n",
      " nine, target:  eight   ==>   predicted: [' twelve', ' seven', ' eight', ' ten', ' twenty']\n",
      " ten, target:  nine   ==>   predicted: [' twelve', ' eight', ' seven', ' twenty', ' eleven']\n",
      " eleven, target:  ten   ==>   predicted: [' twelve', ' eight', ' seven', ' twenty', ' six']\n",
      " twelve, target:  eleven   ==>   predicted: [' seven', ' five', ' eight', ' twelve', ' twenty']\n",
      " thirteen, target:  twelve   ==>   predicted: [' twelve', ' seven', ' eight', ' five', ' nine']\n",
      " fourteen, target:  thirteen   ==>   predicted: [' twelve', ' seven', ' five', ' eight', ' nine']\n",
      " fifteen, target:  fourteen   ==>   predicted: [' twelve', ' seven', ' eight', ' six', ' twenty']\n",
      " sixteen, target:  fifteen   ==>   predicted: [' twelve', ' seven', ' eight', ' five', ' ten']\n",
      " seventeen, target:  sixteen   ==>   predicted: [' twelve', ' eight', ' five', ' seven', ' twenty']\n",
      " eighteen, target:  seventeen   ==>   predicted: [' seven', ' twelve', ' five', ' eight', ' nine']\n",
      " nineteen, target:  eighteen   ==>   predicted: [' seven', ' twelve', ' eight', ' six', ' twenty']\n",
      " twenty, target:  nineteen   ==>   predicted: [' seven', ' eight', ' five', ' nine', ' six']\n"
     ]
    }
   ],
   "source": [
    "relation = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.eye(model.config.n_embd).to(model.dtype).to(model.device),\n",
    "    bias = simple_corner\n",
    ")\n",
    "check_with_test_cases(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_JB(top_performers, relation_prompt, num_icl = 3, calculate_at_lnf = False):\n",
    "    try:\n",
    "        jbs = []\n",
    "        for s, s_idx, o in tqdm(top_performers):\n",
    "            others = set(top_performers) - {(s, s_idx, o)}\n",
    "            others = random.sample(list(others), k = min(num_icl, len(list(others)))) \n",
    "            prompt = \"\"\n",
    "            prompt += \"\\n\".join(relation_prompt.format(s_other) + f\" {o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "            prompt += relation_prompt\n",
    "            print(\"subject: \", s)\n",
    "            print(prompt)\n",
    "\n",
    "            jb, _ = estimate.relation_operator_from_sample(\n",
    "                model, tokenizer,\n",
    "                s, prompt,\n",
    "                subject_token_index= s_idx,\n",
    "                layer = 15,\n",
    "                device = model.device,\n",
    "                # calculate_at_lnf = calculate_at_lnf\n",
    "            )\n",
    "            print(jb.weight.norm(), jb.bias.norm())\n",
    "            print()\n",
    "            jbs.append(jb)\n",
    "        \n",
    "        weight = torch.stack([jb.weight for jb in jbs]).mean(dim=0)\n",
    "        bias  = torch.stack([jb.bias for jb in jbs]).mean(dim=0)\n",
    "\n",
    "        return weight, bias\n",
    "    except RuntimeError as e:\n",
    "        if(str(e).startswith(\"CUDA out of memory\")):\n",
    "            print(\"CUDA out of memory\")\n",
    "        if(num_icl > 1):\n",
    "            num_icl -= 1\n",
    "            print(\"trying with smaller icl >> \", num_icl)\n",
    "            return get_averaged_JB(top_performers, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        else:\n",
    "            raise Exception(\"RuntimeError >> can't calculate Jacobian with minimum number of icl examples\")\n",
    "\n",
    "def get_multiple_averaged_JB(top_performers, relation_prompt, N = 3, num_icl = 2, calculate_at_lnf = False):\n",
    "    weights_and_biases = []\n",
    "    sample_size = min(len(top_performers), num_icl + 2)\n",
    "    for _ in tqdm(range(N)):\n",
    "        cur_sample = random.sample(top_performers, k = sample_size)\n",
    "        weight, bias = get_averaged_JB(cur_sample, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        weights_and_biases.append({\n",
    "            'weight': weight,\n",
    "            'bias'  : bias\n",
    "        })\n",
    "    return weights_and_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' two', -1, ' one'), (' three', -1, ' two'), (' four', -1, ' three'), (' five', -1, ' four'), (' six', -1, ' five'), (' seven', -1, ' six'), (' eight', -1, ' seven'), (' nine', -1, ' eight'), (' ten', -1, ' nine'), (' eleven', -1, ' ten'), (' twelve', -1, ' eleven'), (' thirteen', -1, ' twelve'), (' fourteen', -1, ' thirteen'), (' fifteen', -1, ' fourteen'), (' sixteen', -1, ' fifteen'), (' seventeen', -1, ' sixteen'), (' eighteen', -1, ' seventeen'), (' nineteen', -1, ' eighteen'), (' twenty', -1, ' nineteen')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d49cde927d6453cbe3465a8eb0de275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cf9f469bf94fbd980cdac8a57ce16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:   eighteen\n",
      " ten starts with  nine.\n",
      " eleven starts with  ten.\n",
      "{} starts with\n",
      "tensor(12.2578, device='cuda:0', dtype=torch.float16) tensor(330.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   ten\n",
      " eleven starts with  ten.\n",
      " two starts with  one.\n",
      "{} starts with\n",
      "tensor(5.1523, device='cuda:0', dtype=torch.float16) tensor(371.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   two\n",
      " eleven starts with  ten.\n",
      " ten starts with  nine.\n",
      "{} starts with\n",
      "tensor(6.8242, device='cuda:0', dtype=torch.float16) tensor(363.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   eleven\n",
      " ten starts with  nine.\n",
      " two starts with  one.\n",
      "{} starts with\n",
      "tensor(10.9375, device='cuda:0', dtype=torch.float16) tensor(352.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da838cb137f847b5b657c64bc80b4843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:   five\n",
      " twelve starts with  eleven.\n",
      " seven starts with  six.\n",
      "{} starts with\n",
      "tensor(6.3320, device='cuda:0', dtype=torch.float16) tensor(401.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   twelve\n",
      " five starts with  four.\n",
      " seven starts with  six.\n",
      "{} starts with\n",
      "tensor(7.6875, device='cuda:0', dtype=torch.float16) tensor(396.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   four\n",
      " twelve starts with  eleven.\n",
      " seven starts with  six.\n",
      "{} starts with\n",
      "tensor(6.6211, device='cuda:0', dtype=torch.float16) tensor(398.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   seven\n",
      " twelve starts with  eleven.\n",
      " five starts with  four.\n",
      "{} starts with\n",
      "tensor(7.7461, device='cuda:0', dtype=torch.float16) tensor(381., device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9121ad10a8402898828b1adee5f2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:   twenty\n",
      " four starts with  three.\n",
      " sixteen starts with  fifteen.\n",
      "{} starts with\n",
      "tensor(9.1016, device='cuda:0', dtype=torch.float16) tensor(400.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   eighteen\n",
      " four starts with  three.\n",
      " twenty starts with  nineteen.\n",
      "{} starts with\n",
      "tensor(10.4141, device='cuda:0', dtype=torch.float16) tensor(382.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   four\n",
      " eighteen starts with  seventeen.\n",
      " twenty starts with  nineteen.\n",
      "{} starts with\n",
      "tensor(10.9531, device='cuda:0', dtype=torch.float16) tensor(382., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   sixteen\n",
      " eighteen starts with  seventeen.\n",
      " four starts with  three.\n",
      "{} starts with\n",
      "tensor(13.7188, device='cuda:0', dtype=torch.float16) tensor(379.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "        (objects[i+1], -1, objects[i]) for i in range(len(objects) - 1)\n",
    "    ]\n",
    "print(samples)\n",
    "\n",
    "weights_and_biases = get_multiple_averaged_JB(\n",
    "    samples, \n",
    "    relation_prompt=\"{} starts with\", \n",
    "    N = 3, \n",
    "    calculate_at_lnf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three comes after two\\nsix comes after five\\n{} comes after'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " two, target:  one   ==>   predicted: [' twelve', ' seven', ' six', ' eight', ' five']\n",
      " three, target:  two   ==>   predicted: [' twelve', ' seven', ' six', ' eight', ' five']\n",
      " four, target:  three   ==>   predicted: [' twelve', ' seven', ' five', ' six', ' eight']\n",
      " five, target:  four   ==>   predicted: [' twelve', ' five', ' seven', ' six', ' eight']\n",
      " six, target:  five   ==>   predicted: [' twelve', ' six', ' seven', ' eight', ' five']\n",
      " seven, target:  six   ==>   predicted: [' seven', ' twelve', ' six', ' eight', ' five']\n",
      " eight, target:  seven   ==>   predicted: [' twelve', ' eight', ' seven', ' nine', ' six']\n",
      " nine, target:  eight   ==>   predicted: [' twelve', ' nine', ' seven', ' eight', ' ten']\n",
      " ten, target:  nine   ==>   predicted: [' twelve', ' ten', ' eleven', ' fourteen', ' nine']\n",
      " eleven, target:  ten   ==>   predicted: [' twelve', ' eleven', ' fourteen', ' thirteen', ' fifteen']\n",
      " twelve, target:  eleven   ==>   predicted: [' twelve', ' thirteen', ' fourteen', ' eleven', ' fifteen']\n",
      " thirteen, target:  twelve   ==>   predicted: [' twelve', ' thirteen', ' fourteen', ' fifteen', ' sixteen']\n",
      " fourteen, target:  thirteen   ==>   predicted: [' fourteen', ' twelve', ' fifteen', ' thirteen', ' sixteen']\n",
      " fifteen, target:  fourteen   ==>   predicted: [' twelve', ' fifteen', ' fourteen', ' thirteen', ' sixteen']\n",
      " sixteen, target:  fifteen   ==>   predicted: [' twelve', ' sixteen', ' fourteen', ' fifteen', ' thirteen']\n",
      " seventeen, target:  sixteen   ==>   predicted: [' twelve', ' fourteen', ' sixteen', ' fifteen', ' thirteen']\n",
      " eighteen, target:  seventeen   ==>   predicted: [' twelve', ' fourteen', ' sixteen', ' fifteen', ' thirteen']\n",
      " nineteen, target:  eighteen   ==>   predicted: [' twelve', ' fourteen', ' sixteen', ' fifteen', ' twenty']\n",
      " twenty, target:  nineteen   ==>   predicted: [' twelve', ' twenty', ' sixteen', ' fourteen', ' fifteen']\n"
     ]
    }
   ],
   "source": [
    "relation_operator = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer= tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.stack(\n",
    "        [wb['weight'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0),\n",
    "    # bias = torch.stack(\n",
    "    #     [wb['bias'] for wb in weights_and_biases]\n",
    "    # ).mean(dim=0),\n",
    "    bias = simple_corner\n",
    ")\n",
    "\n",
    "check_with_test_cases(relation_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '.', '\\n', ' three', ' seven']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(\n",
    "    torch.stack(\n",
    "        [wb['bias'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3439fe3f7dcaddaf51997811d25ada8e7c0985d2997d22a3ed461af94d2f9f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
