{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from baukit import nethook\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B ==> device: cuda:0, memory: 24320971760\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\" # \"facebook/galactica-6.7b\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "n_embd_field = \"hidden_size\"\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float32)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "\n",
    "def untuple(x):\n",
    "    if(isinstance(x, tuple)):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "def extract_zh(subject, relation, layer, subject_token_index = -1):\n",
    "    prompt = relation.format(subject)\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", return_offsets_mapping=True\n",
    "    ).to(model.device)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    subject_i, subject_j = estimate._find_token_range(\n",
    "        prompt, subject, offset_mapping=offset_mapping[0]\n",
    "    )\n",
    "\n",
    "    h_token_index = estimate._determine_token_index(subject_i, subject_j, subject_token_index)\n",
    "    h_layer_name = f\"transformer.h.{layer}\"\n",
    "    z_layer_name = f\"transformer.h.{model.config.n_layer-1}\"\n",
    "    with baukit.TraceDict(\n",
    "        model, [h_layer_name, z_layer_name], retain_input=True\n",
    "    ) as traces:\n",
    "        model(**inputs)\n",
    "    \n",
    "    ret_dict = {\n",
    "        \"h_token_index\": h_token_index\n",
    "    }\n",
    "\n",
    "    print(untuple(traces[h_layer_name].input).norm())\n",
    "    return (\n",
    "        untuple(traces[h_layer_name].output)[0, h_token_index], \n",
    "        untuple(traces[z_layer_name].output)[0, -1]\n",
    "    ), ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['superlative of large is largest  superlative of quick is quickest  superlative of tough is  toughest  superlative',\n",
       " 'superlative of large is largest  superlative of quick is quickest  superlative of safe is  safest  superlative']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "# relation = \"{} is located in the country of\"\n",
    "# subject_o = \"Niagara Falls\"\n",
    "# subject_c = \"The Great Wall\"\n",
    "relation = '''superlative of large is largest\n",
    " superlative of quick is quickest\n",
    " superlative of {} is\n",
    "'''\n",
    "subject_o = \"tough\"\n",
    "subject_c = \"safe\"\n",
    "layer = 27\n",
    "######################################################################################\n",
    "\n",
    "prompts = [\n",
    "    relation.format(subject_o),\n",
    "    relation.format(subject_c)\n",
    "]\n",
    "\n",
    "txt, ret_dict = model_utils.generate_fast(\n",
    "    model, tokenizer,\n",
    "    prompts=prompts,\n",
    "    argmax_greedy=True,\n",
    "    get_answer_tokens= True,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'top_token': ' toughest',\n",
       "  'candidates': [{'token': ' toughest', 'token_id': 28212, 'p': 0.4606},\n",
       "   {'token': ' strongest', 'token_id': 12841, 'p': 0.0774},\n",
       "   {'token': ' tough', 'token_id': 5802, 'p': 0.0537},\n",
       "   {'token': ' ', 'token_id': 220, 'p': 0.0513},\n",
       "   {'token': '\\n', 'token_id': 198, 'p': 0.0512}]},\n",
       " {'top_token': ' safest',\n",
       "  'candidates': [{'token': ' safest', 'token_id': 33630, 'p': 0.5988},\n",
       "   {'token': ' safe', 'token_id': 3338, 'p': 0.0967},\n",
       "   {'token': ' super', 'token_id': 2208, 'p': 0.0513},\n",
       "   {'token': '\\n', 'token_id': 198, 'p': 0.0273},\n",
       "   {'token': ' safer', 'token_id': 14178, 'p': 0.027}]}]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_dict[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[5802],\n",
       "        [3338]], device='cuda:0'), 'attention_mask': tensor([[1],\n",
       "        [1]], device='cuda:0')}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\" \" + subject_o, \" \" + subject_c], padding = True, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1965.3038, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096]), torch.Size([4096]), {'h_token_index': 20})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h_o, z_o), ret_dict = extract_zh(\n",
    "    subject = subject_o, relation=relation,\n",
    "    layer = layer\n",
    ")\n",
    "\n",
    "h_o.shape, z_o.shape, ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1965.8682, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096]), torch.Size([4096]), {'h_token_index': 20})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h_c, z_c), ret_dict = extract_zh(\n",
    "    subject = subject_c, relation=relation,\n",
    "    layer = layer\n",
    ")\n",
    "\n",
    "h_c.shape, z_c.shape, ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relations.corner import CornerEstimator\n",
    "\n",
    "corner_estimator = CornerEstimator(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' toughest', 16.008),\n",
       " (' strongest', 14.224),\n",
       " (' tough', 13.859),\n",
       " (' ', 13.813),\n",
       " ('\\n', 13.811)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(z_o, get_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' safest', 16.283),\n",
       " (' safe', 14.46),\n",
       " (' super', 13.826),\n",
       " ('\\n', 13.193),\n",
       " (' safer', 13.185)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(z_c, get_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_latent(h_r, int_layer, int_token):\n",
    "    def intervention(output, layer):\n",
    "        print(layer, int_layer)\n",
    "        if(layer != int_layer):\n",
    "            return output\n",
    "        print(int_layer, \" >> \", output[0].shape, h_r.shape)\n",
    "        output[0][0, int_token] = h_r\n",
    "        return output\n",
    "    return intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.27 transformer.h.27\n",
      "transformer.h.27  >>  torch.Size([1, 23, 4096]) torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "prompt = relation.format(subject_c)\n",
    "tokenized = tokenizer(\n",
    "    prompt, return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "intervention_layer = f\"transformer.h.{layer}\"\n",
    "final_layer = f\"transformer.h.{model.config.n_layer - 1}\"\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    model, [intervention_layer, final_layer], \n",
    "    edit_output= replace_latent(h_r=h_o, int_layer = intervention_layer, int_token=20)\n",
    ") as traces:\n",
    "    output = model(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' safest', 16.283),\n",
       " (' safe', 14.46),\n",
       " (' super', 13.826),\n",
       " ('\\n', 13.193),\n",
       " (' safer', 13.185)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(untuple(traces[final_layer].output)[0, -1], get_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
