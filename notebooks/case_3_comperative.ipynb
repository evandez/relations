{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from baukit import nethook\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B ==> device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('angry', 'angrier', 'angriest'),\n",
       " ('bad', 'worse', 'worst'),\n",
       " ('big', 'bigger', 'biggest'),\n",
       " ('bitter', 'bitterer', 'bitterest'),\n",
       " ('black', 'blacker', 'blackest'),\n",
       " ('bland', 'blander', 'blandest'),\n",
       " ('bloody', 'bloodier', 'bloodiest'),\n",
       " ('blue', 'bluer', 'bluest'),\n",
       " ('bold', 'bolder', 'boldest'),\n",
       " ('bossy', 'bossier', 'bossiest'),\n",
       " ('brave', 'braver', 'bravest'),\n",
       " ('brief', 'briefer', 'briefest'),\n",
       " ('bright', 'brighter', 'brightest'),\n",
       " ('broad', 'broader', 'broadest'),\n",
       " ('busy', 'busier', 'busiest'),\n",
       " ('calm', 'calmer', 'calmest'),\n",
       " ('cheap', 'cheaper', 'cheapest'),\n",
       " ('chewy', 'chewier', 'chewiest'),\n",
       " ('chubby', 'chubbier', 'chubbiest'),\n",
       " ('classy', 'classier', 'classiest'),\n",
       " ('clean', 'cleaner', 'cleanest'),\n",
       " ('clear', 'clear', 'clearest'),\n",
       " ('clever', 'cleverer', 'cleverest'),\n",
       " ('close', 'closer', 'closest'),\n",
       " ('cloudy', 'cloudier', 'cloudiest'),\n",
       " ('clumsy', 'clumsier', 'clumsiest'),\n",
       " ('coarse', 'coarser', 'coarsest'),\n",
       " ('cold', 'colder', 'coldest'),\n",
       " ('cool', 'cooler', 'coolest'),\n",
       " ('crazy', 'crazier', 'craziest'),\n",
       " ('creamy', 'creamier', 'creamiest'),\n",
       " ('creepy', 'creepier', 'creepiest'),\n",
       " ('crispy', 'crispier', 'crispiest'),\n",
       " ('cruel', 'crueller', 'cruellest'),\n",
       " ('crunchy', 'crunchier', 'crunchiest'),\n",
       " ('curly', 'curly', 'curliest'),\n",
       " ('curvy', 'curvier', 'curviest'),\n",
       " ('cute', 'cuter', 'cutest'),\n",
       " ('damp', 'damper', 'dampest'),\n",
       " ('dark', 'darker', 'darkest'),\n",
       " ('deadly', 'deadlier', 'deadliest'),\n",
       " ('deep', 'deeper', 'deepest'),\n",
       " ('dense', 'denser', 'densest'),\n",
       " ('dirty', 'dirtier', 'dirtiest'),\n",
       " ('dry', 'drier', 'driest'),\n",
       " ('dull', 'duller', 'dullest'),\n",
       " ('dumb', 'dumber', 'dumbest'),\n",
       " ('dusty', 'dustier', 'dustiest'),\n",
       " ('early', 'earlier', 'earliest'),\n",
       " ('easy', 'easier', 'easiest'),\n",
       " ('faint', 'fainter', 'faintest'),\n",
       " ('fair', 'fairer', 'fairest'),\n",
       " ('fancy', 'fancier', 'fanciest'),\n",
       " ('far', 'further/farther', 'furthest/farthest'),\n",
       " ('fast', 'faster', 'fastest'),\n",
       " ('fat', 'fatter', 'fattest'),\n",
       " ('few', 'fewer', 'fewest'),\n",
       " ('fierce', 'fiercer', 'fiercest'),\n",
       " ('filthy', 'filthier', 'filthiest'),\n",
       " ('fine', 'finer', 'finest'),\n",
       " ('firm', 'firmer', 'firmest'),\n",
       " ('fit', 'fitter', 'fittest'),\n",
       " ('flaky', 'flakier', 'flakiest'),\n",
       " ('flat', 'flatter', 'flattest'),\n",
       " ('fresh', 'fresher', 'freshest'),\n",
       " ('friendly', 'friendlier', 'friendliest'),\n",
       " ('full', 'fuller', 'fullest'),\n",
       " ('funny', 'funnier', 'funniest'),\n",
       " ('gentle', 'gentler', 'gentlest'),\n",
       " ('gloomy', 'gloomier', 'gloomiest'),\n",
       " ('good', 'better', 'best'),\n",
       " ('grand', 'grander', 'grandest'),\n",
       " ('grave', 'graver', 'gravest'),\n",
       " ('greasy', 'greasier', 'greasiest'),\n",
       " ('great', 'greater', 'greatest'),\n",
       " ('greedy', 'greedier', 'greediest'),\n",
       " ('gross', 'grosser', 'grossest'),\n",
       " ('guilty', 'guilter', 'guiltiest'),\n",
       " ('hairy', 'hairier', 'hairiest'),\n",
       " ('handy', 'handier', 'handiest'),\n",
       " ('happy', 'happier', 'happiest'),\n",
       " ('hard', 'harder', 'hardest'),\n",
       " ('harsh', 'harsher', 'harshest'),\n",
       " ('healthy', 'healthier', 'healthiest'),\n",
       " ('heavy', 'heavier', 'heaviest'),\n",
       " ('high', 'higher', 'highest'),\n",
       " ('hip', 'hipper', 'hippest'),\n",
       " ('hot', 'hotter', 'hottest'),\n",
       " ('humble', 'humbler', 'humblest'),\n",
       " ('hungry', 'hungrier', 'hungriest'),\n",
       " ('icy', 'icier', 'iciest'),\n",
       " ('itchy', 'itchier', 'itchiest'),\n",
       " ('juicy', 'juicier', 'juiciest'),\n",
       " ('kind', 'kinder', 'kindest'),\n",
       " ('large', 'larger', 'largest'),\n",
       " ('late', 'later', 'latest'),\n",
       " ('lazy', 'lazier', 'laziest'),\n",
       " ('light', 'lighter', 'lightest'),\n",
       " ('likely', 'likelier', 'likeliest'),\n",
       " ('little', 'littler', 'littlest'),\n",
       " ('lively', 'livelier', 'liveliest'),\n",
       " ('lonely', 'lonlier', 'loneliest'),\n",
       " ('long', 'longer', 'longest'),\n",
       " ('loud', 'louder', 'loudest'),\n",
       " ('lovely', 'lovelier', 'loveliest'),\n",
       " ('low', 'lower', 'lowest'),\n",
       " ('mad', 'madder', 'maddest'),\n",
       " ('mean', 'meaner', 'meanest'),\n",
       " ('messy', 'messier', 'messiest'),\n",
       " ('mild', 'milder', 'mildest'),\n",
       " ('moist', 'moister', 'moistest'),\n",
       " ('narrow', 'narrower', 'narrowest'),\n",
       " ('nasty', 'nastier', 'nastiest'),\n",
       " ('naughty', 'naughtier', 'naughtiest'),\n",
       " ('near', 'nearer', 'nearest'),\n",
       " ('neat', 'neater', 'neatest'),\n",
       " ('needy', 'needier', 'neediest'),\n",
       " ('new', 'newer', 'newest'),\n",
       " ('nice', 'nicer', 'nicest'),\n",
       " ('noisy', 'noisier', 'noisiest'),\n",
       " ('odd', 'odder', 'oddest'),\n",
       " ('oily', 'oilier', 'oiliest'),\n",
       " ('old', 'older/elder', 'oldest/eldest'),\n",
       " ('plain', 'plainer', 'plainest'),\n",
       " ('polite', 'politer', 'politest'),\n",
       " ('poor', 'poorer', 'poorest'),\n",
       " ('pretty', 'prettier', 'prettiest'),\n",
       " ('proud', 'prouder', 'proudest'),\n",
       " ('pure', 'purer', 'purest'),\n",
       " ('quick', 'quicker', 'quickest'),\n",
       " ('quiet', 'quieter', 'quietest'),\n",
       " ('rare', 'rarer', 'rarest'),\n",
       " ('raw', 'rawer', 'rawest'),\n",
       " ('rich', 'richer', 'richest'),\n",
       " ('ripe', 'riper', 'ripest'),\n",
       " ('risky', 'riskier', 'riskiest'),\n",
       " ('roomy', 'roomier', 'roomiest'),\n",
       " ('rough', 'rougher', 'roughest'),\n",
       " ('rude', 'ruder', 'rudest'),\n",
       " ('rusty', 'rustier', 'rustiest'),\n",
       " ('sad', 'sadder', 'saddest'),\n",
       " ('safe', 'safer', 'safest'),\n",
       " ('salty', 'saltier', 'saltiest'),\n",
       " ('sane', 'saner', 'sanest'),\n",
       " ('scary', 'scarier', 'scariest'),\n",
       " ('shallow', 'shallower', 'shallowest'),\n",
       " ('sharp', 'sharper', 'sharpest'),\n",
       " ('shiny', 'shinier', 'shiniest'),\n",
       " ('short', 'shorter', 'shortest'),\n",
       " ('shy', 'shyer', 'shyest'),\n",
       " ('silly', 'sillier', 'silliest'),\n",
       " ('simple', 'simpler', 'simplest'),\n",
       " ('sincere', 'sincerer', 'sincerest'),\n",
       " ('skinny', 'skinnier', 'skinniest'),\n",
       " ('sleepy', 'sleepier', 'sleepiest'),\n",
       " ('slim', 'slimmer', 'slimmest'),\n",
       " ('slimy', 'slimier', 'slimiest'),\n",
       " ('slow', 'slower', 'slowest'),\n",
       " ('small', 'smaller', 'smallest'),\n",
       " ('smart', 'smarter', 'smartest'),\n",
       " ('smelly', 'smellier', 'smelliest'),\n",
       " ('smoky', 'smokier', 'smokiest'),\n",
       " ('smooth', 'smoother', 'smoothest'),\n",
       " ('soft', 'softer', 'softest'),\n",
       " ('soon', 'sooner', 'soonest'),\n",
       " ('sore', 'sorer', 'sorest'),\n",
       " ('sorry', 'sorrier', 'sorriest'),\n",
       " ('sour', 'sourer', 'sourest'),\n",
       " ('spicy', 'spicier', 'spiciest'),\n",
       " ('steep', 'steeper', 'steepest'),\n",
       " ('stingy', 'stingier', 'stingiest'),\n",
       " ('strange', 'stranger', 'strangest'),\n",
       " ('strict', 'stricter', 'strictest'),\n",
       " ('strong', 'stronger', 'strongest'),\n",
       " ('sunny', 'sunnier', 'sunniest'),\n",
       " ('sweaty', 'sweatier', 'sweatiest'),\n",
       " ('sweet', 'sweeter', 'sweetest'),\n",
       " ('tall', 'taller', 'tallest'),\n",
       " ('tan', 'tanner', 'tannest'),\n",
       " ('tasty', 'tastier', 'tastiest'),\n",
       " ('thick', 'thicker', 'thickest'),\n",
       " ('thin', 'thinner', 'thinnest'),\n",
       " ('thirsty', 'thirstier', 'thirstiest'),\n",
       " ('tiny', 'tinier', 'tiniest'),\n",
       " ('tough', 'tougher', 'toughest'),\n",
       " ('true', 'truer', 'truest'),\n",
       " ('ugly', 'uglier', 'ugliest'),\n",
       " ('warm', 'warmer', 'warmest'),\n",
       " ('weak', 'weaker', 'weakest'),\n",
       " ('wealthy', 'wealthier', 'wealthiest'),\n",
       " ('weird', 'weirder', 'weirdest'),\n",
       " ('wet', 'wetter', 'wettest'),\n",
       " ('wide', 'wider', 'widest'),\n",
       " ('wild', 'wilder', 'wildest'),\n",
       " ('windy', 'windier', 'windiest'),\n",
       " ('wise', 'wiser', 'wisest'),\n",
       " ('worldly', 'worldlier', 'worldliest'),\n",
       " ('worthy', 'worthier', 'worthiest'),\n",
       " ('young', 'younger', 'youngest')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"comperative-superlative.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        w = line.strip()\n",
    "        if(len(w) == 0):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "base = words[0 : len(words) : 3]\n",
    "comparative = words[1 : len(words) : 3]\n",
    "superlative = words[2 : len(words) : 3]\n",
    "\n",
    "list(zip(base, comparative, superlative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[48486, 50256],\n",
       "        [ 1398,   959],\n",
       "        [ 1398,  6386]]), 'attention_mask': tensor([[1, 0],\n",
       "        [1, 1],\n",
       "        [1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([' classy', ' classier', ' classiest'], padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 'worse', 'worst'),\n",
       " ('big', 'bigger', 'biggest'),\n",
       " ('bright', 'brighter', 'brightest'),\n",
       " ('cheap', 'cheaper', 'cheapest'),\n",
       " ('close', 'closer', 'closest'),\n",
       " ('cool', 'cooler', 'coolest'),\n",
       " ('dark', 'darker', 'darkest'),\n",
       " ('deep', 'deeper', 'deepest'),\n",
       " ('early', 'earlier', 'earliest'),\n",
       " ('easy', 'easier', 'easiest'),\n",
       " ('fast', 'faster', 'fastest'),\n",
       " ('fine', 'finer', 'finest'),\n",
       " ('full', 'fuller', 'fullest'),\n",
       " ('good', 'better', 'best'),\n",
       " ('great', 'greater', 'greatest'),\n",
       " ('happy', 'happier', 'happiest'),\n",
       " ('hard', 'harder', 'hardest'),\n",
       " ('heavy', 'heavier', 'heaviest'),\n",
       " ('high', 'higher', 'highest'),\n",
       " ('hot', 'hotter', 'hottest'),\n",
       " ('large', 'larger', 'largest'),\n",
       " ('late', 'later', 'latest'),\n",
       " ('long', 'longer', 'longest'),\n",
       " ('low', 'lower', 'lowest'),\n",
       " ('near', 'nearer', 'nearest'),\n",
       " ('new', 'newer', 'newest'),\n",
       " ('poor', 'poorer', 'poorest'),\n",
       " ('quick', 'quicker', 'quickest'),\n",
       " ('rich', 'richer', 'richest'),\n",
       " ('safe', 'safer', 'safest'),\n",
       " ('short', 'shorter', 'shortest'),\n",
       " ('simple', 'simpler', 'simplest'),\n",
       " ('small', 'smaller', 'smallest'),\n",
       " ('smart', 'smarter', 'smartest'),\n",
       " ('strong', 'stronger', 'strongest'),\n",
       " ('tall', 'taller', 'tallest'),\n",
       " ('tough', 'tougher', 'toughest'),\n",
       " ('weak', 'weaker', 'weakest'),\n",
       " ('wealthy', 'wealthier', 'wealthiest'),\n",
       " ('wide', 'wider', 'widest'),\n",
       " ('young', 'younger', 'youngest')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_single_token = []\n",
    "for b, c, s in zip(base, comparative, superlative):\n",
    "    tokenized = tokenizer([\" \" + b, \" \" + c, \" \" + s], padding=True, return_tensors='pt').input_ids\n",
    "    if(tokenized.shape[1] > 1):\n",
    "        continue\n",
    "    filter_single_token.append((b, c, s))\n",
    "filter_single_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe ===>  safest\n",
      "low ===>  lowest\n",
      "weak ===>  weakest\n",
      "tough ===>  toughest\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"superlative of late is latest\n",
    "superlative of strong is strongest\n",
    "superlaitve of {} is\"\"\"\n",
    "\n",
    "# prompt = \"\"\"grape ends with E\n",
    "# monitor ends with R\n",
    "# glass ends with\"\"\"\n",
    "\n",
    "words = ['safe', 'low', 'weak', 'tough']\n",
    "\n",
    "for w in words:\n",
    "    txt, ret_dict = model_utils.generate_fast(\n",
    "        model, tokenizer, \n",
    "        prompts=[prompt.format(w)], max_new_tokens=10, \n",
    "        get_answer_tokens=True, argmax_greedy=True\n",
    "    )\n",
    "    print(f\"{w} ===> {ret_dict['answer'][0]['top_token']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [\" \" + o[2] for o in filter_single_token]\n",
    "\n",
    "from relations.corner import CornerEstimator\n",
    "corner_estimator = CornerEstimator(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.53125 [(' largest', 104.312), (' highest', 103.062), (' longest', 99.688), (' strongest', 99.375), (' smallest', 99.125)]\n"
     ]
    }
   ],
   "source": [
    "simple_corner = corner_estimator.estimate_simple_corner(objects, scale_up=70)\n",
    "print(simple_corner.norm().item(), corner_estimator.get_vocab_representation(simple_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating inverse of unbedding weights . . .\n",
      "48.71875 [(' strongest', 74.875), (' fastest', 73.938), (' longest', 73.375), (' largest', 72.0), (' smallest', 70.562)]\n"
     ]
    }
   ],
   "source": [
    "lin_inv_corner = corner_estimator.estimate_lin_inv_corner(objects, target_logit_value=50)\n",
    "print(lin_inv_corner.norm().item(), corner_estimator.get_vocab_representation(lin_inv_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.34375 [(' strongest', 85.0), (' best', 83.562), (' nearest', 83.375), (' closest', 83.312), (' highest', 83.188)]\n"
     ]
    }
   ],
   "source": [
    "lst_sq_corner = corner_estimator.estimate_corner_lstsq_solve(objects[:30], target_logit=30)\n",
    "print(lst_sq_corner.norm().item(), corner_estimator.get_vocab_representation(lst_sq_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_corner = corner_estimator.estimate_average_corner_with_gradient_descent(objects, average_on=5, target_logit_value=50, verbose=False)\n",
    "# print(avg_corner.norm().item(), corner_estimator.get_vocab_representation(avg_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_test_cases(relation_operator):\n",
    "    test_cases = [\n",
    "        (b, -1, s) for b, c, s in filter_single_token[30:]\n",
    "    ]\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        answer = relation_operator(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short, target: shortest   ==>   predicted: [' highest', ' largest', ' greatest', ' smallest', ' lowest']\n",
      "simple, target: simplest   ==>   predicted: [' lowest', ' largest', ' highest', ' greatest', ' smallest']\n",
      "small, target: smallest   ==>   predicted: [' highest', ' best', ' largest', ' strongest', ' lowest']\n",
      "smart, target: smartest   ==>   predicted: [' highest', ' best', ' lowest', ' closest', ' smallest']\n",
      "strong, target: strongest   ==>   predicted: [' nearest', ' lowest', ' highest', ' best', ' greatest']\n",
      "tall, target: tallest   ==>   predicted: [' smallest', ' largest', ' highest', ' best', ' greatest']\n",
      "tough, target: toughest   ==>   predicted: [' lowest', ' highest', ' best', ' worst', ' greatest']\n",
      "weak, target: weakest   ==>   predicted: [' lowest', ' largest', ' smallest', ' highest', ' greatest']\n",
      "wealthy, target: wealthiest   ==>   predicted: [' largest', ' greatest', ' smallest', ' highest', ' best']\n",
      "wide, target: widest   ==>   predicted: [' greatest', ' smallest', ' largest', ' widest', ' strongest']\n",
      "young, target: youngest   ==>   predicted: [' largest', ' smallest', ' lowest', ' highest', ' greatest']\n"
     ]
    }
   ],
   "source": [
    "relation = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.eye(model.config.n_embd).to(model.dtype).to(model.device),\n",
    "    bias = lst_sq_corner\n",
    ")\n",
    "check_with_test_cases(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_JB(top_performers, relation_prompt, num_icl = 3, calculate_at_lnf = False):\n",
    "    try:\n",
    "        jbs = []\n",
    "        for s, s_idx, o in tqdm(top_performers):\n",
    "            others = set(top_performers) - {(s, s_idx, o)}\n",
    "            others = random.sample(list(others), k = min(num_icl, len(list(others)))) \n",
    "            prompt = \"\"\n",
    "            prompt += \"\\n\".join(relation_prompt.format(s_other) + f\" {o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "            prompt += relation_prompt\n",
    "            print(\"subject: \", s)\n",
    "            print(prompt)\n",
    "\n",
    "            jb, _ = estimate.relation_operator_from_sample(\n",
    "                model, tokenizer,\n",
    "                s, prompt,\n",
    "                subject_token_index= s_idx,\n",
    "                layer = 15,\n",
    "                device = model.device,\n",
    "                # calculate_at_lnf = calculate_at_lnf\n",
    "            )\n",
    "            print(jb.weight.norm(), jb.bias.norm())\n",
    "            print()\n",
    "            jbs.append(jb)\n",
    "        \n",
    "        weight = torch.stack([jb.weight for jb in jbs]).mean(dim=0)\n",
    "        bias  = torch.stack([jb.bias for jb in jbs]).mean(dim=0)\n",
    "\n",
    "        return weight, bias\n",
    "    except RuntimeError as e:\n",
    "        if(str(e).startswith(\"CUDA out of memory\")):\n",
    "            print(\"CUDA out of memory\")\n",
    "        if(num_icl > 1):\n",
    "            num_icl -= 1\n",
    "            print(\"trying with smaller icl >> \", num_icl)\n",
    "            return get_averaged_JB(top_performers, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        else:\n",
    "            raise Exception(\"RuntimeError >> can't calculate Jacobian with minimum number of icl examples\")\n",
    "\n",
    "def get_multiple_averaged_JB(top_performers, relation_prompt, N = 3, num_icl = 2, calculate_at_lnf = False):\n",
    "    weights_and_biases = []\n",
    "    sample_size = min(len(top_performers), num_icl + 2)\n",
    "    for _ in tqdm(range(N)):\n",
    "        cur_sample = random.sample(top_performers, k = sample_size)\n",
    "        weight, bias = get_averaged_JB(cur_sample, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        weights_and_biases.append({\n",
    "            'weight': weight,\n",
    "            'bias'  : bias\n",
    "        })\n",
    "    return weights_and_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad', -1, 'worst'), ('big', -1, 'biggest'), ('bright', -1, 'brightest'), ('cheap', -1, 'cheapest'), ('close', -1, 'closest'), ('cool', -1, 'coolest'), ('dark', -1, 'darkest'), ('deep', -1, 'deepest'), ('early', -1, 'earliest'), ('easy', -1, 'easiest')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edc6864e57340739b90b62bd26aa8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14b97a9448e4f13a7fa2451c1ec6656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  easy\n",
      "superlative of bad is worst.\n",
      "superlative of dark is darkest.\n",
      "superlative of {} is\n",
      "tensor(31.9062, device='cuda:0', dtype=torch.float16) tensor(298., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  close\n",
      "superlative of dark is darkest.\n",
      "superlative of easy is easiest.\n",
      "superlative of {} is\n",
      "tensor(29.2656, device='cuda:0', dtype=torch.float16) tensor(304., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  bad\n",
      "superlative of close is closest.\n",
      "superlative of dark is darkest.\n",
      "superlative of {} is\n",
      "tensor(31.6250, device='cuda:0', dtype=torch.float16) tensor(283., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  dark\n",
      "superlative of close is closest.\n",
      "superlative of easy is easiest.\n",
      "superlative of {} is\n",
      "tensor(33.5312, device='cuda:0', dtype=torch.float16) tensor(272., device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7163cd1109b94c88b31c470dfc563472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  early\n",
      "superlative of big is biggest.\n",
      "superlative of easy is easiest.\n",
      "superlative of {} is\n",
      "tensor(30.7031, device='cuda:0', dtype=torch.float16) tensor(272., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  cheap\n",
      "superlative of easy is easiest.\n",
      "superlative of big is biggest.\n",
      "superlative of {} is\n",
      "tensor(28.9688, device='cuda:0', dtype=torch.float16) tensor(284.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  easy\n",
      "superlative of big is biggest.\n",
      "superlative of cheap is cheapest.\n",
      "superlative of {} is\n",
      "tensor(30.8750, device='cuda:0', dtype=torch.float16) tensor(292.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  big\n",
      "superlative of easy is easiest.\n",
      "superlative of early is earliest.\n",
      "superlative of {} is\n",
      "tensor(27.0156, device='cuda:0', dtype=torch.float16) tensor(278.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e1cd1ac4b348c6b7a584689d8486b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  bad\n",
      "superlative of big is biggest.\n",
      "superlative of cheap is cheapest.\n",
      "superlative of {} is\n",
      "tensor(28.6250, device='cuda:0', dtype=torch.float16) tensor(289., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  easy\n",
      "superlative of cheap is cheapest.\n",
      "superlative of big is biggest.\n",
      "superlative of {} is\n",
      "tensor(29.8594, device='cuda:0', dtype=torch.float16) tensor(308.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  big\n",
      "superlative of bad is worst.\n",
      "superlative of cheap is cheapest.\n",
      "superlative of {} is\n",
      "tensor(25.4844, device='cuda:0', dtype=torch.float16) tensor(307., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  cheap\n",
      "superlative of easy is easiest.\n",
      "superlative of big is biggest.\n",
      "superlative of {} is\n",
      "tensor(28.9688, device='cuda:0', dtype=torch.float16) tensor(284.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "        (b, -1, s) for b, c, s in filter_single_token[:10]\n",
    "    ]\n",
    "print(samples)\n",
    "\n",
    "weights_and_biases = get_multiple_averaged_JB(\n",
    "    samples, \n",
    "    relation_prompt=\"superlative of {} is\", \n",
    "    N = 3, \n",
    "    calculate_at_lnf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'shorter' in objects[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short, target: shortest   ==>   predicted: [' shortest', ' longest', ' quickest', ' smallest', ' earliest']\n",
      "simple, target: simplest   ==>   predicted: [' simplest', ' easiest', ' cheapest', ' smallest', ' best']\n",
      "small, target: smallest   ==>   predicted: [' smallest', ' lowest', ' cheapest', ' poorest', ' largest']\n",
      "smart, target: smartest   ==>   predicted: [' smartest', ' best', ' brightest', ' easiest', ' quickest']\n",
      "strong, target: strongest   ==>   predicted: [' strongest', ' weakest', ' heaviest', ' hardest', ' best']\n",
      "tall, target: tallest   ==>   predicted: [' tallest', ' longest', ' shortest', ' highest', ' best']\n",
      "tough, target: toughest   ==>   predicted: [' toughest', ' hardest', ' strongest', ' worst', ' best']\n",
      "weak, target: weakest   ==>   predicted: [' weakest', ' strongest', ' poorest', ' lowest', ' worst']\n",
      "wealthy, target: wealthiest   ==>   predicted: [' richest', ' cheapest', ' poorest', ' best', ' wealthiest']\n",
      "wide, target: widest   ==>   predicted: [' widest', ' largest', ' fullest', ' deepest', ' biggest']\n",
      "young, target: youngest   ==>   predicted: [' youngest', ' newest', ' earliest', ' oldest', ' latest']\n"
     ]
    }
   ],
   "source": [
    "relation_operator = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer= tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.stack(\n",
    "        [wb['weight'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0),\n",
    "    # bias = torch.stack(\n",
    "    #     [wb['bias'] for wb in weights_and_biases]\n",
    "    # ).mean(dim=0),\n",
    "    bias = lst_sq_corner\n",
    ")\n",
    "\n",
    "check_with_test_cases(relation_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' most', ' the', '\\n', ' least', ' ']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(\n",
    "    torch.stack(\n",
    "        [wb['bias'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50.0028, 49.9846, 50.0143, 49.9919, 50.0005, 50.0105, 49.9887, 50.0128,\n",
      "        50.0124, 49.9906, 49.9848, 49.9848, 49.9904, 50.0138, 49.9965, 49.9854,\n",
      "        49.9973, 50.0067, 50.0075, 50.0016, 49.9978, 49.9894, 50.0059, 50.0106,\n",
      "        50.0045, 50.0000, 50.0047, 50.0028, 49.9897, 50.0096, 50.0153, 50.0059,\n",
      "        50.0144, 50.0063, 50.0045, 49.9968, 50.0085, 49.9871, 49.9907, 50.0133,\n",
      "        50.0077], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096]), tensor(74., device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Sequence, TypeAlias, List\n",
    "\n",
    "unembedder = nethook.get_module(model, \"lm_head\")\n",
    "ln_f = nethook.get_module(model, \"transformer.ln_f\")\n",
    "\n",
    "def estimate_corner_lstsq_solve(\n",
    "    target_words: List[str],\n",
    "    target_logit: int = 50,\n",
    "):\n",
    "    target_tokenized = tokenizer(target_words, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    # print(target_tokenized)\n",
    "    W = torch.stack([unembedder.weight[r[0].item()] for r in target_tokenized.input_ids])\n",
    "    # print(target_tokenized.input_ids.shape, W.shape)\n",
    "    b = unembedder.bias[target_tokenized.input_ids]\n",
    "    b = b.reshape(b.shape[0])\n",
    "    y = (torch.ones(len(target_words)) * target_logit).to(model.dtype).to(model.device) - b\n",
    "    # print(b.shape, y.shape)\n",
    "    if(model.dtype == torch.float16):\n",
    "        W = W.to(torch.float32)\n",
    "        y = y.to(torch.float32)\n",
    "    x = torch.linalg.lstsq(W, y).solution\n",
    "    print(W@x + b)\n",
    "    return x.to(model.dtype)\n",
    "\n",
    "corner = estimate_corner_lstsq_solve(objects)\n",
    "corner.shape, corner.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' more', 75.688),\n",
       " (' lighter', 75.5),\n",
       " (' later', 74.438),\n",
       " (' lower', 74.312),\n",
       " (' better', 74.312)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_representation(\n",
    "    h, \n",
    "    perform_layer_norm = True, return_top_k = 5, get_logits = False\n",
    "):\n",
    "    \"\"\"\n",
    "    get representation of vector `h` in the vocabulary space. basically applied logit lens\n",
    "    \"\"\"\n",
    "    z = h.clone()\n",
    "    if(perform_layer_norm == True):\n",
    "        z = ln_f(z)\n",
    "    logits = unembedder(z)\n",
    "    token_ids = logits.topk(dim=-1, k=return_top_k).indices.squeeze().tolist()\n",
    "    logit_values = logits.topk(dim=-1, k=return_top_k).values.squeeze().tolist()\n",
    "    return [\n",
    "        tokenizer.decode(t) if get_logits == False else (tokenizer.decode(t), np.round(v, 3))\n",
    "        for t, v in zip(token_ids, logit_values)\n",
    "    ]\n",
    "\n",
    "get_vocab_representation(corner, get_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3439fe3f7dcaddaf51997811d25ada8e7c0985d2997d22a3ed461af94d2f9f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
