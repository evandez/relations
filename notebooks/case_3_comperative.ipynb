{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from baukit import nethook\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/galactica-6.7b ==> device: cuda:0, memory: 13314719744\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"facebook/galactica-6.7b\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "n_embd_field = \"hidden_size\"\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('angry', 'angrier', 'angriest'),\n",
       " ('bad', 'worse', 'worst'),\n",
       " ('big', 'bigger', 'biggest'),\n",
       " ('bitter', 'bitterer', 'bitterest'),\n",
       " ('black', 'blacker', 'blackest'),\n",
       " ('bland', 'blander', 'blandest'),\n",
       " ('bloody', 'bloodier', 'bloodiest'),\n",
       " ('blue', 'bluer', 'bluest'),\n",
       " ('bold', 'bolder', 'boldest'),\n",
       " ('bossy', 'bossier', 'bossiest'),\n",
       " ('brave', 'braver', 'bravest'),\n",
       " ('brief', 'briefer', 'briefest'),\n",
       " ('bright', 'brighter', 'brightest'),\n",
       " ('broad', 'broader', 'broadest'),\n",
       " ('busy', 'busier', 'busiest'),\n",
       " ('calm', 'calmer', 'calmest'),\n",
       " ('cheap', 'cheaper', 'cheapest'),\n",
       " ('chewy', 'chewier', 'chewiest'),\n",
       " ('chubby', 'chubbier', 'chubbiest'),\n",
       " ('classy', 'classier', 'classiest'),\n",
       " ('clean', 'cleaner', 'cleanest'),\n",
       " ('clear', 'clear', 'clearest'),\n",
       " ('clever', 'cleverer', 'cleverest'),\n",
       " ('close', 'closer', 'closest'),\n",
       " ('cloudy', 'cloudier', 'cloudiest'),\n",
       " ('clumsy', 'clumsier', 'clumsiest'),\n",
       " ('coarse', 'coarser', 'coarsest'),\n",
       " ('cold', 'colder', 'coldest'),\n",
       " ('cool', 'cooler', 'coolest'),\n",
       " ('crazy', 'crazier', 'craziest'),\n",
       " ('creamy', 'creamier', 'creamiest'),\n",
       " ('creepy', 'creepier', 'creepiest'),\n",
       " ('crispy', 'crispier', 'crispiest'),\n",
       " ('cruel', 'crueller', 'cruellest'),\n",
       " ('crunchy', 'crunchier', 'crunchiest'),\n",
       " ('curly', 'curly', 'curliest'),\n",
       " ('curvy', 'curvier', 'curviest'),\n",
       " ('cute', 'cuter', 'cutest'),\n",
       " ('damp', 'damper', 'dampest'),\n",
       " ('dark', 'darker', 'darkest'),\n",
       " ('deadly', 'deadlier', 'deadliest'),\n",
       " ('deep', 'deeper', 'deepest'),\n",
       " ('dense', 'denser', 'densest'),\n",
       " ('dirty', 'dirtier', 'dirtiest'),\n",
       " ('dry', 'drier', 'driest'),\n",
       " ('dull', 'duller', 'dullest'),\n",
       " ('dumb', 'dumber', 'dumbest'),\n",
       " ('dusty', 'dustier', 'dustiest'),\n",
       " ('early', 'earlier', 'earliest'),\n",
       " ('easy', 'easier', 'easiest'),\n",
       " ('faint', 'fainter', 'faintest'),\n",
       " ('fair', 'fairer', 'fairest'),\n",
       " ('fancy', 'fancier', 'fanciest'),\n",
       " ('far', 'further/farther', 'furthest/farthest'),\n",
       " ('fast', 'faster', 'fastest'),\n",
       " ('fat', 'fatter', 'fattest'),\n",
       " ('few', 'fewer', 'fewest'),\n",
       " ('fierce', 'fiercer', 'fiercest'),\n",
       " ('filthy', 'filthier', 'filthiest'),\n",
       " ('fine', 'finer', 'finest'),\n",
       " ('firm', 'firmer', 'firmest'),\n",
       " ('fit', 'fitter', 'fittest'),\n",
       " ('flaky', 'flakier', 'flakiest'),\n",
       " ('flat', 'flatter', 'flattest'),\n",
       " ('fresh', 'fresher', 'freshest'),\n",
       " ('friendly', 'friendlier', 'friendliest'),\n",
       " ('full', 'fuller', 'fullest'),\n",
       " ('funny', 'funnier', 'funniest'),\n",
       " ('gentle', 'gentler', 'gentlest'),\n",
       " ('gloomy', 'gloomier', 'gloomiest'),\n",
       " ('good', 'better', 'best'),\n",
       " ('grand', 'grander', 'grandest'),\n",
       " ('grave', 'graver', 'gravest'),\n",
       " ('greasy', 'greasier', 'greasiest'),\n",
       " ('great', 'greater', 'greatest'),\n",
       " ('greedy', 'greedier', 'greediest'),\n",
       " ('gross', 'grosser', 'grossest'),\n",
       " ('guilty', 'guilter', 'guiltiest'),\n",
       " ('hairy', 'hairier', 'hairiest'),\n",
       " ('handy', 'handier', 'handiest'),\n",
       " ('happy', 'happier', 'happiest'),\n",
       " ('hard', 'harder', 'hardest'),\n",
       " ('harsh', 'harsher', 'harshest'),\n",
       " ('healthy', 'healthier', 'healthiest'),\n",
       " ('heavy', 'heavier', 'heaviest'),\n",
       " ('high', 'higher', 'highest'),\n",
       " ('hip', 'hipper', 'hippest'),\n",
       " ('hot', 'hotter', 'hottest'),\n",
       " ('humble', 'humbler', 'humblest'),\n",
       " ('hungry', 'hungrier', 'hungriest'),\n",
       " ('icy', 'icier', 'iciest'),\n",
       " ('itchy', 'itchier', 'itchiest'),\n",
       " ('juicy', 'juicier', 'juiciest'),\n",
       " ('kind', 'kinder', 'kindest'),\n",
       " ('large', 'larger', 'largest'),\n",
       " ('late', 'later', 'latest'),\n",
       " ('lazy', 'lazier', 'laziest'),\n",
       " ('light', 'lighter', 'lightest'),\n",
       " ('likely', 'likelier', 'likeliest'),\n",
       " ('little', 'littler', 'littlest'),\n",
       " ('lively', 'livelier', 'liveliest'),\n",
       " ('lonely', 'lonlier', 'loneliest'),\n",
       " ('long', 'longer', 'longest'),\n",
       " ('loud', 'louder', 'loudest'),\n",
       " ('lovely', 'lovelier', 'loveliest'),\n",
       " ('low', 'lower', 'lowest'),\n",
       " ('mad', 'madder', 'maddest'),\n",
       " ('mean', 'meaner', 'meanest'),\n",
       " ('messy', 'messier', 'messiest'),\n",
       " ('mild', 'milder', 'mildest'),\n",
       " ('moist', 'moister', 'moistest'),\n",
       " ('narrow', 'narrower', 'narrowest'),\n",
       " ('nasty', 'nastier', 'nastiest'),\n",
       " ('naughty', 'naughtier', 'naughtiest'),\n",
       " ('near', 'nearer', 'nearest'),\n",
       " ('neat', 'neater', 'neatest'),\n",
       " ('needy', 'needier', 'neediest'),\n",
       " ('new', 'newer', 'newest'),\n",
       " ('nice', 'nicer', 'nicest'),\n",
       " ('noisy', 'noisier', 'noisiest'),\n",
       " ('odd', 'odder', 'oddest'),\n",
       " ('oily', 'oilier', 'oiliest'),\n",
       " ('old', 'older/elder', 'oldest/eldest'),\n",
       " ('plain', 'plainer', 'plainest'),\n",
       " ('polite', 'politer', 'politest'),\n",
       " ('poor', 'poorer', 'poorest'),\n",
       " ('pretty', 'prettier', 'prettiest'),\n",
       " ('proud', 'prouder', 'proudest'),\n",
       " ('pure', 'purer', 'purest'),\n",
       " ('quick', 'quicker', 'quickest'),\n",
       " ('quiet', 'quieter', 'quietest'),\n",
       " ('rare', 'rarer', 'rarest'),\n",
       " ('raw', 'rawer', 'rawest'),\n",
       " ('rich', 'richer', 'richest'),\n",
       " ('ripe', 'riper', 'ripest'),\n",
       " ('risky', 'riskier', 'riskiest'),\n",
       " ('roomy', 'roomier', 'roomiest'),\n",
       " ('rough', 'rougher', 'roughest'),\n",
       " ('rude', 'ruder', 'rudest'),\n",
       " ('rusty', 'rustier', 'rustiest'),\n",
       " ('sad', 'sadder', 'saddest'),\n",
       " ('safe', 'safer', 'safest'),\n",
       " ('salty', 'saltier', 'saltiest'),\n",
       " ('sane', 'saner', 'sanest'),\n",
       " ('scary', 'scarier', 'scariest'),\n",
       " ('shallow', 'shallower', 'shallowest'),\n",
       " ('sharp', 'sharper', 'sharpest'),\n",
       " ('shiny', 'shinier', 'shiniest'),\n",
       " ('short', 'shorter', 'shortest'),\n",
       " ('shy', 'shyer', 'shyest'),\n",
       " ('silly', 'sillier', 'silliest'),\n",
       " ('simple', 'simpler', 'simplest'),\n",
       " ('sincere', 'sincerer', 'sincerest'),\n",
       " ('skinny', 'skinnier', 'skinniest'),\n",
       " ('sleepy', 'sleepier', 'sleepiest'),\n",
       " ('slim', 'slimmer', 'slimmest'),\n",
       " ('slimy', 'slimier', 'slimiest'),\n",
       " ('slow', 'slower', 'slowest'),\n",
       " ('small', 'smaller', 'smallest'),\n",
       " ('smart', 'smarter', 'smartest'),\n",
       " ('smelly', 'smellier', 'smelliest'),\n",
       " ('smoky', 'smokier', 'smokiest'),\n",
       " ('smooth', 'smoother', 'smoothest'),\n",
       " ('soft', 'softer', 'softest'),\n",
       " ('soon', 'sooner', 'soonest'),\n",
       " ('sore', 'sorer', 'sorest'),\n",
       " ('sorry', 'sorrier', 'sorriest'),\n",
       " ('sour', 'sourer', 'sourest'),\n",
       " ('spicy', 'spicier', 'spiciest'),\n",
       " ('steep', 'steeper', 'steepest'),\n",
       " ('stingy', 'stingier', 'stingiest'),\n",
       " ('strange', 'stranger', 'strangest'),\n",
       " ('strict', 'stricter', 'strictest'),\n",
       " ('strong', 'stronger', 'strongest'),\n",
       " ('sunny', 'sunnier', 'sunniest'),\n",
       " ('sweaty', 'sweatier', 'sweatiest'),\n",
       " ('sweet', 'sweeter', 'sweetest'),\n",
       " ('tall', 'taller', 'tallest'),\n",
       " ('tan', 'tanner', 'tannest'),\n",
       " ('tasty', 'tastier', 'tastiest'),\n",
       " ('thick', 'thicker', 'thickest'),\n",
       " ('thin', 'thinner', 'thinnest'),\n",
       " ('thirsty', 'thirstier', 'thirstiest'),\n",
       " ('tiny', 'tinier', 'tiniest'),\n",
       " ('tough', 'tougher', 'toughest'),\n",
       " ('true', 'truer', 'truest'),\n",
       " ('ugly', 'uglier', 'ugliest'),\n",
       " ('warm', 'warmer', 'warmest'),\n",
       " ('weak', 'weaker', 'weakest'),\n",
       " ('wealthy', 'wealthier', 'wealthiest'),\n",
       " ('weird', 'weirder', 'weirdest'),\n",
       " ('wet', 'wetter', 'wettest'),\n",
       " ('wide', 'wider', 'widest'),\n",
       " ('wild', 'wilder', 'wildest'),\n",
       " ('windy', 'windier', 'windiest'),\n",
       " ('wise', 'wiser', 'wisest'),\n",
       " ('worldly', 'worldlier', 'worldliest'),\n",
       " ('worthy', 'worthier', 'worthiest'),\n",
       " ('young', 'younger', 'youngest')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"comperative-superlative.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        w = line.strip()\n",
    "        if(len(w) == 0):\n",
    "            continue\n",
    "        words.append(w)\n",
    "\n",
    "base = words[0 : len(words) : 3]\n",
    "comparative = words[1 : len(words) : 3]\n",
    "superlative = words[2 : len(words) : 3]\n",
    "\n",
    "list(zip(base, comparative, superlative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1067, 50000],\n",
       "        [ 1067,   111],\n",
       "        [ 1067,  1331],\n",
       "        [ 1067, 24072]]), 'token_type_ids': tensor([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]]), 'attention_mask': tensor([[1, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([' class', ' classy', ' classier', ' classiest'], padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 'worse', 'worst'),\n",
       " ('big', 'bigger', 'biggest'),\n",
       " ('bright', 'brighter', 'brightest'),\n",
       " ('close', 'closer', 'closest'),\n",
       " ('deep', 'deeper', 'deepest'),\n",
       " ('early', 'earlier', 'earliest'),\n",
       " ('easy', 'easier', 'easiest'),\n",
       " ('fast', 'faster', 'fastest'),\n",
       " ('fine', 'finer', 'finest'),\n",
       " ('good', 'better', 'best'),\n",
       " ('great', 'greater', 'greatest'),\n",
       " ('high', 'higher', 'highest'),\n",
       " ('large', 'larger', 'largest'),\n",
       " ('late', 'later', 'latest'),\n",
       " ('light', 'lighter', 'lightest'),\n",
       " ('long', 'longer', 'longest'),\n",
       " ('low', 'lower', 'lowest'),\n",
       " ('short', 'shorter', 'shortest'),\n",
       " ('simple', 'simpler', 'simplest'),\n",
       " ('small', 'smaller', 'smallest'),\n",
       " ('steep', 'steeper', 'steepest'),\n",
       " ('strong', 'stronger', 'strongest'),\n",
       " ('weak', 'weaker', 'weakest'),\n",
       " ('young', 'younger', 'youngest')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_single_token = []\n",
    "for b, c, s in zip(base, comparative, superlative):\n",
    "    tokenized = tokenizer([\" \" + b, \" \" + c, \" \" + s], padding=True, return_tensors='pt').input_ids\n",
    "    if(tokenized.shape[1] > 1):\n",
    "        continue\n",
    "    filter_single_token.append((b, c, s))\n",
    "filter_single_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strong ===>  strongest\n",
      "big ===>  biggest\n",
      "deep ===>  deepest\n",
      "young ===>  youngest\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"superlative of late is latest\n",
    "superlative of strong is strongest\n",
    "superlaitve of {} is\"\"\"\n",
    "\n",
    "# prompt = \"\"\"grape ends with E\n",
    "# monitor ends with R\n",
    "# glass ends with\"\"\"\n",
    "\n",
    "words = ['strong', 'big', 'deep', 'young']\n",
    "\n",
    "for w in words:\n",
    "    txt, ret_dict = model_utils.generate_fast(\n",
    "        model, tokenizer, \n",
    "        prompts=[prompt.format(w)], max_new_tokens=10, \n",
    "        get_answer_tokens=True, argmax_greedy=True\n",
    "    )\n",
    "    print(f\"{w} ===> {ret_dict['answer'][0]['top_token']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [\" \" + o[2] for o in filter_single_token]\n",
    "\n",
    "from relations.corner import CornerEstimator\n",
    "corner_estimator = CornerEstimator(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    ln_f_name= \"model.decoder.final_layer_norm\", \n",
    "    unembedder_module_name=\"lm_head\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.5625 [(' longest', 54.344), (' fastest', 53.938), (' smallest', 53.312), (' strongest', 52.75), (' weakest', 52.562)]\n"
     ]
    }
   ],
   "source": [
    "simple_corner = corner_estimator.estimate_simple_corner(objects, scale_up=70)\n",
    "print(simple_corner.norm().item(), corner_estimator.get_vocab_representation(simple_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating inverse of unbedding weights . . .\n",
      "28.59375 [(' largest', 46.594), (' longest', 46.5), (' smallest', 45.125), (' fastest', 44.969), (' strongest', 44.656)]\n"
     ]
    }
   ],
   "source": [
    "lin_inv_corner = corner_estimator.estimate_lin_inv_corner(objects, target_logit_value=50)\n",
    "print(lin_inv_corner.norm().item(), corner_estimator.get_vocab_representation(lin_inv_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.875 [(' greatest', 45.875), (' largest', 45.844), (' best', 45.844), (' worst', 45.844), (' highest', 45.844)]\n"
     ]
    }
   ],
   "source": [
    "lst_sq_corner = corner_estimator.estimate_corner_lstsq_solve(objects[:15], target_logit=30)\n",
    "print(lst_sq_corner.norm().item(), corner_estimator.get_vocab_representation(lst_sq_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_corner = corner_estimator.estimate_average_corner_with_gradient_descent(objects, average_on=5, target_logit_value=50, verbose=False)\n",
    "# print(avg_corner.norm().item(), corner_estimator.get_vocab_representation(avg_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_test_cases(relation_operator):\n",
    "    test_cases = [\n",
    "        (b, -1, s) for b, c, s in filter_single_token[15:]\n",
    "    ]\n",
    "    # print(test_cases)\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        answer = relation_operator(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long, target: longest   ==>   predicted: [' best', ' greatest', ' highest', ' largest', ' worst']\n",
      "low, target: lowest   ==>   predicted: [' best', ' greatest', ' worst', ' highest', ' strongest']\n",
      "short, target: shortest   ==>   predicted: [' best', ' greatest', ' highest', ' worst', ' largest']\n",
      "simple, target: simplest   ==>   predicted: [' best', ' greatest', ' highest', ' largest', ' worst']\n",
      "small, target: smallest   ==>   predicted: [' greatest', ' highest', ' best', ' worst', ' latest']\n",
      "steep, target: steepest   ==>   predicted: [' best', ' worst', ' greatest', ' highest', 'est']\n",
      "strong, target: strongest   ==>   predicted: [' best', ' worst', ' greatest', ' highest', ' largest']\n",
      "weak, target: weakest   ==>   predicted: [' best', ' greatest', ' highest', ' worst', ' largest']\n",
      "young, target: youngest   ==>   predicted: [' best', ' highest', ' greatest', ' latest', ' worst']\n"
     ]
    }
   ],
   "source": [
    "relation = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15, layer_name_format = \"model.decoder.layers.{}\",\n",
    "    weight = torch.eye(getattr(model.config, n_embd_field)).to(model.dtype).to(model.device),\n",
    "    bias = lst_sq_corner,\n",
    "\n",
    "    ln_f_name = \"model.decoder.final_layer_norm\"\n",
    ")\n",
    "\n",
    "check_with_test_cases(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_JB(top_performers, relation_prompt, num_icl = 3, calculate_at_lnf = False):\n",
    "    try:\n",
    "        jbs = []\n",
    "        for s, s_idx, o in tqdm(top_performers):\n",
    "            others = set(top_performers) - {(s, s_idx, o)}\n",
    "            others = random.sample(list(others), k = min(num_icl, len(list(others)))) \n",
    "            prompt = \"\"\n",
    "            prompt += \"\\n\".join(relation_prompt.format(s_other) + f\" {o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "            prompt += relation_prompt\n",
    "            print(\"subject: \", s)\n",
    "            print(prompt)\n",
    "\n",
    "            jb, _ = estimate.relation_operator_from_sample(\n",
    "                model, tokenizer,\n",
    "                s, prompt,\n",
    "                subject_token_index= s_idx,\n",
    "                layer = 15,\n",
    "                device = model.device,\n",
    "                # calculate_at_lnf = calculate_at_lnf\n",
    "\n",
    "                layer_name_format = \"model.decoder.layers.{}\",\n",
    "                ln_f_name = \"model.decoder.final_layer_norm\",\n",
    "                n_layer_field = \"num_hidden_layers\"\n",
    "            )\n",
    "            print(jb.weight.norm(), jb.bias.norm())\n",
    "            print()\n",
    "            jbs.append(jb)\n",
    "        \n",
    "        weight = torch.stack([jb.weight for jb in jbs]).mean(dim=0)\n",
    "        bias  = torch.stack([jb.bias for jb in jbs]).mean(dim=0)\n",
    "\n",
    "        return weight, bias\n",
    "    except RuntimeError as e:\n",
    "        if(str(e).startswith(\"CUDA out of memory\")):\n",
    "            print(\"CUDA out of memory\")\n",
    "        if(num_icl > 1):\n",
    "            num_icl -= 1\n",
    "            print(\"trying with smaller icl >> \", num_icl)\n",
    "            return get_averaged_JB(top_performers, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        else:\n",
    "            raise Exception(\"RuntimeError >> can't calculate Jacobian with minimum number of icl examples\")\n",
    "\n",
    "def get_multiple_averaged_JB(top_performers, relation_prompt, N = 3, num_icl = 2, calculate_at_lnf = False):\n",
    "    weights_and_biases = []\n",
    "    sample_size = min(len(top_performers), num_icl + 2)\n",
    "    for _ in tqdm(range(N)):\n",
    "        cur_sample = random.sample(top_performers, k = sample_size)\n",
    "        weight, bias = get_averaged_JB(cur_sample, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        weights_and_biases.append({\n",
    "            'weight': weight,\n",
    "            'bias'  : bias\n",
    "        })\n",
    "    return weights_and_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad', -1, 'worst'), ('big', -1, 'biggest'), ('bright', -1, 'brightest'), ('close', -1, 'closest'), ('deep', -1, 'deepest'), ('early', -1, 'earliest'), ('easy', -1, 'easiest'), ('fast', -1, 'fastest'), ('fine', -1, 'finest'), ('good', -1, 'best')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f064453779b480ab6f0dcda19b5e219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaff838d6a046dfb7756cd11420d18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  bad\n",
      "superlative of big is biggest.\n",
      "superlative of bright is brightest.\n",
      "superlative of {} is\n",
      "tensor(52.3438, device='cuda:0', dtype=torch.float16) tensor(254.1250, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  bright\n",
      "superlative of easy is easiest.\n",
      "superlative of big is biggest.\n",
      "superlative of {} is\n",
      "tensor(44.6875, device='cuda:0', dtype=torch.float16) tensor(258.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  easy\n",
      "superlative of bright is brightest.\n",
      "superlative of bad is worst.\n",
      "superlative of {} is\n",
      "tensor(56.1250, device='cuda:0', dtype=torch.float16) tensor(319.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  big\n",
      "superlative of bright is brightest.\n",
      "superlative of easy is easiest.\n",
      "superlative of {} is\n",
      "tensor(49.0938, device='cuda:0', dtype=torch.float16) tensor(278.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f5fb2ebb2c4883ac870ec964806a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  fast\n",
      "superlative of close is closest.\n",
      "superlative of easy is easiest.\n",
      "superlative of {} is\n",
      "tensor(46.3750, device='cuda:0', dtype=torch.float16) tensor(260.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  bad\n",
      "superlative of close is closest.\n",
      "superlative of fast is fastest.\n",
      "superlative of {} is\n",
      "tensor(55.7500, device='cuda:0', dtype=torch.float16) tensor(254.3750, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  close\n",
      "superlative of fast is fastest.\n",
      "superlative of bad is worst.\n",
      "superlative of {} is\n",
      "tensor(57.1250, device='cuda:0', dtype=torch.float16) tensor(301.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  easy\n",
      "superlative of close is closest.\n",
      "superlative of bad is worst.\n",
      "superlative of {} is\n",
      "tensor(56.6562, device='cuda:0', dtype=torch.float16) tensor(256.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32864ef135f348f9b510bbc9ed2f8140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  early\n",
      "superlative of fine is finest.\n",
      "superlative of fast is fastest.\n",
      "superlative of {} is\n",
      "tensor(55.5625, device='cuda:0', dtype=torch.float16) tensor(268., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  fine\n",
      "superlative of fast is fastest.\n",
      "superlative of early is earliest.\n",
      "superlative of {} is\n",
      "tensor(56.5625, device='cuda:0', dtype=torch.float16) tensor(265.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  close\n",
      "superlative of fine is finest.\n",
      "superlative of early is earliest.\n",
      "superlative of {} is\n",
      "tensor(56.3438, device='cuda:0', dtype=torch.float16) tensor(259.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  fast\n",
      "superlative of fine is finest.\n",
      "superlative of close is closest.\n",
      "superlative of {} is\n",
      "tensor(49.7500, device='cuda:0', dtype=torch.float16) tensor(265.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "        (b, -1, s) for b, c, s in filter_single_token[:10]\n",
    "    ]\n",
    "print(samples)\n",
    "\n",
    "weights_and_biases = get_multiple_averaged_JB(\n",
    "    samples, \n",
    "    relation_prompt=\"superlative of {} is\", \n",
    "    N = 3, \n",
    "    calculate_at_lnf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'shortest' in objects[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long, target: longest   ==>   predicted: [' longest', ' deepest', ' longer', ' largest', ' latest']\n",
      "low, target: lowest   ==>   predicted: [' lowest', 'lowest', ' lightest', ' weakest', ' smallest']\n",
      "short, target: shortest   ==>   predicted: [' shortest', ' longest', ' smallest', ' lightest', ' earliest']\n",
      "simple, target: simplest   ==>   predicted: [' simplest', ' easiest', ' lightest', ' smallest', ' finest']\n",
      "small, target: smallest   ==>   predicted: [' smallest', ' lowest', ' lightest', ' weakest', ' finest']\n",
      "steep, target: steepest   ==>   predicted: [' steepest', ' deepest', ' highest', ' steep', ' strongest']\n",
      "strong, target: strongest   ==>   predicted: [' strongest', ' brightest', ' best', ' greatest', ' highest']\n",
      "weak, target: weakest   ==>   predicted: [' weakest', ' strongest', ' lightest', ' worst', ' lowest']\n",
      "young, target: youngest   ==>   predicted: [' youngest', ' oldest', ' young', ' earliest', ' youth']\n"
     ]
    }
   ],
   "source": [
    "relation_operator = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer= tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.stack(\n",
    "        [wb['weight'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0),\n",
    "    # bias = torch.stack(\n",
    "    #     [wb['bias'] for wb in weights_and_biases]\n",
    "    # ).mean(dim=0),\n",
    "    bias = lst_sq_corner,\n",
    "\n",
    "    layer_name_format = \"model.decoder.layers.{}\",\n",
    "    ln_f_name = \"model.decoder.final_layer_norm\",\n",
    ")\n",
    "\n",
    "check_with_test_cases(relation_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' most', 16.906),\n",
       " (' best', 16.422),\n",
       " (' the', 15.648),\n",
       " (' greatest', 13.75),\n",
       " (' ', 13.617)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(\n",
    "    torch.stack(\n",
    "        [wb['bias'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0), get_logits=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4104f34302edfbfea7294aa0a5e7d82342a152e8e30f6673f70b28d5f99d4ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
