{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from baukit import nethook\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B ==> device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month ===>  M\n",
      "major ===>  A\n",
      "star ===>  S\n",
      "areas ===>  A\n",
      "future ===>  F\n",
      "space ===>  S\n",
      "committee ===>  C\n",
      "london ===>  L\n",
      "washington ===>  W\n",
      "meeting ===>  M\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"grape starts with G\n",
    "monitor starts with M\n",
    "{} starts with\"\"\"\n",
    "\n",
    "words = ['month', 'major', 'star', 'areas', 'future', 'space', 'committee', 'london', 'washington', 'meeting']\n",
    "\n",
    "# prompt = \"\"\"grape ends with E\n",
    "# monitor ends with R\n",
    "# glass ends with\"\"\"\n",
    "\n",
    "for w in words:\n",
    "    txt, ret_dict = model_utils.generate_fast(\n",
    "        model, tokenizer, \n",
    "        prompts=[prompt.format(w)], max_new_tokens=10, \n",
    "        get_answer_tokens=True, argmax_greedy=True\n",
    "    )\n",
    "    print(f\"{w} ===> {ret_dict['answer'][0]['top_token']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relations.corner import CornerEstimator\n",
    "corner_estimator = CornerEstimator(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "objects = list(string.ascii_uppercase)\n",
    "objects = [\" \" + o for o in objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.875 [' C', ' S', ' M', ' L', ' P']\n"
     ]
    }
   ],
   "source": [
    "simple_corner = corner_estimator.estimate_simple_corner(objects, scale_up=70)\n",
    "print(simple_corner.norm().item(), corner_estimator.get_vocab_representation(simple_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_inv_corner = corner_estimator.estimate_lin_inv_corner(objects, target_logit_value=50)\n",
    "# print(lin_inv_corner.norm().item(), corner_estimator.get_vocab_representation(lin_inv_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_corner = corner_estimator.estimate_average_corner_with_gradient_descent(objects, average_on=5, target_logit_value=50, verbose=False)\n",
    "# print(avg_corner.norm().item(), corner_estimator.get_vocab_representation(avg_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_test_cases(relation_operator):\n",
    "\n",
    "    test_cases = [\n",
    "        (w, -1, w[0].upper()) for w in words\n",
    "    ]\n",
    "\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        objects = relation_operator(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {objects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month, target: M   ==>   predicted: [' C', ' S', ' M', ' P', ' B']\n",
      "major, target: M   ==>   predicted: [' M', ' C', ' D', ' P', ' S']\n",
      "star, target: S   ==>   predicted: [' S', ' M', ' T', ' P', ' C']\n",
      "areas, target: A   ==>   predicted: [' C', ' B', ' S', ' P', ' F']\n",
      "future, target: F   ==>   predicted: [' T', ' C', ' E', ' D', ' B']\n",
      "space, target: S   ==>   predicted: [' S', ' M', ' P', ' B', ' R']\n",
      "committee, target: C   ==>   predicted: [' C', ' B', ' T', ' D', ' S']\n",
      "london, target: L   ==>   predicted: [' C', ' B', ' S', ' D', ' P']\n",
      "washington, target: W   ==>   predicted: [' D', ' C', ' L', ' S', ' B']\n",
      "meeting, target: M   ==>   predicted: [' C', ' T', ' S', ' M', ' B']\n"
     ]
    }
   ],
   "source": [
    "relation = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.eye(model.config.n_embd).to(model.dtype).to(model.device),\n",
    "    bias = simple_corner\n",
    ")\n",
    "check_with_test_cases(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_JB(top_performers, relation_prompt, num_icl = 3, calculate_at_lnf = False):\n",
    "    try:\n",
    "        jbs = []\n",
    "        for s, s_idx, o in tqdm(top_performers):\n",
    "            others = set(top_performers) - {(s, s_idx, o)}\n",
    "            others = random.sample(list(others), k = min(num_icl, len(list(others)))) \n",
    "            prompt = \"\"\n",
    "            prompt += \"\\n\".join(relation_prompt.format(s_other) + f\" {o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "            prompt += relation_prompt\n",
    "            print(\"subject: \", s)\n",
    "            print(prompt)\n",
    "\n",
    "            jb, _ = estimate.relation_operator_from_sample(\n",
    "                model, tokenizer,\n",
    "                s, prompt,\n",
    "                subject_token_index= s_idx,\n",
    "                layer = 15,\n",
    "                device = model.device,\n",
    "                # calculate_at_lnf = calculate_at_lnf\n",
    "            )\n",
    "            print(jb.weight.norm(), jb.bias.norm())\n",
    "            print()\n",
    "            jbs.append(jb)\n",
    "        \n",
    "        weight = torch.stack([jb.weight for jb in jbs]).mean(dim=0)\n",
    "        bias  = torch.stack([jb.bias for jb in jbs]).mean(dim=0)\n",
    "\n",
    "        return weight, bias\n",
    "    except RuntimeError as e:\n",
    "        if(str(e).startswith(\"CUDA out of memory\")):\n",
    "            print(\"CUDA out of memory\")\n",
    "        if(num_icl > 1):\n",
    "            num_icl -= 1\n",
    "            print(\"trying with smaller icl >> \", num_icl)\n",
    "            return get_averaged_JB(top_performers, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        else:\n",
    "            raise Exception(\"RuntimeError >> can't calculate Jacobian with minimum number of icl examples\")\n",
    "\n",
    "def get_multiple_averaged_JB(top_performers, relation_prompt, N = 3, num_icl = 2, calculate_at_lnf = False):\n",
    "    weights_and_biases = []\n",
    "    sample_size = min(len(top_performers), num_icl + 2)\n",
    "    for _ in tqdm(range(N)):\n",
    "        cur_sample = random.sample(top_performers, k = sample_size)\n",
    "        weight, bias = get_averaged_JB(cur_sample, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        weights_and_biases.append({\n",
    "            'weight': weight,\n",
    "            'bias'  : bias\n",
    "        })\n",
    "    return weights_and_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('major', -1, ' M'), ('star', -1, ' S'), ('areas', -1, ' A'), ('future', -1, ' F')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dcbc95ea2641ae9036fbfa522e8be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad068872e9943fe8e2bce4c97f2112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  future\n",
      "star starts with  S.\n",
      "major starts with  M.\n",
      "{} starts with\n",
      "tensor(22.5000, device='cuda:0', dtype=torch.float16) tensor(387.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  areas\n",
      "star starts with  S.\n",
      "future starts with  F.\n",
      "{} starts with\n",
      "tensor(17.0156, device='cuda:0', dtype=torch.float16) tensor(412.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  major\n",
      "future starts with  F.\n",
      "areas starts with  A.\n",
      "{} starts with\n",
      "tensor(19.5781, device='cuda:0', dtype=torch.float16) tensor(399.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  star\n",
      "areas starts with  A.\n",
      "major starts with  M.\n",
      "{} starts with\n",
      "tensor(1.1416, device='cuda:0', dtype=torch.float16) tensor(413.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54641263b134f2b93f6299e9f1414a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  major\n",
      "star starts with  S.\n",
      "areas starts with  A.\n",
      "{} starts with\n",
      "tensor(19.8438, device='cuda:0', dtype=torch.float16) tensor(407.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  future\n",
      "major starts with  M.\n",
      "star starts with  S.\n",
      "{} starts with\n",
      "tensor(21.2031, device='cuda:0', dtype=torch.float16) tensor(386.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  star\n",
      "major starts with  M.\n",
      "areas starts with  A.\n",
      "{} starts with\n",
      "tensor(3.6602, device='cuda:0', dtype=torch.float16) tensor(399.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  areas\n",
      "future starts with  F.\n",
      "major starts with  M.\n",
      "{} starts with\n",
      "tensor(21.2656, device='cuda:0', dtype=torch.float16) tensor(392.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cd38dbb86f480db722090578a58233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  star\n",
      "major starts with  M.\n",
      "areas starts with  A.\n",
      "{} starts with\n",
      "tensor(3.6602, device='cuda:0', dtype=torch.float16) tensor(399.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  areas\n",
      "major starts with  M.\n",
      "star starts with  S.\n",
      "{} starts with\n",
      "tensor(21.4688, device='cuda:0', dtype=torch.float16) tensor(396.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  future\n",
      "star starts with  S.\n",
      "major starts with  M.\n",
      "{} starts with\n",
      "tensor(22.5000, device='cuda:0', dtype=torch.float16) tensor(387.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  major\n",
      "star starts with  S.\n",
      "future starts with  F.\n",
      "{} starts with\n",
      "tensor(26.1875, device='cuda:0', dtype=torch.float16) tensor(378.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [(w, -1, \" \" + w[0].upper()) for w in words][1:5]\n",
    "print(samples)\n",
    "\n",
    "weights_and_biases = get_multiple_averaged_JB(\n",
    "    samples, \n",
    "    relation_prompt=\"{} starts with\", \n",
    "    N = 3, \n",
    "    calculate_at_lnf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grape starts with G\\nmonitor starts with M\\n{} starts with'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month, target: M   ==>   predicted: [' M', ' C', ' T', ' L', ' B']\n",
      "major, target: M   ==>   predicted: [' M', ' D', ' C', ' T', ' S']\n",
      "star, target: S   ==>   predicted: [' T', ' L', ' C', ' P', ' S']\n",
      "areas, target: A   ==>   predicted: [' T', ' M', ' S', ' C', ' P']\n",
      "future, target: F   ==>   predicted: [' M', ' D', ' C', ' S', ' P']\n",
      "space, target: S   ==>   predicted: [' S', ' T', ' M', ' C', ' P']\n",
      "committee, target: C   ==>   predicted: [' C', ' T', ' M', ' D', ' P']\n",
      "london, target: L   ==>   predicted: [' L', ' M', ' C', ' D', ' T']\n",
      "washington, target: W   ==>   predicted: [' D', ' W', ' M', ' C', ' S']\n",
      "meeting, target: M   ==>   predicted: [' M', ' T', ' C', ' P', ' S']\n"
     ]
    }
   ],
   "source": [
    "relation_operator = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer= tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.stack(\n",
    "        [wb['weight'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0),\n",
    "    # bias = torch.stack(\n",
    "    #     [wb['bias'] for wb in weights_and_biases]\n",
    "    # ).mean(dim=0),\n",
    "    bias = simple_corner\n",
    ")\n",
    "\n",
    "check_with_test_cases(relation_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' a', ' the', ' A', ' an']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(\n",
    "    torch.stack(\n",
    "        [wb['bias'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġfifteen']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\" fifteen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3439fe3f7dcaddaf51997811d25ada8e7c0985d2997d22a3ed461af94d2f9f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
