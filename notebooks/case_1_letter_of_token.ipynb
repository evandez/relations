{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from baukit import nethook\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B ==> device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month ===>  M\n",
      "major ===>  A\n",
      "star ===>  S\n",
      "areas ===>  A\n",
      "future ===>  F\n",
      "space ===>  S\n",
      "committee ===>  C\n",
      "london ===>  L\n",
      "washington ===>  W\n",
      "meeting ===>  M\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"grape starts with G\n",
    "monitor starts with M\n",
    "{} starts with\"\"\"\n",
    "\n",
    "words = ['month', 'major', 'star', 'areas', 'future', 'space', 'committee', 'london', 'washington', 'meeting']\n",
    "\n",
    "# prompt = \"\"\"grape ends with E\n",
    "# monitor ends with R\n",
    "# glass ends with\"\"\"\n",
    "\n",
    "for w in words:\n",
    "    txt, ret_dict = model_utils.generate_fast(\n",
    "        model, tokenizer, \n",
    "        prompts=[prompt.format(w)], max_new_tokens=10, \n",
    "        get_answer_tokens=True, argmax_greedy=True\n",
    "    )\n",
    "    print(f\"{w} ===> {ret_dict['answer'][0]['top_token']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"grape is spelled G-R-A-P-E\n",
    "# monitor starts with M-O-N-I-T-O-R\n",
    "# {} is spelled\"\"\"\n",
    "    \n",
    "# txt, ret_dict = model_utils.generate_fast(\n",
    "#     model, tokenizer, \n",
    "#     prompts=[prompt.format('arnab')], max_new_tokens=20, \n",
    "#     get_answer_tokens=True, argmax_greedy=True\n",
    "# )\n",
    "# txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relations.corner import CornerEstimator\n",
    "corner_estimator = CornerEstimator(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "objects = list(string.ascii_uppercase)\n",
    "objects = [\" \" + o for o in objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.875 [(' C', 117.062), (' S', 117.0), (' M', 116.812), (' L', 116.688), (' P', 116.438)]\n"
     ]
    }
   ],
   "source": [
    "simple_corner = corner_estimator.estimate_simple_corner(objects, scale_up=70)\n",
    "print(simple_corner.norm().item(), corner_estimator.get_vocab_representation(simple_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.5 [(' S', 46.125), (' C', 46.0), (' D', 45.781), (' P', 45.062), (' R', 45.031)]\n"
     ]
    }
   ],
   "source": [
    "lin_inv_corner = corner_estimator.estimate_lin_inv_corner(objects, target_logit_value=50)\n",
    "print(lin_inv_corner.norm().item(), corner_estimator.get_vocab_representation(lin_inv_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.5625 [(' Q', 109.312), (' X', 109.188), (' I', 109.188), (' Y', 109.125), (' O', 109.125)]\n"
     ]
    }
   ],
   "source": [
    "lst_sq_corner = corner_estimator.estimate_corner_lstsq_solve(objects, target_logit=40)\n",
    "print(lst_sq_corner.norm().item(), corner_estimator.get_vocab_representation(lst_sq_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'major',\n",
       " 'star',\n",
       " 'areas',\n",
       " 'future',\n",
       " 'space',\n",
       " 'committee',\n",
       " 'london',\n",
       " 'washington',\n",
       " 'meeting']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_test_cases(relation_operator, word_list = words):\n",
    "\n",
    "    test_cases = [\n",
    "        (w, -1, w[0].upper()) for w in word_list\n",
    "    ]\n",
    "\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        objects = relation_operator(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {objects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month, target: M   ==>   predicted: [' C', ' S', ' M', ' P', ' B']\n",
      "major, target: M   ==>   predicted: [' M', ' C', ' D', ' P', ' S']\n",
      "star, target: S   ==>   predicted: [' S', ' M', ' T', ' P', ' C']\n",
      "areas, target: A   ==>   predicted: [' C', ' B', ' S', ' P', ' F']\n",
      "future, target: F   ==>   predicted: [' T', ' C', ' D', ' E', ' B']\n",
      "space, target: S   ==>   predicted: [' S', ' M', ' P', ' B', ' R']\n",
      "committee, target: C   ==>   predicted: [' C', ' B', ' T', ' D', ' S']\n",
      "london, target: L   ==>   predicted: [' C', ' B', ' S', ' D', ' P']\n",
      "washington, target: W   ==>   predicted: [' D', ' C', ' L', ' S', ' B']\n",
      "meeting, target: M   ==>   predicted: [' C', ' T', ' S', ' M', ' B']\n"
     ]
    }
   ],
   "source": [
    "relation = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.eye(model.config.n_embd).to(model.dtype).to(model.device),\n",
    "    bias = simple_corner\n",
    ")\n",
    "check_with_test_cases(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_JB(top_performers, relation_prompt, num_icl = 3, calculate_at_lnf = False):\n",
    "    try:\n",
    "        jbs = []\n",
    "        for s, s_idx, o in tqdm(top_performers):\n",
    "            others = set(top_performers) - {(s, s_idx, o)}\n",
    "            others = random.sample(list(others), k = min(num_icl, len(list(others)))) \n",
    "            prompt = \"\"\n",
    "            prompt += \"\\n\".join(relation_prompt.format(s_other) + f\"{o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "            prompt += relation_prompt\n",
    "            print(\"subject: \", s)\n",
    "            print(prompt)\n",
    "\n",
    "            jb, _ = estimate.relation_operator_from_sample(\n",
    "                model, tokenizer,\n",
    "                s, prompt,\n",
    "                subject_token_index= s_idx,\n",
    "                layer = 15,\n",
    "                device = model.device,\n",
    "                # calculate_at_lnf = calculate_at_lnf\n",
    "            )\n",
    "            print(jb.weight.norm(), jb.bias.norm())\n",
    "            print()\n",
    "            jbs.append(jb)\n",
    "        \n",
    "        weight = torch.stack([jb.weight for jb in jbs]).mean(dim=0)\n",
    "        bias  = torch.stack([jb.bias for jb in jbs]).mean(dim=0)\n",
    "\n",
    "        return weight, bias\n",
    "    except RuntimeError as e:\n",
    "        if(str(e).startswith(\"CUDA out of memory\")):\n",
    "            print(\"CUDA out of memory\")\n",
    "        if(num_icl > 1):\n",
    "            num_icl -= 1\n",
    "            print(\"trying with smaller icl >> \", num_icl)\n",
    "            return get_averaged_JB(top_performers, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        else:\n",
    "            raise Exception(\"RuntimeError >> can't calculate Jacobian with minimum number of icl examples\")\n",
    "\n",
    "def get_multiple_averaged_JB(top_performers, relation_prompt, N = 3, num_icl = 2, calculate_at_lnf = False):\n",
    "    weights_and_biases = []\n",
    "    sample_size = min(len(top_performers), num_icl + 2)\n",
    "    for _ in tqdm(range(N)):\n",
    "        cur_sample = random.sample(top_performers, k = sample_size)\n",
    "        weight, bias = get_averaged_JB(cur_sample, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        weights_and_biases.append({\n",
    "            'weight': weight,\n",
    "            'bias'  : bias\n",
    "        })\n",
    "    return weights_and_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('month', -1, ' M'), ('major', -1, ' M'), ('star', -1, ' S'), ('areas', -1, ' A'), ('future', -1, ' F'), ('space', -1, ' S'), ('committee', -1, ' C'), ('london', -1, ' L'), ('washington', -1, ' W'), ('meeting', -1, ' M')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e071843a661b4761871b767237d5664b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3171197d171845d6b3490975d633a554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  future\n",
      " space starts with S.\n",
      " london starts with L.\n",
      " {} starts with\n",
      "tensor(29., device='cuda:0', dtype=torch.float16) tensor(358.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  london\n",
      " areas starts with A.\n",
      " future starts with F.\n",
      " {} starts with\n",
      "tensor(34.9375, device='cuda:0', dtype=torch.float16) tensor(308.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  areas\n",
      " space starts with S.\n",
      " future starts with F.\n",
      " {} starts with\n",
      "tensor(24.3594, device='cuda:0', dtype=torch.float16) tensor(382.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  space\n",
      " london starts with L.\n",
      " future starts with F.\n",
      " {} starts with\n",
      "tensor(22.4844, device='cuda:0', dtype=torch.float16) tensor(296.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb711b4b41240308ebcdcc24b770bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  major\n",
      " space starts with S.\n",
      " areas starts with A.\n",
      " {} starts with\n",
      "tensor(31.6562, device='cuda:0', dtype=torch.float16) tensor(369.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  space\n",
      " future starts with F.\n",
      " areas starts with A.\n",
      " {} starts with\n",
      "tensor(19.2500, device='cuda:0', dtype=torch.float16) tensor(351.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  areas\n",
      " major starts with M.\n",
      " future starts with F.\n",
      " {} starts with\n",
      "tensor(20.4688, device='cuda:0', dtype=torch.float16) tensor(354., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  future\n",
      " major starts with M.\n",
      " areas starts with A.\n",
      " {} starts with\n",
      "tensor(20.2656, device='cuda:0', dtype=torch.float16) tensor(365., device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403c9208291a439386443cd9ec3a0b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:  future\n",
      " london starts with L.\n",
      " star starts with S.\n",
      " {} starts with\n",
      "tensor(27.4375, device='cuda:0', dtype=torch.float16) tensor(342.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  london\n",
      " meeting starts with M.\n",
      " future starts with F.\n",
      " {} starts with\n",
      "tensor(37.5000, device='cuda:0', dtype=torch.float16) tensor(320.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  meeting\n",
      " london starts with L.\n",
      " future starts with F.\n",
      " {} starts with\n",
      "tensor(35.2500, device='cuda:0', dtype=torch.float16) tensor(365.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:  star\n",
      " future starts with F.\n",
      " london starts with L.\n",
      " {} starts with\n",
      "tensor(1.7354, device='cuda:0', dtype=torch.float16) tensor(387.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [(w, -1, \" \" + w[0].upper()) for w in words]\n",
    "print(samples)\n",
    "\n",
    "weights_and_biases = get_multiple_averaged_JB(\n",
    "    samples, \n",
    "    relation_prompt=\" {} starts with\", \n",
    "    N = 3, \n",
    "    calculate_at_lnf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction, target: P   ==>   predicted: [' P', ' Q', ' X', ' Y', ' G']\n",
      "induction, target: I   ==>   predicted: [' I', ' U', ' X', ' N', ' H']\n",
      "antonym, target: A   ==>   predicted: [' N', ' X', ' A', ' Y', ' T']\n",
      "xerox, target: X   ==>   predicted: [' X', ' Q', ' Z', ' R', ' O']\n",
      "geometry, target: G   ==>   predicted: [' G', ' Q', ' E', ' Y', ' R']\n",
      "activation, target: A   ==>   predicted: [' A', ' V', ' X', ' C', ' Y']\n",
      "jerusalem, target: J   ==>   predicted: [' J', ' Y', ' Z', ' U', ' G']\n",
      "blueberry, target: B   ==>   predicted: [' B', ' Q', ' R', ' W', ' G']\n",
      "understand, target: U   ==>   predicted: [' U', ' H', ' N', ' R', ' B']\n",
      "granite, target: G   ==>   predicted: [' G', ' R', ' Q', ' K', ' J']\n",
      "hygiene, target: H   ==>   predicted: [' H', ' G', ' Y', ' Q', ' K']\n",
      "antic, target: A   ==>   predicted: [' A', ' T', ' C', ' N', ' O']\n"
     ]
    }
   ],
   "source": [
    "relation_operator = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer= tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.stack(\n",
    "        [wb['weight'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0),\n",
    "    # bias = torch.stack(\n",
    "    #     [wb['bias'] for wb in weights_and_biases]\n",
    "    # ).mean(dim=0),\n",
    "    bias = lst_sq_corner\n",
    ")\n",
    "\n",
    "check_with_test_cases(\n",
    "    relation_operator,\n",
    "    word_list= [\n",
    "        \"prediction\", \"induction\", \"antonym\", 'xerox', 'geometry',\n",
    "        \"activation\", \"jerusalem\", \"blueberry\", \"understand\", \"granite\",\n",
    "        \"hygiene\", \"antic\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' S', ' A', ' a', ' E', ' M']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(\n",
    "    torch.stack(\n",
    "        [wb['bias'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ä fifteen']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\" fifteen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3439fe3f7dcaddaf51997811d25ada8e7c0985d2997d22a3ed461af94d2f9f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
