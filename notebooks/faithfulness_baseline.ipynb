{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src import models, data\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float16, device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "mt = models.load_model(\"gptj\", device=device)\n",
    "print(\n",
    "    f\"dtype: {mt.model.dtype}, device: {mt.model.device}, memory: {mt.model.get_memory_footprint()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Chicago', 0.51),\n",
       " (' Newark', 0.174),\n",
       " (' Boston', 0.069),\n",
       " (' San', 0.031),\n",
       " (' Houston', 0.014),\n",
       " (' Minneapolis', 0.011),\n",
       " (' Jersey', 0.011),\n",
       " (' London', 0.011),\n",
       " (' Detroit', 0.01),\n",
       " (' Baltimore', 0.009)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.lens import interpret_logits, logit_lens\n",
    "from src.functional import untuple\n",
    "\n",
    "prompt = \"Prudential Tower is located in the city of\"\n",
    "tokenized = mt.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(mt.model.device)\n",
    "\n",
    "import baukit\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    mt.model,\n",
    "    models.determine_layer_paths(mt)\n",
    ") as traces:\n",
    "    output = mt.model(**tokenized)\n",
    "    \n",
    "interpret_logits(mt, output.logits[0][-1], get_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(' Chicago', 0.514),\n",
       "  (' Newark', 0.172),\n",
       "  (' Boston', 0.068),\n",
       "  (' San', 0.031),\n",
       "  (' Houston', 0.014),\n",
       "  (' Minneapolis', 0.011),\n",
       "  (' Jersey', 0.011),\n",
       "  (' London', 0.011),\n",
       "  (' Detroit', 0.01),\n",
       "  (' Baltimore', 0.009)],\n",
       " {tensor(7312, device='cuda:0'): (0.0017547607421875, ' Seattle'),\n",
       "  tensor(6342, device='cuda:0'): (3.3974647521972656e-06, ' Paris'),\n",
       "  tensor(20529, device='cuda:0'): (9.1552734375e-05, ' Dh')})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interested_words = [\" Seattle\", \" Paris\", \" Dhaka\"]\n",
    "int_tokenized = mt.tokenizer(interested_words, return_tensors=\"pt\", padding=True).to(\n",
    "    mt.model.device\n",
    ")\n",
    "int_tokenized.input_ids\n",
    "\n",
    "z = untuple(traces[models.determine_layer_paths(mt)[-1]].output)[0][-1]\n",
    "print(z.shape)\n",
    "\n",
    "logit_lens(mt, z, [t[0] for t in int_tokenized.input_ids], get_proba=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $F(\\mathbf{h_{s}}) = \\mathbf{h_{s}}$, set $W_{r} = I$ and $bias = \\mathbf{0}$, basically logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.operators import LinearRelationOperator\n",
    "\n",
    "logit_lens_operator = LinearRelationOperator(\n",
    "    mt = mt, \n",
    "    h_layer = -1,\n",
    "    weight = None, bias = None, # basically logit lens if both weight and bias set to None\n",
    "    prompt_template=\"{} is located in the city of\",\n",
    "    z_layer = -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRelationOutput(predictions=[PredictedToken(token=' Chicago', prob=0.5139787793159485), PredictedToken(token=' Newark', prob=0.17216132581233978), PredictedToken(token=' Boston', prob=0.06848104298114777), PredictedToken(token=' San', prob=0.030866824090480804), PredictedToken(token=' Houston', prob=0.013804498128592968), PredictedToken(token=' Minneapolis', prob=0.011266903951764107), PredictedToken(token=' Jersey', prob=0.011092226952314377), PredictedToken(token=' London', prob=0.010501908138394356), PredictedToken(token=' Detroit', prob=0.010178797878324986), PredictedToken(token=' Baltimore', prob=0.00850470457226038)], h=tensor([-1.4648,  0.7959, -0.9663,  ..., -0.6025,  0.8594, -4.9844],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>), z=tensor([-1.4648,  0.7959, -0.9663,  ..., -0.6025,  0.8594, -4.9844],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_lens_operator(\n",
    "    subject = \"The Space Needle\",\n",
    "    k = 10,\n",
    "    h = z\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50400, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unembedding = baukit.nethook.get_module(mt.model, \"lm_head\")\n",
    "unembedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRelationOutput(predictions=[PredictedToken(token=' Chicago', prob=1.0), PredictedToken(token='Chicago', prob=6.665581192860808e-23), PredictedToken(token=' Detroit', prob=3.3952676397964373e-28), PredictedToken(token=' Illinois', prob=2.333531109663677e-28), PredictedToken(token=' Boston', prob=5.542621749889874e-29), PredictedToken(token=' Milwaukee', prob=4.0150617628607155e-30), PredictedToken(token=' Philadelphia', prob=3.1269332749387515e-30), PredictedToken(token=' Seattle', prob=6.157297243929202e-31), PredictedToken(token=' Toronto', prob=3.096083083978257e-31), PredictedToken(token=' Atlanta', prob=1.3738785357297722e-31)], h=tensor([-0.0243, -0.0335, -0.0092,  ..., -0.0075,  0.0104,  0.0171],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>), z=tensor([-0.0243, -0.0335, -0.0092,  ..., -0.0075,  0.0104,  0.0171],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = \" Chicago\"\n",
    "emb_subject = unembedding.weight[mt.tokenizer(subject).input_ids[0]]\n",
    "logit_lens_operator(\n",
    "    subject = \"Whatever\",\n",
    "    k = 10,\n",
    "    h = emb_subject\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data import load_dataset\n",
    "dataset = load_dataset()\n",
    "cur_relation = [\n",
    "    d for d in dataset if d.name == \"country capital city\"\n",
    "][0]\n",
    "train, test = cur_relation.split(size = 10)\n",
    "len(train.samples), len(test.samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICL-Mean, our flagship method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    }
   ],
   "source": [
    "from src.operators import JacobianIclMeanEstimator\n",
    "\n",
    "mean_estimator = JacobianIclMeanEstimator(\n",
    "    mt = mt,\n",
    "    h_layer = 12,\n",
    "    beta= 0.5\n",
    ")\n",
    "\n",
    "icl_mean = mean_estimator(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Linear Model baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    }
   ],
   "source": [
    "from src.operators import LearnedLinearEstimatorBaseline\n",
    "    \n",
    "learned_estimator = LearnedLinearEstimatorBaseline(\n",
    "    mt=mt,\n",
    "    h_layer=15,\n",
    ")\n",
    "\n",
    "learned_operator = learned_estimator(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset Model (a simpler version of the `corner` approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    }
   ],
   "source": [
    "from src.operators import OffsetEstimatorBaseline\n",
    "    \n",
    "offset_estimator = OffsetEstimatorBaseline(\n",
    "    mt=mt,\n",
    "    h_layer=15,\n",
    "    # scaling_factor=70\n",
    ")\n",
    "\n",
    "offset_operator = offset_estimator(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PredictedToken(token='...', prob=0.04671017453074455), PredictedToken(token=' Ben', prob=0.03872409835457802), PredictedToken(token='...', prob=0.02312321402132511), PredictedToken(token=' politically', prob=0.012090252712368965), PredictedToken(token=' capital', prob=0.011357740499079227)]\n",
      "[PredictedToken(token=' Paris', prob=0.9594237208366394), PredictedToken(token=' French', prob=0.028972085565328598), PredictedToken(token=' France', prob=0.008835986256599426), PredictedToken(token=' Franc', prob=0.0010888072429224849), PredictedToken(token='Paris', prob=0.0006203835946507752)]\n",
      "[PredictedToken(token=' Paris', prob=0.926771879196167), PredictedToken(token=' Moscow', prob=0.032719049602746964), PredictedToken(token=' Berlin', prob=0.012418750673532486), PredictedToken(token=' Tokyo', prob=0.007075990084558725), PredictedToken(token=' London', prob=0.007075990084558725)]\n"
     ]
    }
   ],
   "source": [
    "subject = \"France\"\n",
    "\n",
    "print(learned_operator(subject).predictions)\n",
    "print(icl_mean(subject).predictions)\n",
    "print(offset_operator(subject).predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the `hparams` and calculating results for each baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import functional, metrics\n",
    "from src.operators import (\n",
    "    JacobianEstimator,\n",
    "    LinearRelationOperator,\n",
    "    OffsetEstimatorBaseline,\n",
    "    LearnedLinearEstimatorBaseline,\n",
    "    JacobianIclMeanEstimator,\n",
    ")\n",
    "\n",
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        operator: LinearRelationOperator, \n",
    "        test_set: data.Relation, \n",
    "        k: int = 10\n",
    ")-> dict:\n",
    "    pred_objects = []\n",
    "    test_objects = [x.object for x in test_set.samples]\n",
    "    subject_to_pred = {}\n",
    "    for sample in test_set.samples:\n",
    "        preds = operator(subject = sample.subject, k=k)\n",
    "        pred_objects.append([p.token for p in preds.predictions])\n",
    "        subject_to_pred[sample.subject] = [p.token for p in preds.predictions]\n",
    "    return {\n",
    "        'recall': metrics.recall(pred_objects, test_objects),\n",
    "        'predictions': subject_to_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_path = \"../hparams/gptj\"\n",
    "save_dir = \"../results/faithfulness_baselines\"\n",
    "N_TRIALS = 3\n",
    "N_TRAINING = functional.DEFAULT_N_ICL_LM\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country capital city | h_layer: 6 | beta: 0.38333334028720856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09097699babb49a6b44ecc13ff4956bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "filter dataset:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known samples: 24/24\n",
      "prompt template: The capital city of {} is\n",
      "\n",
      "trial 1/3\n",
      "train: ['Nigeria -> Abuja', 'Venezuela -> Caracas', 'Australia -> Canberra', 'Mexico -> Mexico City', 'Chile -> Santiago']\n",
      "zero shot recall: [0.2631578947368421, 0.42105263157894735, 0.47368421052631576, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit lens recall: [0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.10526315789473684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset recall: [0.05263157894736842, 0.15789473684210525, 0.15789473684210525, 0.21052631578947367, 0.3157894736842105, 0.3684210526315789, 0.3684210526315789, 0.3684210526315789, 0.47368421052631576, 0.5263157894736842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned recall: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icl mean recall (emb): [0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8421052631578947, 0.8947368421052632, 0.8947368421052632, 0.8947368421052632, 0.8947368421052632, 0.8947368421052632]\n",
      "icl mean recall: [0.9473684210526315, 0.9473684210526315, 0.9473684210526315, 0.9473684210526315, 0.9473684210526315, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "trial 2/3\n",
      "train: ['Spain -> Madrid', 'Brazil -> Bras\\\\u00edlia', 'Turkey -> Ankara', 'South Korea -> Seoul', 'United States -> Washington D.C.']\n",
      "zero shot recall: [0.3684210526315789, 0.47368421052631576, 0.47368421052631576, 0.47368421052631576, 0.47368421052631576, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527, 0.5789473684210527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit lens recall: [0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.10526315789473684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset recall: [0.05263157894736842, 0.10526315789473684, 0.10526315789473684, 0.15789473684210525, 0.21052631578947367, 0.2631578947368421, 0.2631578947368421, 0.2631578947368421, 0.3684210526315789, 0.3684210526315789]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned recall: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05263157894736842, 0.05263157894736842, 0.10526315789473684, 0.15789473684210525]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icl mean recall (emb): [0.21052631578947367, 0.2631578947368421, 0.3684210526315789, 0.3684210526315789, 0.5789473684210527, 0.631578947368421, 0.631578947368421, 0.631578947368421, 0.6842105263157895, 0.6842105263157895]\n",
      "icl mean recall: [0.8421052631578947, 0.8947368421052632, 0.9473684210526315, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "trial 3/3\n",
      "train: ['South Korea -> Seoul', 'Germany -> Berlin', 'United States -> Washington D.C.', 'Spain -> Madrid', 'Brazil -> Bras\\\\u00edlia']\n",
      "zero shot recall: [0.21052631578947367, 0.2631578947368421, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit lens recall: [0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842, 0.05263157894736842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset recall: [0.05263157894736842, 0.10526315789473684, 0.10526315789473684, 0.10526315789473684, 0.15789473684210525, 0.21052631578947367, 0.21052631578947367, 0.21052631578947367, 0.3157894736842105, 0.3157894736842105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "relation has > 1 prompt_templates, will use first (The capital city of {} is)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned recall: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05263157894736842, 0.10526315789473684]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 104\u001b[0m\n\u001b[1;32m     97\u001b[0m trial_results[\u001b[39m\"\u001b[39m\u001b[39mlearned_linear\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m learned_recall\n\u001b[1;32m     99\u001b[0m mean_emb_estimator \u001b[39m=\u001b[39m JacobianIclMeanEstimator(\n\u001b[1;32m    100\u001b[0m     mt\u001b[39m=\u001b[39mmt,\n\u001b[1;32m    101\u001b[0m     h_layer \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39memb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m     beta\u001b[39m=\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    103\u001b[0m )\n\u001b[0;32m--> 104\u001b[0m mean_emb_operator \u001b[39m=\u001b[39m mean_emb_estimator(train)\n\u001b[1;32m    105\u001b[0m mean_emb_recall \u001b[39m=\u001b[39m evaluate(mean_emb_operator, test)\n\u001b[1;32m    106\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39micl mean recall (emb): \u001b[39m\u001b[39m{\u001b[39;00mmean_emb_recall[\u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Codes/relations/notebooks/../src/operators.py:236\u001b[0m, in \u001b[0;36mJacobianIclMeanEstimator.__call__\u001b[0;34m(self, relation)\u001b[0m\n\u001b[1;32m    229\u001b[0m     h_index, inputs \u001b[39m=\u001b[39m functional\u001b[39m.\u001b[39mfind_subject_token_index(\n\u001b[1;32m    230\u001b[0m         mt\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmt,\n\u001b[1;32m    231\u001b[0m         prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m    232\u001b[0m         subject\u001b[39m=\u001b[39msample\u001b[39m.\u001b[39msubject,\n\u001b[1;32m    233\u001b[0m     )\n\u001b[1;32m    234\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnote that subject=\u001b[39m\u001b[39m{\u001b[39;00msample\u001b[39m.\u001b[39msubject\u001b[39m}\u001b[39;00m\u001b[39m, h_index=\u001b[39m\u001b[39m{\u001b[39;00mh_index\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     approx \u001b[39m=\u001b[39m functional\u001b[39m.\u001b[39;49morder_1_approx(\n\u001b[1;32m    237\u001b[0m         mt\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmt,\n\u001b[1;32m    238\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m    239\u001b[0m         h_layer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mh_layer,\n\u001b[1;32m    240\u001b[0m         h_index\u001b[39m=\u001b[39;49mh_index,\n\u001b[1;32m    241\u001b[0m         z_layer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mz_layer,\n\u001b[1;32m    242\u001b[0m         z_index\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    243\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m     approxes\u001b[39m.\u001b[39mappend(approx)\n\u001b[1;32m    247\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([approx\u001b[39m.\u001b[39mweight \u001b[39mfor\u001b[39;00m approx \u001b[39min\u001b[39;00m approxes])\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Codes/relations/notebooks/../src/functional.py:135\u001b[0m, in \u001b[0;36morder_1_approx\u001b[0;34m(mt, prompt, h_layer, h_index, z_layer, z_index, inputs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         mt\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m    129\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    130\u001b[0m             past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m    131\u001b[0m             use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    132\u001b[0m         )\n\u001b[1;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m untuple(ret[z_layer_name]\u001b[39m.\u001b[39moutput)[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 135\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mjacobian(compute_z_from_h, h, vectorize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    136\u001b[0m bias \u001b[39m=\u001b[39m z[\u001b[39mNone\u001b[39;00m] \u001b[39m-\u001b[39m h[\u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mmm(weight\u001b[39m.\u001b[39mt())\n\u001b[1;32m    137\u001b[0m approx \u001b[39m=\u001b[39m Order1ApproxOutput(\n\u001b[1;32m    138\u001b[0m     h\u001b[39m=\u001b[39mh,\n\u001b[1;32m    139\u001b[0m     h_layer\u001b[39m=\u001b[39mh_layer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m     },\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/autograd/functional.py:657\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    654\u001b[0m         vj[el_idx] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(inputs[el_idx])\u001b[39m.\u001b[39mexpand((\u001b[39msum\u001b[39m(output_numels),) \u001b[39m+\u001b[39m inputs[el_idx]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    655\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(vj)\n\u001b[0;32m--> 657\u001b[0m jacobians_of_flat_output \u001b[39m=\u001b[39m vjp(grad_outputs)\n\u001b[1;32m    659\u001b[0m \u001b[39m# Step 3: The returned jacobian is one big tensor per input. In this step,\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# we split each Tensor by output.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m jacobian_input_output \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/autograd/functional.py:650\u001b[0m, in \u001b[0;36mjacobian.<locals>.vjp\u001b[0;34m(grad_output)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(grad_output):\n\u001b[0;32m--> 650\u001b[0m     vj \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_autograd_grad(flat_outputs, inputs, grad_output, create_graph\u001b[39m=\u001b[39;49mcreate_graph, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    651\u001b[0m     \u001b[39mfor\u001b[39;00m el_idx, vj_el \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(vj):\n\u001b[1;32m    652\u001b[0m         \u001b[39mif\u001b[39;00m vj_el \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/autograd/functional.py:167\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m,) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(inputs)\n\u001b[1;32m    166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(new_outputs, inputs, new_grad_outputs, allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m                                create_graph\u001b[39m=\u001b[39;49mcreate_graph, retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    169\u001b[0m                                is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/autograd/__init__.py:301\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(gO):\n\u001b[1;32m    298\u001b[0m         \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    299\u001b[0m             t_outputs, gO, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    300\u001b[0m             allow_unused, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39;49m_vmap(vjp, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, allow_none_pass_through\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/_vmap_internals.py:223\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     batched_inputs, batch_size \u001b[39m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    221\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mbatched_inputs)\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    225\u001b[0m         _validate_outputs(batched_outputs, func)\n",
      "File \u001b[0;32m~/miniconda3/envs/relations/lib/python3.10/site-packages/torch/autograd/__init__.py:298\u001b[0m, in \u001b[0;36mgrad.<locals>.vjp\u001b[0;34m(gO)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(gO):\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    299\u001b[0m         t_outputs, gO, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    300\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for relation_hparams in os.listdir(hparams_path):\n",
    "    with open(os.path.join(hparams_path, relation_hparams), \"r\") as f:\n",
    "        hparams = json.load(f)\n",
    "    if(hparams['relation_name'] != \"country capital city\"):\n",
    "        continue\n",
    "    print(f\"{hparams['relation_name']} | h_layer: {hparams['h_layer']} | beta: {hparams['beta']}\")\n",
    "    result = {\n",
    "        \"relation_name\": hparams[\"relation_name\"],\n",
    "        \"h_layer\": hparams[\"h_layer\"],\n",
    "        \"beta\": hparams[\"beta\"],\n",
    "    }\n",
    "    cur_relation = dataset.filter(\n",
    "        relation_names=[hparams[\"relation_name\"]],\n",
    "    )\n",
    "    cur_relation_known = functional.filter_dataset_samples(mt=mt, dataset=cur_relation)\n",
    "    cur_relation = cur_relation[0]\n",
    "    cur_relation_known = cur_relation_known[0]\n",
    "\n",
    "    print(f\"known samples: {len(cur_relation_known.samples)}/{len(cur_relation.samples)}\")\n",
    "    result[\"known_samples\"] = len(cur_relation_known.samples)\n",
    "    result[\"total_samples\"] = len(cur_relation.samples)\n",
    "    result[\"trials\"] = []\n",
    "\n",
    "    prompt_template = cur_relation_known.prompt_templates[0]\n",
    "    print(f\"prompt template: {prompt_template}\")\n",
    "    result[\"prompt_template\"] = prompt_template\n",
    "    print()\n",
    "\n",
    "    for trial in range(N_TRIALS):\n",
    "        print(f\"trial {trial + 1}/{N_TRIALS}\")\n",
    "        train, test = cur_relation_known.split(size=N_TRAINING)\n",
    "        print(f\"train: {[str(sample) for sample in train.samples]}\")\n",
    "\n",
    "        icl_prompt = functional.make_prompt(\n",
    "            mt=mt, \n",
    "            prompt_template=prompt_template, \n",
    "            examples=train.samples, \n",
    "            subject=\"{}\"\n",
    "        )\n",
    "\n",
    "        trial_results = {\n",
    "            \"icl_prompt\": icl_prompt,\n",
    "            \"train\": [{\n",
    "                \"subject\": sample.subject,\n",
    "                \"object\": sample.object,\n",
    "            } for sample in train.samples],\n",
    "            \"zero_shot\": [],        # W_r and b_r calculated without any ICL examples\n",
    "            \"logit_lens\": [],       # F(h) = h \n",
    "            \"corner\": [],           # F(h) = h + b\n",
    "            \"learned_linear\": [],   # F(h) = Wh + b, W is learned with linear regression\n",
    "            \"icl_mean_emb\": [],     # ICL-Mean but h set to embedding\n",
    "            \"icl_mean\": [],         # flagship method\n",
    "        }\n",
    "        \n",
    "        zero_shot_estimator = JacobianEstimator(\n",
    "            mt=mt,\n",
    "            h_layer=hparams[\"h_layer\"],\n",
    "            beta=hparams[\"beta\"],\n",
    "        )\n",
    "        zero_shot_operator = zero_shot_estimator.estimate_for_subject(\n",
    "            subject=train.samples[0].subject,\n",
    "            prompt_template=prompt_template,\n",
    "        )\n",
    "        zero_shot_recall = evaluate(zero_shot_operator, test)\n",
    "        print(f\"zero shot recall: {zero_shot_recall['recall']}\")\n",
    "        trial_results[\"zero_shot\"] = zero_shot_recall\n",
    "        \n",
    "        logit_lens_operator = LinearRelationOperator(\n",
    "            mt = mt, \n",
    "            h_layer = hparams[\"h_layer\"],\n",
    "            weight = None, bias = None,\n",
    "            prompt_template=icl_prompt,\n",
    "            z_layer = -1,\n",
    "        )\n",
    "        logit_lens_recall = evaluate(logit_lens_operator, test)\n",
    "        print(f\"logit lens recall: {logit_lens_recall['recall']}\")\n",
    "        trial_results[\"logit_lens\"] = logit_lens_recall\n",
    "\n",
    "        offset_estimator = OffsetEstimatorBaseline(\n",
    "            mt=mt,\n",
    "            h_layer=hparams[\"h_layer\"],\n",
    "        )\n",
    "        offset_operator = offset_estimator(train)\n",
    "        offset_recall = evaluate(offset_operator, test)\n",
    "        print(f\"offset recall: {offset_recall['recall']}\")\n",
    "        trial_results[\"corner\"] = offset_recall\n",
    "\n",
    "        learned_estimator = LearnedLinearEstimatorBaseline(\n",
    "            mt=mt,\n",
    "            h_layer=hparams[\"h_layer\"],\n",
    "        )\n",
    "        learned_operator = learned_estimator(train)\n",
    "        learned_recall = evaluate(learned_operator, test)\n",
    "        print(f\"learned recall: {learned_recall['recall']}\")\n",
    "        trial_results[\"learned_linear\"] = learned_recall\n",
    "\n",
    "        mean_emb_estimator = JacobianIclMeanEstimator(\n",
    "            mt=mt,\n",
    "            h_layer = \"emb\",\n",
    "            beta=hparams[\"beta\"],\n",
    "        )\n",
    "        mean_emb_operator = mean_emb_estimator(train)\n",
    "        mean_emb_recall = evaluate(mean_emb_operator, test)\n",
    "        print(f\"icl mean recall (emb): {mean_emb_recall['recall']}\")\n",
    "        trial_results[\"icl_mean_emb\"] = mean_emb_recall\n",
    "\n",
    "        mean_estimator = JacobianIclMeanEstimator(\n",
    "            mt=mt,\n",
    "            h_layer=hparams[\"h_layer\"],\n",
    "            beta=hparams[\"beta\"],\n",
    "        )\n",
    "        mean_operator = mean_estimator(train)\n",
    "        mean_recall = evaluate(mean_operator, test)\n",
    "        print(f\"icl mean recall: {mean_recall['recall']}\")\n",
    "        trial_results[\"icl_mean\"] = mean_recall\n",
    "\n",
    "        result[\"trials\"].append(trial_results)\n",
    "        print()\n",
    "\n",
    "    all_results.append(result)\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    with open(f\"{save_dir}/gptj.json\", \"w\") as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
