{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"..\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import models, data, operators, editors, functional, metrics, lens\n",
    "from src.utils import logging_utils, experiment_utils\n",
    "import logging\n",
    "import torch\n",
    "import baukit\n",
    "\n",
    "logging_utils.configure(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-28 17:12:39 src.models INFO     loading EleutherAI/gpt-j-6B (device=cuda, fp16=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-28 17:12:47 src.models INFO     dtype: torch.float16, device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "mt = models.load_model(\"gptj\", fp16=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-28 17:31:27 src.utils.experiment_utils INFO     setting all seeds to 123456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-28 17:32:11 src.operators WARNING  relation has > 1 prompt_templates, will use first ({} works as a)\n",
      "2023-07-28 17:32:46 src.operators WARNING  relation has > 1 prompt_templates, will use first (For university, {} attended)\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "layer = 3\n",
    "rank = 170\n",
    "beta = 2.25\n",
    "n_train = 5\n",
    "selected_relations = [r for r in dataset if r.name in [\n",
    "        \"person occupation\",\n",
    "        \"name birthplace\",\n",
    "        \"person university\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "experiment_utils.set_seed(123456)\n",
    "##################################\n",
    "\n",
    "\n",
    "relation_properties = {}\n",
    "\n",
    "for relation in selected_relations:\n",
    "    train, test = relation.split(n_train)\n",
    "    prompt_template = relation.prompt_templates[0]\n",
    "\n",
    "    relation_prompt = functional.make_prompt(\n",
    "        mt=mt,\n",
    "        prompt_template=prompt_template,\n",
    "        subject=\"{}\",\n",
    "        examples=train.samples,\n",
    "    )\n",
    "\n",
    "    estimator = operators.JacobianIclMeanEstimator(\n",
    "        mt = mt, h_layer=layer, beta=beta, rank=rank\n",
    "    )\n",
    "    operator = estimator(train)\n",
    "\n",
    "    relation_properties[relation.name] = {\n",
    "        \"train\": train,\n",
    "        \"prompt_template\": prompt_template,\n",
    "        \"prompt\": relation_prompt,\n",
    "        \"operator\": operator,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "name birthplace\n",
      "-----------------------------------\n",
      "['Rohit -> India', 'Sakura -> Japan', 'Marco -> Italy', 'Hong -> China', 'Kraipob -> Thailand']\n",
      "<|endoftext|>Rohit was born in the country of India\n",
      "Sakura was born in the country of Japan\n",
      "Marco was born in the country of Italy\n",
      "Hong was born in the country of China\n",
      "Kraipob was born in the country of Thailand\n",
      "{} was born in the country of\n",
      "-----------------------------------\n",
      "person occupation\n",
      "-----------------------------------\n",
      "['Yakubu Gowon -> politician', 'Geraldine McNulty -> actor', 'Andrew Salkey -> poet', 'Wilhelm Magnus -> mathematician', 'Samuel Medary -> journalist']\n",
      "<|endoftext|>Yakubu Gowon works as a politician\n",
      "Geraldine McNulty works as a actor\n",
      "Andrew Salkey works as a poet\n",
      "Wilhelm Magnus works as a mathematician\n",
      "Samuel Medary works as a journalist\n",
      "{} works as a\n",
      "-----------------------------------\n",
      "person university\n",
      "-----------------------------------\n",
      "['Ursula K. Le Guin -> Columbia University', 'Michelle Obama -> Princeton University', 'Hillary Clinton -> Wellesley College', 'Britney Spears -> University of Nebraska High School', 'Johann Wolfgang von Goethe -> University of Strasbourg']\n",
      "<|endoftext|>For university, Ursula K. Le Guin attended Columbia University\n",
      "For university, Michelle Obama attended Princeton University\n",
      "For university, Hillary Clinton attended Wellesley College\n",
      "For university, Britney Spears attended University of Nebraska High School\n",
      "For university, Johann Wolfgang von Goethe attended University of Strasbourg\n",
      "For university, {} attended\n"
     ]
    }
   ],
   "source": [
    "for relation_name in relation_properties:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(relation_name)\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"{[sample.__str__() for sample in relation_properties[relation_name]['train'].samples]}\")\n",
    "    print(relation_properties[relation_name]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation_name in relation_properties:\n",
    "    relation_properties[relation_name][\"W_inv\"] = functional.low_rank_pinv(\n",
    "        matrix = relation_properties[relation_name][\"operator\"].weight,\n",
    "        rank=rank,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X -- person occupation --  politician (p=0.067)\n",
      "X -- name birthplace --  Russia (p=0.076)\n",
      "X -- person university --  University (p=0.348)\n",
      "=================================\n",
      "Sherlock Holmes -- person occupation --  detective (p=0.523)\n",
      "Jackie Chan -- name birthplace --  China (p=0.578)\n",
      "Bill Gates -- person university --  Harvard (p=0.893)\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "source_subject = \"X\"\n",
    "targ_prop_for_subj = {\n",
    "    \"person occupation\": \"Sherlock Holmes\",\n",
    "    \"name birthplace\": \"Jackie Chan\",\n",
    "    \"person university\": \"Bill Gates\",  \n",
    "}\n",
    "##################################################\n",
    "\n",
    "for prop in targ_prop_for_subj:\n",
    "    prompt = relation_properties[prop]['prompt'].format(source_subject)\n",
    "    obj = functional.predict_next_token(mt = mt, prompt = prompt, k=3)[0]\n",
    "    print(f\"{source_subject} -- {prop} -- {obj[0].__str__()}\")\n",
    "print(\"=================================\")\n",
    "\n",
    "for prop, subj in targ_prop_for_subj.items():\n",
    "    prompt = relation_properties[prop]['prompt'].format(subj)\n",
    "    obj = functional.predict_next_token(mt = mt, prompt = prompt, k=3)[0]\n",
    "    print(f\"{subj} -- {prop} -- {obj[0].__str__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_s(\n",
    "    prop, \n",
    "    source_subject, \n",
    "    target_subject,\n",
    "    fix_latent_norm = None,\n",
    "):\n",
    "    w_p_inv = relation_properties[prop][\"W_inv\"]\n",
    "    hs_and_zs = functional.compute_hs_and_zs(\n",
    "        mt = mt,\n",
    "        prompt_template = relation_properties[prop][\"prompt_template\"],\n",
    "        subjects = [source_subject, target_subject],\n",
    "        h_layer= layer,\n",
    "        z_layer=-1,\n",
    "        examples= relation_properties[prop][\"train\"].samples,\n",
    "    )\n",
    "\n",
    "    z_source = hs_and_zs.z_by_subj[source_subject]\n",
    "    z_target = hs_and_zs.z_by_subj[targ_prop_for_subj[prop]]\n",
    "    # print(z_target.norm().item(), z_source.norm().item())\n",
    "\n",
    "    h_source = hs_and_zs.h_by_subj[source_subject]\n",
    "    h_target = hs_and_zs.h_by_subj[targ_prop_for_subj[prop]]\n",
    "\n",
    "    z_source *= fix_latent_norm / z_source.norm() if fix_latent_norm is not None else 1.0\n",
    "    z_target *= z_source.norm() / z_target.norm() if fix_latent_norm is not None else 1.0\n",
    "    print(z_target.norm().item(), z_source.norm().item())\n",
    "\n",
    "    delta_s = w_p_inv @  (z_target.squeeze() - z_source.squeeze())\n",
    "    \n",
    "    print(f\"h_source: {h_source.norm().item()} | h_target: {h_target.norm().item()}\")\n",
    "    print(f\"inv_h_source: {(w_p_inv @ z_source).norm().item()} | inv_h_target: {(w_p_inv @ z_target).norm().item()}\")\n",
    "    print(delta_s.norm().item())\n",
    "\n",
    "    return delta_s, hs_and_zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312.75 363.5\n",
      "h_source: 46.5 | h_target: 53.28125\n",
      "inv_h_source: 149.5 | inv_h_target: 143.0\n",
      "85.3125\n",
      "241.125 319.0\n",
      "h_source: 46.25 | h_target: 51.75\n",
      "inv_h_source: 72.5625 | inv_h_target: 72.0625\n",
      "53.96875\n",
      "159.125 350.25\n",
      "h_source: 43.875 | h_target: 50.03125\n",
      "inv_h_source: 47.40625 | inv_h_target: 44.8125\n",
      "47.78125\n"
     ]
    }
   ],
   "source": [
    "delta_s_by_prop = {}\n",
    "for relation_name in targ_prop_for_subj:\n",
    "    delta_s, hs_and_zs = get_delta_s(\n",
    "        prop = relation_name,\n",
    "        source_subject = source_subject,\n",
    "        target_subject = targ_prop_for_subj[relation_name],\n",
    "        # fix_latent_norm=250\n",
    "    )\n",
    "\n",
    "    delta_s_by_prop[relation_name] = {\n",
    "        \"delta_s\": delta_s,\n",
    "        \"hs_and_zs\": hs_and_zs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- 0 -- 7284.0, 0.9990234375\n",
      "0 -- 1 -- 346.75, 0.07525634765625\n",
      "0 -- 2 -- -11.109375, -0.0027179718017578125\n",
      "1 -- 0 -- 346.75, 0.07525634765625\n",
      "1 -- 1 -- 2912.0, 0.99951171875\n",
      "1 -- 2 -- -113.5625, -0.044036865234375\n",
      "2 -- 0 -- -11.109375, -0.0027179718017578125\n",
      "2 -- 1 -- -113.5625, -0.044036865234375\n",
      "2 -- 2 -- 2282.0, 0.99951171875\n"
     ]
    }
   ],
   "source": [
    "drr = [prop[\"delta_s\"] for relation, prop in delta_s_by_prop.items()]\n",
    "for i in range(len(drr)):\n",
    "    for j in range(len(drr)):\n",
    "        print(f\"{i} -- {j} -- {(drr[i][None] @ drr[j][None].T).squeeze().item()}, {torch.cosine_similarity(drr[i], drr[j], dim=0).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096]), 49.71875)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_norm = np.array([delta_s_by_prop[relation_name][\"delta_s\"].norm().item() for relation_name in delta_s_by_prop.keys()]).max()\n",
    "\n",
    "cumulative_delta_s = torch.zeros_like(delta_s_by_prop[prop][\"delta_s\"])\n",
    "for relation_name in delta_s_by_prop:\n",
    "    ds = delta_s_by_prop[relation_name][\"delta_s\"]\n",
    "    ds = ds*max_norm / ds.norm()\n",
    "    delta_s_by_prop[relation_name][\"delta_s\"] = ds\n",
    "    cumulative_delta_s += ds\n",
    "cumulative_delta_s /= 3\n",
    "cumulative_delta_s.shape, cumulative_delta_s.norm().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_names = [\n",
    "#         \"person occupation\",\n",
    "#         \"name birthplace\",\n",
    "#         \"person university\"\n",
    "# ]\n",
    "\n",
    "# cumulative_delta_s = (\n",
    "#     delta_s_by_prop[relation_names[0]][\"delta_s\"] + \n",
    "#     delta_s_by_prop[relation_names[1]][\"delta_s\"] + \n",
    "#     delta_s_by_prop[relation_names[2]][\"delta_s\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.125 250.0\n",
      "h_source: 43.875 | h_target: 50.03125\n",
      "inv_h_source: 33.84375 | inv_h_target: 70.4375\n",
      "62.375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' University', 20.953),\n",
       " (' the', 19.109),\n",
       " (' St', 18.812),\n",
       " (' Duke', 18.062),\n",
       " (' Oxford', 18.016),\n",
       " (' Stanford', 17.75),\n",
       " (' Brown', 17.703),\n",
       " (' Harvard', 17.672),\n",
       " (' City', 17.531),\n",
       " (' Sun', 17.469)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prop = \"person occupation\"\n",
    "# prop = \"name birthplace\"\n",
    "prop = \"person university\"\n",
    "\n",
    "delta_s, hs_and_zs = get_delta_s(\n",
    "    prop = prop,\n",
    "    source_subject = source_subject,\n",
    "    target_subject = targ_prop_for_subj[prop],\n",
    "    fix_latent_norm = 250\n",
    ")\n",
    "\n",
    "def get_intervention(h, int_layer, subj_idx):\n",
    "    def edit_output(output, layer):\n",
    "        if(layer != int_layer):\n",
    "            return output\n",
    "        functional.untuple(output)[:, subj_idx] = h\n",
    "        return output\n",
    "    return edit_output\n",
    "\n",
    "prompt = relation_properties[prop][\"prompt\"].format(source_subject)\n",
    "\n",
    "h_index, inputs = functional.find_subject_token_index(\n",
    "    mt=mt,\n",
    "    prompt=prompt,\n",
    "    subject=source_subject,\n",
    ")\n",
    "\n",
    "h_layer, z_layer = models.determine_layer_paths(model = mt, layers = [layer, -1])\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    mt.model, layers = [h_layer, z_layer],\n",
    "    edit_output=get_intervention(\n",
    "        # h = hs_and_zs.h_by_subj[source_subject]\n",
    "        # h = hs_and_zs.h_by_subj[source_subject] + delta_s,\n",
    "        h = hs_and_zs.h_by_subj[source_subject] + cumulative_delta_s, \n",
    "        int_layer = h_layer, \n",
    "        subj_idx = h_index\n",
    "    )\n",
    ") as traces:\n",
    "    outputs = mt.model(\n",
    "        input_ids = inputs.input_ids,\n",
    "        attention_mask = inputs.attention_mask,\n",
    "    )\n",
    "\n",
    "lens.interpret_logits(\n",
    "    mt = mt, \n",
    "    logits = outputs.logits[0][-1], \n",
    "    # get_proba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
