{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src import models, data\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs(\"layer_sweep/Jacobian_plots\", exist_ok=True)\n",
    "os.makedirs(\"layer_sweep/weights_and_biases\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float16, device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "mt = models.load_model(\"gptj\", device=device)\n",
    "print(\n",
    "    f\"dtype: {mt.model.dtype}, device: {mt.model.device}, memory: {mt.model.get_memory_footprint()}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The Space Needle is located in the city of\"\n",
    "tokenized = mt.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(mt.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "\n",
    "with baukit.TraceDict(\n",
    "    mt.model,\n",
    "    models.determine_layer_paths(mt)\n",
    ") as traces:\n",
    "    output = mt.model(**tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def interpret_logits(mt, logits, top_k=10, get_proba = False):\n",
    "    logits = torch.nn.functional.softmax(logits, dim=-1) if get_proba else logits\n",
    "    token_ids = logits.topk(dim=-1, k=top_k).indices.squeeze().tolist()\n",
    "    logit_values = logits.topk(dim=-1, k=top_k).values.squeeze().tolist()\n",
    "    return [\n",
    "        (mt.tokenizer.decode(t), round(v, 3)) for t, v in zip(token_ids, logit_values)\n",
    "    ]\n",
    "\n",
    "\n",
    "def logit_lens(\n",
    "    mt,\n",
    "    h,\n",
    "    interested_tokens=[],\n",
    "    get_proba=False,\n",
    "):\n",
    "    logits = mt.lm_head(h)\n",
    "    logits = torch.nn.functional.softmax(logits, dim=-1) if get_proba else logits\n",
    "    candidates = interpret_logits(mt, logits)\n",
    "    interested_logits = {\n",
    "        t.item(): (logits[t].item(), mt.tokenizer.decode(t)) for t in interested_tokens\n",
    "    }\n",
    "    return candidates, interested_logits\n",
    "\n",
    "\n",
    "# interpret_logits(mt, output.logits[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interested_words = [\" Seattle\", \" Paris\", \" Dhaka\"]\n",
    "# int_tokenized = mt.tokenizer(interested_words, return_tensors=\"pt\", padding=True).to(\n",
    "#     mt.model.device\n",
    "# )\n",
    "# int_tokenized.input_ids\n",
    "\n",
    "# z = untuple(traces[models.determine_layer_paths(mt)[-1]].output)[0][-1]\n",
    "# print(z.shape)\n",
    "\n",
    "# logit_lens(mt, z, [t[0] for t in int_tokenized.input_ids], get_proba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_model_knowledge(mt, relation_prompt, relation_samples):\n",
    "    model_knows = []\n",
    "    for sample in relation_samples:\n",
    "        tokenized = relation_prompt.format(sample.subject)\n",
    "        output = mt.model(\n",
    "            **mt.tokenizer(tokenized, return_tensors=\"pt\", padding=True).to(\n",
    "                mt.model.device\n",
    "            )\n",
    "        )\n",
    "\n",
    "        object_id = output.logits[0][-1].argmax().item()\n",
    "        object = mt.tokenizer.decode(object_id)\n",
    "\n",
    "        tick = sample.object.strip().startswith(object.strip())\n",
    "        # print(object, sample.object, tick)\n",
    "\n",
    "        if tick:\n",
    "            model_knows.append(sample)\n",
    "\n",
    "    return model_knows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'prompt_templates', 'samples', 'properties', '_domain', '_range'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data.load_dataset()\n",
    "capital_cities = dataset[0]\n",
    "capital_cities.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.functional import make_prompt\n",
    "\n",
    "icl_indices = np.random.choice(range(len(capital_cities.samples)), 3, replace=False)\n",
    "icl_samples = [capital_cities.samples[i] for i in icl_indices]\n",
    "\n",
    "icl_prompt = make_prompt(\n",
    "    prompt_template = capital_cities.prompt_templates[0],\n",
    "    subject=\"{}\",\n",
    "    examples=icl_samples,\n",
    ")\n",
    "\n",
    "print(icl_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knows = filter_by_model_knowledge(mt, icl_prompt, capital_cities.samples)\n",
    "len(model_knows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Richness based on logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "def layer_c_measure(\n",
    "    mt, relation_prompt, subject, \n",
    "    verbose=False, measure: Literal [\"completeness\", \"contribution\"] = \"contribution\"\n",
    "):\n",
    "    tokenized = relation_prompt.format(subject)\n",
    "    with baukit.TraceDict(mt.model, layers=models.determine_layer_paths(mt)) as traces:\n",
    "        output = mt.model(\n",
    "            **mt.tokenizer(tokenized, return_tensors=\"pt\", padding=True).to(\n",
    "                mt.model.device\n",
    "            )\n",
    "        )\n",
    "\n",
    "    object_id = output.logits[0][-1].argmax().item()\n",
    "    object = mt.tokenizer.decode(object_id)\n",
    "    # base_logit = output.logits[0][-1][object_id].item()\n",
    "    base_score = torch.nn.functional.softmax(output.logits[0][-1], dim=-1)[\n",
    "        object_id\n",
    "    ].item()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"object ==> {object} [{object_id}], base = {base_score}\")\n",
    "\n",
    "    layer_contributions = {}\n",
    "\n",
    "    prev_score = 0\n",
    "    for layer in models.determine_layer_paths(mt):\n",
    "        h = untuple(traces[layer].output)[0][-1]\n",
    "        candidates, interested_logits = logit_lens(\n",
    "            mt, h, torch.tensor([object_id]), get_proba=True\n",
    "        )\n",
    "        layer_score = interested_logits[object_id][0]\n",
    "        sub_score = base_score if measure == \"completeness\" else prev_score\n",
    "        cur_layer_contribution = (layer_score - sub_score) / base_score\n",
    "\n",
    "        layer_contributions[layer] = cur_layer_contribution\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"layer: {layer}, diff: {cur_layer_contribution}\")\n",
    "\n",
    "        prev_score = layer_score\n",
    "\n",
    "    return layer_contributions\n",
    "\n",
    "\n",
    "relation_prompt = mt.tokenizer.eos_token + \" {} is located in the city of\"\n",
    "subject = \"The Space Needle\"\n",
    "# layer_c_measure(mt, relation_prompt, subject, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_c_info = {layer: [] for layer in models.determine_layer_paths(mt)}\n",
    "\n",
    "for sample in tqdm(model_knows):\n",
    "    cur_richness = layer_c_measure(mt, icl_prompt, sample.subject)\n",
    "    for layer in models.determine_layer_paths(mt):\n",
    "        layer_c_info[layer].append(cur_richness[layer])\n",
    "\n",
    "# with open(\"layer_sweep/layer_contribution_info.json\", \"w\") as f:\n",
    "with open(\"layer_sweep/layer_completeness_info.json\", \"w\") as f:\n",
    "    json.dump(layer_c_info, f)\n",
    "\n",
    "for layer in models.determine_layer_paths(mt):\n",
    "    layer_c_info[layer] = np.array(layer_c_info[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_richness = [layer_c_info[layer].mean() for layer in models.determine_layer_paths(mt)]\n",
    "low_richness = [layer_c_info[layer].min() for layer in models.determine_layer_paths(mt)]\n",
    "high_richness = [layer_c_info[layer].max() for layer in models.determine_layer_paths(mt)]\n",
    "\n",
    "plt.plot(mean_richness, color=\"blue\")\n",
    "plt.fill_between(range(len(mean_richness)), low_richness, high_richness, alpha=0.2)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"completeness\")\n",
    "plt.xticks(range(0, len(mean_richness), 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Richness based on `Jh_norm` and `J_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "capital_cities_known = copy.deepcopy(capital_cities.__dict__)\n",
    "capital_cities_known[\"samples\"] = model_knows\n",
    "\n",
    "capital_cities_known = data.Relation(**capital_cities_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.operators import JacobianEstimator, JacobianIclMeanEstimator\n",
    "from src.data import RelationSample\n",
    "\n",
    "# indices = np.random.choice(range(len(capital_cities.samples)), 3, replace=False)\n",
    "# samples = [capital_cities.samples[i] for i in indices]\n",
    "\n",
    "# training_samples = copy.deepcopy(capital_cities.__dict__)\n",
    "# training_samples[\"samples\"] = samples\n",
    "# training_samples = data.Relation(**training_samples)\n",
    "\n",
    "# mean_estimator = JacobianIclMeanEstimator(\n",
    "#     mt=mt,\n",
    "#     h_layer=12,\n",
    "# )\n",
    "\n",
    "# operator = mean_estimator(training_samples)\n",
    "# operator(\"Russia\", k = 10).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = JacobianEstimator(\n",
    "    mt=mt,\n",
    "    h_layer=12,\n",
    ")\n",
    "\n",
    "operator = estimator.call_on_sample(\n",
    "    sample = RelationSample(subject=\"United States\", object=\"Washington\"),\n",
    "    prompt_template= icl_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator.metadata['Jh'].norm().item(), operator.weight.norm().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerwise_jh = {layer: [] for layer in models.determine_layer_paths(mt)}\n",
    "\n",
    "for sample in tqdm(set(model_knows) - set(icl_samples)):\n",
    "    for h_layer in range(0, 24):\n",
    "        layer_name = models.determine_layer_paths(mt)[h_layer]\n",
    "        estimator = JacobianEstimator(\n",
    "            mt=mt,\n",
    "            h_layer=h_layer,\n",
    "        )\n",
    "        operator = estimator.call_on_sample(\n",
    "            # sample = RelationSample(subject=\"Russia\", object=\"Moscow\"),\n",
    "            sample = sample,\n",
    "            prompt_template= icl_prompt\n",
    "        )\n",
    "\n",
    "        # print(h_layer, \" ===> \", f\"J:{operator.weight.norm().item()},  Jh: {operator.misc['Jh'].norm().item()}\")\n",
    "        layerwise_jh[layer_name].append({\n",
    "            \"J\": operator.weight.norm().item(),\n",
    "            \"Jh\": operator.metadata['Jh'].norm().item(),\n",
    "            \"bias\": operator.bias.norm().item()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in models.determine_layer_paths(mt):\n",
    "    if layer in layerwise_jh and len(layerwise_jh[layer]) == 0:\n",
    "        layerwise_jh.pop(layer)\n",
    "\n",
    "with open(\"layer_sweep/layer_jh_info.json\", \"w\") as f:\n",
    "    json.dump(layerwise_jh, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"Jh\"\n",
    "\n",
    "info = {\n",
    "    layer: np.array([layerwise_jh[layer][i][key] for i in range(len(layerwise_jh[layer]))])\n",
    "    for layer in layerwise_jh.keys()\n",
    "}\n",
    "\n",
    "mean = [info[layer].mean() for layer in info.keys()]\n",
    "plt.plot(mean, color=\"blue\", linewidth=4)\n",
    "plt.xticks(range(0, len(mean), 2))\n",
    "plt.ylabel(f\"{key}_norm\")\n",
    "\n",
    "for i in range(len(set(model_knows) - set(icl_samples))):\n",
    "    arr = []\n",
    "    for layer in layerwise_jh.keys():\n",
    "        arr.append(layerwise_jh[layer][i][key])\n",
    "    plt.plot(arr, alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing on `subject_last`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.operators import _compute_h_index\n",
    "\n",
    "# h_idx, inputs = _compute_h_index(\n",
    "#     mt = mt, \n",
    "#     prompt = \"The location of {} is in the city of\".format(subject_original),\n",
    "#     subject = subject_original,\n",
    "#     offset=-1\n",
    "# )\n",
    "\n",
    "# print(h_idx, inputs)\n",
    "# for t in inputs.input_ids[0]:\n",
    "#     print(t.item(), mt.tokenizer.decode(t.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replace_intervention(intervention_layer, intervention_tok_idx, h_intervention):\n",
    "    def intervention(output, layer):\n",
    "        if(layer != intervention_layer):\n",
    "            return output\n",
    "        output[0][0][intervention_tok_idx] = h_intervention\n",
    "        return output\n",
    "    return intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_tracing(\n",
    "    mt,\n",
    "    prompt_template,\n",
    "    subject_original, subject_corruption,\n",
    "    verbose = False\n",
    "): \n",
    "    h_idx_orig, tokenized_orig = _compute_h_index(\n",
    "        mt = mt,\n",
    "        prompt = prompt_template.format(subject_original),\n",
    "        subject = subject_original,\n",
    "        offset=-1\n",
    "    ) \n",
    "\n",
    "    h_idx_corr, tokenized_corr = _compute_h_index(\n",
    "        mt = mt,\n",
    "        prompt = prompt_template.format(subject_corruption),\n",
    "        subject = subject_corruption,\n",
    "        offset=-1\n",
    "    )\n",
    "\n",
    "    layer_names = models.determine_layer_paths(mt)\n",
    "    with baukit.TraceDict(\n",
    "        mt.model, layer_names\n",
    "    ) as traces_o:\n",
    "        output_o = mt.model(**tokenized_orig)\n",
    "\n",
    "    answer, p_answer = interpret_logits(mt, output_o.logits[0][-1], get_proba=True)[0]\n",
    "    answer_t = mt.tokenizer(answer, return_tensors=\"pt\").to(device).input_ids[0]\n",
    "\n",
    "    if(verbose):\n",
    "        print(f\"answer: {answer}[{answer_t.item()}], p(answer): {p_answer:.3f}\")\n",
    "\n",
    "    result = {}\n",
    "    for intervention_layer in layer_names:\n",
    "        with baukit.TraceDict(\n",
    "            mt.model, \n",
    "            layers = layer_names,\n",
    "            edit_output = get_replace_intervention(\n",
    "                intervention_layer= intervention_layer,\n",
    "                intervention_tok_idx= h_idx_corr,\n",
    "                h_intervention = untuple(traces_o[intervention_layer].output)[0][h_idx_orig]\n",
    "            )\n",
    "        ) as traces_i:\n",
    "            output_i = mt.model(**mt.tokenizer(prompt_template.format(subject_corruption), return_tensors=\"pt\").to(device))\n",
    "\n",
    "        z = untuple(traces_i[layer_names[-1]].output)[0][-1]\n",
    "        candidates, interested = logit_lens(mt, z, [answer_t], get_proba=True)\n",
    "        layer_p = interested[answer_t.item()][0]\n",
    "\n",
    "        if(verbose):\n",
    "            print(intervention_layer, layer_p)\n",
    "        result[intervention_layer] = (layer_p - p_answer)/p_answer\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "causal_tracing(\n",
    "    mt,\n",
    "    prompt_template = \"The location of {} is in the city of\",\n",
    "    subject_original = \"The Space Needle\",\n",
    "    subject_corruption = \"The Statue of Liberty\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "capital_cities_known = copy.deepcopy(capital_cities.__dict__)\n",
    "capital_cities_known[\"samples\"] = model_knows\n",
    "\n",
    "capital_cities_known = data.Relation(**capital_cities_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_icl = 3\n",
    "\n",
    "icl_indices = np.random.choice(range(len(capital_cities_known.samples)), num_icl, replace=False)\n",
    "icl_samples = [capital_cities.samples[i] for i in icl_indices]\n",
    "icl_prompt = [\n",
    "    f\"{capital_cities.prompt_templates[0].format(sample.subject)} {sample.object}\"\n",
    "    for sample in icl_samples\n",
    "]\n",
    "icl_prompt = \"\\n\".join(icl_prompt) + \"\\n\" + capital_cities.prompt_templates[0]\n",
    "\n",
    "print(icl_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = set(capital_cities_known.samples) - set(icl_samples)\n",
    "causal_tracing_results = {layer: [] for layer in models.determine_layer_paths(mt)}\n",
    "\n",
    "n_runs = 20\n",
    "for run in tqdm(range(n_runs)):\n",
    "    sample_pair = np.random.choice(range(len(test_samples)), 2, replace=False)\n",
    "    sample_pair = [list(test_samples)[i] for i in sample_pair]\n",
    "    print(sample_pair)\n",
    "    \n",
    "    cur_result = causal_tracing(\n",
    "        mt,\n",
    "        prompt_template = icl_prompt,\n",
    "        subject_original = sample_pair[0].subject,\n",
    "        subject_corruption = sample_pair[1].subject,\n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    for layer in models.determine_layer_paths(mt):\n",
    "        causal_tracing_results[layer].append(cur_result[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"layer_sweep/causal_tracing_results.json\", \"w\") as f:\n",
    "    json.dump(causal_tracing_results, f)\n",
    "\n",
    "for layer in models.determine_layer_paths(mt):\n",
    "    causal_tracing_results[layer] = np.array(causal_tracing_results[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.determine_layers(mt)[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [causal_tracing_results[layer].mean() for layer in models.determine_layer_paths(mt)]\n",
    "# low = [causal_tracing_results[layer].min() for layer in mt.layer_names]\n",
    "# high = [causal_tracing_results[layer].max() for layer in mt.layer_names]\n",
    "\n",
    "plt.plot(mean, color=\"blue\", linewidth=3)\n",
    "# plt.fill_between(range(len(mean)), low, high, alpha=0.2)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"layer_score\")\n",
    "plt.xticks(models.determine_layers(mt)[::2])\n",
    "\n",
    "for run in range(n_runs):\n",
    "    arr = []\n",
    "    for layer in models.determine_layer_paths(mt):\n",
    "        arr.append(causal_tracing_results[layer][run])\n",
    "    plt.plot(arr, alpha=0.2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer sweep on mean ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "capital_cities_known = copy.deepcopy(capital_cities.__dict__)\n",
    "capital_cities_known[\"samples\"] = model_knows\n",
    "\n",
    "capital_cities_known = data.Relation(**capital_cities_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(range(len(capital_cities_known.samples)), 2, replace=False)\n",
    "samples = [capital_cities_known.samples[i] for i in indices]\n",
    "\n",
    "capital_cities_subset = copy.deepcopy(capital_cities.__dict__)\n",
    "capital_cities_subset[\"samples\"] = samples\n",
    "capital_cities_subset = data.Relation(**capital_cities_subset)\n",
    "\n",
    "len(capital_cities_subset.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.operators import JacobianIclMeanEstimator\n",
    "\n",
    "mean_estimator = JacobianIclMeanEstimator(\n",
    "    mt=mt,\n",
    "    h_layer=12,\n",
    ")\n",
    "\n",
    "operator = mean_estimator(capital_cities_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator(\"Chile\", k = 10).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "target = []\n",
    "\n",
    "for sample in tqdm(set(capital_cities_known.samples)):\n",
    "    cur_predictions = operator(sample.subject, k = 5).predictions\n",
    "    predictions.append([\n",
    "        p.token for p in cur_predictions\n",
    "    ])\n",
    "    target.append(sample.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import recall\n",
    "\n",
    "recall(predictions, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"layer_sweep/operator_weight.npz\", jacobian = operator.weight.detach().cpu().numpy(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j = np.load(\"layer_sweep/operator_weight.npz\", allow_pickle=True)[\"jacobian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.dist(torch.tensor(j).to(device), operator.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_wise_recall(capital_cities_subset, verbose = True, save_weights = True):\n",
    "\n",
    "    layer_wise_recall = {}\n",
    "\n",
    "    layer_names = models.determine_layer_paths(mt)\n",
    "    for h_layer in tqdm(range(0, 24)):\n",
    "        layer_name = layer_names[h_layer]\n",
    "        mean_estimator = JacobianIclMeanEstimator(\n",
    "            mt=mt,\n",
    "            h_layer=h_layer,\n",
    "        )\n",
    "        operator = mean_estimator(capital_cities_subset)\n",
    "        if(save_weights):\n",
    "            np.savez(\n",
    "                f\"layer_sweep/weights_and_biases/{layer_name}.npz\", \n",
    "                jacobian = operator.weight.detach().cpu().numpy(),\n",
    "                bias = operator.bias.detach().cpu().numpy(), \n",
    "                allow_pickle=True\n",
    "            )\n",
    "\n",
    "        predictions = []\n",
    "        target = []\n",
    "\n",
    "        for sample in set(capital_cities_known.samples) - set(capital_cities_subset.samples):\n",
    "            cur_predictions = operator(sample.subject, k = 5).predictions\n",
    "            predictions.append([\n",
    "                p.token for p in cur_predictions\n",
    "            ])\n",
    "            target.append(sample.object)\n",
    "\n",
    "        layer_wise_recall[layer_name] = recall(predictions, target)\n",
    "        \n",
    "        if(verbose):\n",
    "            print(layer_name, layer_wise_recall[layer_name])\n",
    "    \n",
    "    return layer_wise_recall\n",
    "\n",
    "layer_wise_recall = get_layer_wise_recall(capital_cities_subset, verbose = True, save_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"layer_sweep/layer_wise_recall.json\", \"w\") as f:\n",
    "    json.dump(layer_wise_recall, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_wise_recall_collection = {}\n",
    "number_of_runs = 10\n",
    "\n",
    "for run in tqdm(range(number_of_runs)):\n",
    "    indices = np.random.choice(range(len(capital_cities_known.samples)), 2, replace=False)\n",
    "    samples = [capital_cities_known.samples[i] for i in indices]\n",
    "\n",
    "    capital_cities_subset = copy.deepcopy(capital_cities.__dict__)\n",
    "    capital_cities_subset[\"samples\"] = samples\n",
    "    capital_cities_subset = data.Relation(**capital_cities_subset)\n",
    "\n",
    "    layer_wise_recall = get_layer_wise_recall(capital_cities_subset, verbose=False, save_weights=False)\n",
    "\n",
    "    for layer in layer_wise_recall.keys():\n",
    "        if(layer not in layer_wise_recall_collection):\n",
    "            layer_wise_recall_collection[layer] = []\n",
    "        layer_wise_recall_collection[layer].append(layer_wise_recall[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"layer_sweep/layer_wise_recall_collection.json\", \"w\") as f:\n",
    "    json.dump(layer_wise_recall_collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"layer_sweep/layer_wise_recall_collection.json\") as f:\n",
    "    layer_wise_recall_collection = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_1 = [layer_wise_recall[layer][0] for layer in layer_wise_recall.keys()]\n",
    "# top_2 = [layer_wise_recall[layer][1] for layer in layer_wise_recall.keys()]\n",
    "# top_3 = [layer_wise_recall[layer][2] for layer in layer_wise_recall.keys()]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "top_1 = np.array([\n",
    "    np.array(layer_wise_recall_collection[layer])[:, 0]\n",
    "    for layer in layer_wise_recall_collection.keys()\n",
    "])\n",
    "\n",
    "top_2 = np.array([\n",
    "    np.array(layer_wise_recall_collection[layer])[:, 1]\n",
    "    for layer in layer_wise_recall_collection.keys()\n",
    "])\n",
    "\n",
    "top_3 = np.array([\n",
    "    np.array(layer_wise_recall_collection[layer])[:, 2]\n",
    "    for layer in layer_wise_recall_collection.keys()\n",
    "])\n",
    "\n",
    "\n",
    "plt.plot(top_1.mean(axis=1), color=\"green\", linewidth=3, label=\"recall@1\")\n",
    "plt.plot(top_2.mean(axis=1), color=\"blue\", linewidth=2, label=\"recall@2\")\n",
    "plt.plot(top_3.mean(axis=1), color=\"red\", linewidth=1, label=\"recall@3\")\n",
    "\n",
    "plt.fill_between(\n",
    "    range(len(layer_wise_recall_collection.keys())),\n",
    "    top_1.min(axis=1), top_1.max(axis=1),\n",
    "    color=\"green\", alpha=0.1\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    range(len(layer_wise_recall_collection.keys())),\n",
    "    top_2.min(axis=1), top_2.max(axis=1),\n",
    "    color=\"blue\", alpha=0.05\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    range(len(layer_wise_recall_collection.keys())),\n",
    "    top_3.min(axis=1), top_3.max(axis=1),\n",
    "    color=\"red\", alpha=0.03\n",
    ")\n",
    "\n",
    "\n",
    "plt.xticks(range(0, len(top_1), 2))\n",
    "plt.xlabel(\"layer\")\n",
    "plt.ylabel(\"recall\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.supplimentary import visualize_matrix\n",
    "\n",
    "for layer_name in models.determine_layer_paths(mt)[:24]:\n",
    "    j = np.load(f\"layer_sweep/weights_and_biases/{layer_name}.npz\", allow_pickle=True)[\"jacobian\"]\n",
    "    j = torch.tensor(j).to(device)\n",
    "    print(layer_name, j.shape)\n",
    "    visualize_matrix(j, title = layer_name, save_path=f\"layer_sweep/Jacobian_plots/{layer_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
