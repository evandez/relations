{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from relations import estimate\n",
    "from util import model_utils\n",
    "from baukit import nethook\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-j-6B ==> device: cuda:0, memory: 12219206136\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "mt = model_utils.ModelAndTokenizer(MODEL_NAME, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "\n",
    "model = mt.model\n",
    "tokenizer = mt.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"{MODEL_NAME} ==> device: {model.device}, memory: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"numbers_spelled_out.tsv\", delimiter='\\t')\n",
    "objects = list(df['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eleven ===>  ten\n",
      "twelve ===>  eleven\n",
      "thirteen ===>  twelve\n",
      "fourteen ===>  thirteen\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"three comes after two\n",
    "six comes after five\n",
    "{} comes after\"\"\"\n",
    "\n",
    "# prompt = \"\"\"grape ends with E\n",
    "# monitor ends with R\n",
    "# glass ends with\"\"\"\n",
    "\n",
    "words = [ 'eleven', 'twelve', 'thirteen', 'fourteen']\n",
    "\n",
    "for w in words:\n",
    "    txt, ret_dict = model_utils.generate_fast(\n",
    "        model, tokenizer, \n",
    "        prompts=[prompt.format(w)], max_new_tokens=10, \n",
    "        get_answer_tokens=True, argmax_greedy=True\n",
    "    )\n",
    "    print(f\"{w} ===> {ret_dict['answer'][0]['top_token']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = [\" \" + o for o in df['words']]\n",
    "\n",
    "from relations.corner import CornerEstimator\n",
    "corner_estimator = CornerEstimator(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.5625 [(' twelve', 113.188), (' seven', 110.25), (' fourteen', 110.125), (' fifteen', 109.375), (' six', 109.312)]\n"
     ]
    }
   ],
   "source": [
    "simple_corner = corner_estimator.estimate_simple_corner(objects, scale_up=60)\n",
    "print(simple_corner.norm().item(), corner_estimator.get_vocab_representation(simple_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.75 [(' ten', 100.562), (' eight', 100.5), (' four', 100.438), (' two', 100.438), (' one', 100.438)]\n"
     ]
    }
   ],
   "source": [
    "lst_sq_corner = corner_estimator.estimate_corner_lstsq_solve(objects, target_logit=40)\n",
    "print(lst_sq_corner.norm().item(), corner_estimator.get_vocab_representation(lst_sq_corner, get_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_corner = corner_estimator.estimate_average_corner_with_gradient_descent(objects, average_on=5, target_logit_value=50, verbose=False)\n",
    "# print(avg_corner.norm().item(), corner_estimator.get_vocab_representation(avg_corner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_with_test_cases(relation_operator):\n",
    "\n",
    "    test_cases = [\n",
    "        (objects[i+1], -1, objects[i]) for i in range(len(objects) - 1)\n",
    "    ]\n",
    "\n",
    "    for subject, subject_token_index, target in test_cases:\n",
    "        answer = relation_operator(\n",
    "            subject,\n",
    "            subject_token_index=subject_token_index,\n",
    "            device=model.device,\n",
    "            return_top_k=5,\n",
    "        )\n",
    "        print(f\"{subject}, target: {target}   ==>   predicted: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " two, target:  one   ==>   predicted: [' seven', ' twelve', ' eight', ' six', ' nine']\n",
      " three, target:  two   ==>   predicted: [' twelve', ' ten', ' twenty', ' seven', ' fourteen']\n",
      " four, target:  three   ==>   predicted: [' twelve', ' ten', ' seven', ' five', ' twenty']\n",
      " five, target:  four   ==>   predicted: [' twelve', ' nine', ' twenty', ' eight', ' eleven']\n",
      " six, target:  five   ==>   predicted: [' seven', ' eight', ' five', ' ten', ' twelve']\n",
      " seven, target:  six   ==>   predicted: [' eight', ' twelve', ' nine', ' eleven', ' twenty']\n",
      " eight, target:  seven   ==>   predicted: [' twelve', ' seven', ' nine', ' ten', ' five']\n",
      " nine, target:  eight   ==>   predicted: [' twelve', ' seven', ' eight', ' ten', ' twenty']\n",
      " ten, target:  nine   ==>   predicted: [' twelve', ' eight', ' twenty', ' seven', ' eleven']\n",
      " eleven, target:  ten   ==>   predicted: [' twelve', ' eight', ' seven', ' twenty', ' six']\n",
      " twelve, target:  eleven   ==>   predicted: [' seven', ' five', ' eight', ' twelve', ' twenty']\n",
      " thirteen, target:  twelve   ==>   predicted: [' seven', ' twelve', ' eight', ' five', ' nine']\n",
      " fourteen, target:  thirteen   ==>   predicted: [' twelve', ' seven', ' five', ' eight', ' nine']\n",
      " fifteen, target:  fourteen   ==>   predicted: [' twelve', ' eight', ' seven', ' twenty', ' six']\n",
      " sixteen, target:  fifteen   ==>   predicted: [' twelve', ' seven', ' eight', ' five', ' ten']\n",
      " seventeen, target:  sixteen   ==>   predicted: [' twelve', ' eight', ' five', ' twenty', ' nine']\n",
      " eighteen, target:  seventeen   ==>   predicted: [' seven', ' twelve', ' five', ' eight', ' twenty']\n",
      " nineteen, target:  eighteen   ==>   predicted: [' seven', ' eight', ' twelve', ' twenty', ' six']\n",
      " twenty, target:  nineteen   ==>   predicted: [' seven', ' eight', ' five', ' nine', ' six']\n"
     ]
    }
   ],
   "source": [
    "relation = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.eye(model.config.n_embd).to(model.dtype).to(model.device),\n",
    "    bias = simple_corner\n",
    ")\n",
    "check_with_test_cases(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_JB(top_performers, relation_prompt, num_icl = 3, calculate_at_lnf = False):\n",
    "    try:\n",
    "        jbs = []\n",
    "        for s, s_idx, o in tqdm(top_performers):\n",
    "            others = set(top_performers) - {(s, s_idx, o)}\n",
    "            others = random.sample(list(others), k = min(num_icl, len(list(others)))) \n",
    "            prompt = \"\"\n",
    "            prompt += \"\\n\".join(relation_prompt.format(s_other) + f\"{o_other}.\" for s_other, idx_other, o_other in others) + \"\\n\"\n",
    "            prompt += \" \" + relation_prompt\n",
    "            print(\"subject: \", s)\n",
    "            print(prompt)\n",
    "\n",
    "            jb, _ = estimate.relation_operator_from_sample(\n",
    "                model, tokenizer,\n",
    "                s, prompt,\n",
    "                subject_token_index= s_idx,\n",
    "                layer = 15,\n",
    "                device = model.device,\n",
    "                # calculate_at_lnf = calculate_at_lnf\n",
    "            )\n",
    "            print(jb.weight.norm(), jb.bias.norm())\n",
    "            print()\n",
    "            jbs.append(jb)\n",
    "        \n",
    "        weight = torch.stack([jb.weight for jb in jbs]).mean(dim=0)\n",
    "        bias  = torch.stack([jb.bias for jb in jbs]).mean(dim=0)\n",
    "\n",
    "        return weight, bias\n",
    "    except RuntimeError as e:\n",
    "        if(str(e).startswith(\"CUDA out of memory\")):\n",
    "            print(\"CUDA out of memory\")\n",
    "        if(num_icl > 1):\n",
    "            num_icl -= 1\n",
    "            print(\"trying with smaller icl >> \", num_icl)\n",
    "            return get_averaged_JB(top_performers, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        else:\n",
    "            raise Exception(\"RuntimeError >> can't calculate Jacobian with minimum number of icl examples\")\n",
    "\n",
    "def get_multiple_averaged_JB(top_performers, relation_prompt, N = 3, num_icl = 2, calculate_at_lnf = False):\n",
    "    weights_and_biases = []\n",
    "    sample_size = min(len(top_performers), num_icl + 2)\n",
    "    for _ in tqdm(range(N)):\n",
    "        cur_sample = random.sample(top_performers, k = sample_size)\n",
    "        weight, bias = get_averaged_JB(cur_sample, relation_prompt, num_icl, calculate_at_lnf)\n",
    "        weights_and_biases.append({\n",
    "            'weight': weight,\n",
    "            'bias'  : bias\n",
    "        })\n",
    "    return weights_and_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' two', -1, ' one'), (' three', -1, ' two'), (' four', -1, ' three'), (' five', -1, ' four'), (' six', -1, ' five'), (' seven', -1, ' six'), (' eight', -1, ' seven'), (' nine', -1, ' eight'), (' ten', -1, ' nine'), (' eleven', -1, ' ten'), (' twelve', -1, ' eleven'), (' thirteen', -1, ' twelve'), (' fourteen', -1, ' thirteen'), (' fifteen', -1, ' fourteen'), (' sixteen', -1, ' fifteen'), (' seventeen', -1, ' sixteen'), (' eighteen', -1, ' seventeen'), (' nineteen', -1, ' eighteen'), (' twenty', -1, ' nineteen')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a14949149c4420cad5e7e78908d713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8a89e8cbb641a4be563f3aef19e994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:   twelve\n",
      " thirteen comes after twelve.\n",
      " ten comes after nine.\n",
      " {} comes after\n",
      "tensor(7.0117, device='cuda:0', dtype=torch.float16) tensor(340.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   thirteen\n",
      " ten comes after nine.\n",
      " twelve comes after eleven.\n",
      " {} comes after\n",
      "tensor(15.9766, device='cuda:0', dtype=torch.float16) tensor(303.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   ten\n",
      " thirteen comes after twelve.\n",
      " twelve comes after eleven.\n",
      " {} comes after\n",
      "tensor(16.6719, device='cuda:0', dtype=torch.float16) tensor(356.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   sixteen\n",
      " ten comes after nine.\n",
      " thirteen comes after twelve.\n",
      " {} comes after\n",
      "tensor(16.0469, device='cuda:0', dtype=torch.float16) tensor(331., device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b2aed7fe8a456d91029d2b5fa3f94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:   seven\n",
      " fifteen comes after fourteen.\n",
      " four comes after three.\n",
      " {} comes after\n",
      "tensor(19.5156, device='cuda:0', dtype=torch.float16) tensor(383., device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   four\n",
      " five comes after four.\n",
      " fifteen comes after fourteen.\n",
      " {} comes after\n",
      "tensor(4.6523, device='cuda:0', dtype=torch.float16) tensor(401.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   five\n",
      " four comes after three.\n",
      " seven comes after six.\n",
      " {} comes after\n",
      "tensor(15.0312, device='cuda:0', dtype=torch.float16) tensor(363.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   fifteen\n",
      " four comes after three.\n",
      " five comes after four.\n",
      " {} comes after\n",
      "tensor(17.9062, device='cuda:0', dtype=torch.float16) tensor(369.2500, device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b4fe904fcc4ed0be70ededaf3d479d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject:   sixteen\n",
      " six comes after five.\n",
      " nine comes after eight.\n",
      " {} comes after\n",
      "tensor(17.9062, device='cuda:0', dtype=torch.float16) tensor(312.5000, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   nine\n",
      " seventeen comes after sixteen.\n",
      " sixteen comes after fifteen.\n",
      " {} comes after\n",
      "tensor(16.5938, device='cuda:0', dtype=torch.float16) tensor(399.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   seventeen\n",
      " six comes after five.\n",
      " nine comes after eight.\n",
      " {} comes after\n",
      "tensor(20.6719, device='cuda:0', dtype=torch.float16) tensor(354.7500, device='cuda:0', dtype=torch.float16)\n",
      "\n",
      "subject:   six\n",
      " seventeen comes after sixteen.\n",
      " sixteen comes after fifteen.\n",
      " {} comes after\n",
      "tensor(6.2773, device='cuda:0', dtype=torch.float16) tensor(386., device='cuda:0', dtype=torch.float16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "        (objects[i+1], -1, objects[i]) for i in range(len(objects) - 1)\n",
    "    ]\n",
    "print(samples)\n",
    "\n",
    "weights_and_biases = get_multiple_averaged_JB(\n",
    "    samples, \n",
    "    relation_prompt=\"{} comes after\", \n",
    "    N = 3, \n",
    "    calculate_at_lnf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " two, target:  one   ==>   predicted: [' seven', ' eight', ' nine', ' five', ' eleven']\n",
      " three, target:  two   ==>   predicted: [' nine', ' eight', ' seven', ' twelve', ' eleven']\n",
      " four, target:  three   ==>   predicted: [' four', ' nine', ' eight', ' five', ' seven']\n",
      " five, target:  four   ==>   predicted: [' five', ' ten', ' seven', ' eight', ' six']\n",
      " six, target:  five   ==>   predicted: [' six', ' seven', ' nine', ' five', ' twelve']\n",
      " seven, target:  six   ==>   predicted: [' seven', ' eight', ' six', ' nine', ' three']\n",
      " eight, target:  seven   ==>   predicted: [' nine', ' eight', ' seven', ' four', ' three']\n",
      " nine, target:  eight   ==>   predicted: [' nine', ' eleven', ' eight', ' four', ' seven']\n",
      " ten, target:  nine   ==>   predicted: [' eleven', ' ten', ' nine', ' five', ' one']\n",
      " eleven, target:  ten   ==>   predicted: [' eleven', ' twelve', ' thirteen', ' ten', ' one']\n",
      " twelve, target:  eleven   ==>   predicted: [' eleven', ' twelve', ' thirteen', ' four', ' seven']\n",
      " thirteen, target:  twelve   ==>   predicted: [' fourteen', ' thirteen', ' twelve', ' eleven', ' four']\n",
      " fourteen, target:  thirteen   ==>   predicted: [' fourteen', ' thirteen', ' seven', ' nine', ' four']\n",
      " fifteen, target:  fourteen   ==>   predicted: [' fourteen', ' five', ' four', ' nine', ' six']\n",
      " sixteen, target:  fifteen   ==>   predicted: [' four', ' nine', ' sixteen', ' thirteen', ' eight']\n",
      " seventeen, target:  sixteen   ==>   predicted: [' seventeen', ' eighteen', ' nineteen', ' thirteen', ' fourteen']\n",
      " eighteen, target:  seventeen   ==>   predicted: [' eighteen', ' nineteen', ' seventeen', ' nine', ' fourteen']\n",
      " nineteen, target:  eighteen   ==>   predicted: [' nineteen', ' eighteen', ' twenty', ' seventeen', ' fourteen']\n",
      " twenty, target:  nineteen   ==>   predicted: [' twenty', ' nineteen', ' ten', ' eleven', ' twelve']\n"
     ]
    }
   ],
   "source": [
    "relation_operator = estimate.RelationOperator(\n",
    "    model = model,\n",
    "    tokenizer= tokenizer,\n",
    "    relation = prompt,\n",
    "    layer = 15,\n",
    "    weight = torch.stack(\n",
    "        [wb['weight'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0),\n",
    "    # bias = torch.stack(\n",
    "    #     [wb['bias'] for wb in weights_and_biases]\n",
    "    # ).mean(dim=0),\n",
    "    bias = lst_sq_corner\n",
    ")\n",
    "\n",
    "check_with_test_cases(relation_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '.', ' one', ' three', ' two']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corner_estimator.get_vocab_representation(\n",
    "    torch.stack(\n",
    "        [wb['bias'] for wb in weights_and_biases]\n",
    "    ).mean(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3439fe3f7dcaddaf51997811d25ada8e7c0985d2997d22a3ed461af94d2f9f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
